{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P54JMUGV7emj"
      },
      "source": [
        "<h1>Master paragraph at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-uD3PH0oUw4"
      },
      "source": [
        "<h1>7/12\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz2iswuNlSlE",
        "outputId": "524382e1-8262-4146-f86c-a39d8a395b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXYMbP88r88F",
        "outputId": "b6c78075-4471-4f3f-e887-c3743f244b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.3 wfdb-4.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "62A5b54s6YRe",
        "outputId": "d49b4646-cb03-492f-f7ee-869f2192eefe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 35 CUDB records\n",
            "\n",
            "Processing cu01 ...\n",
            "  Created 65 windows so far\n",
            "Processing cu02 ...\n",
            "  No VTAC start found in cu02, skipping\n",
            "Processing cu03 ...\n",
            "  Created 381 windows so far\n",
            "Processing cu04 ...\n",
            "  Created 387 windows so far\n",
            "Processing cu05 ...\n",
            "  Created 596 windows so far\n",
            "Processing cu06 ...\n",
            "  Created 631 windows so far\n",
            "Processing cu07 ...\n",
            "  Created 664 windows so far\n",
            "Processing cu08 ...\n",
            "  Created 941 windows so far\n",
            "Processing cu09 ...\n",
            "  Created 1031 windows so far\n",
            "Processing cu10 ...\n",
            "  Created 1198 windows so far\n",
            "Processing cu11 ...\n",
            "  Created 1420 windows so far\n",
            "Processing cu12 ...\n",
            "  Created 1532 windows so far\n",
            "Processing cu13 ...\n",
            "  Created 1810 windows so far\n",
            "Processing cu14 ...\n",
            "  No VTAC start found in cu14, skipping\n",
            "Processing cu15 ...\n",
            "  Created 2066 windows so far\n",
            "Processing cu16 ...\n",
            "  Created 2171 windows so far\n",
            "Processing cu17 ...\n",
            "  Created 2404 windows so far\n",
            "Processing cu18 ...\n",
            "  Created 2589 windows so far\n",
            "Processing cu19 ...\n",
            "  Created 2849 windows so far\n",
            "Processing cu20 ...\n",
            "  Created 2944 windows so far\n",
            "Processing cu21 ...\n",
            "  Created 2944 windows so far\n",
            "Processing cu22 ...\n",
            "  Created 3133 windows so far\n",
            "Processing cu23 ...\n",
            "  Created 3318 windows so far\n",
            "Processing cu24 ...\n",
            "  Created 3525 windows so far\n",
            "Processing cu25 ...\n",
            "  Created 3797 windows so far\n",
            "Processing cu26 ...\n",
            "  Created 3797 windows so far\n",
            "Processing cu27 ...\n",
            "  Created 4099 windows so far\n",
            "Processing cu28 ...\n",
            "  Created 4446 windows so far\n",
            "Processing cu29 ...\n",
            "  Created 4675 windows so far\n",
            "Processing cu30 ...\n",
            "  Created 4675 windows so far\n",
            "Processing cu31 ...\n",
            "  Created 5020 windows so far\n",
            "Processing cu32 ...\n",
            "  Created 5314 windows so far\n",
            "Processing cu33 ...\n",
            "  Created 5570 windows so far\n",
            "Processing cu34 ...\n",
            "  Created 5570 windows so far\n",
            "Processing cu35 ...\n",
            "  Created 5904 windows so far\n",
            "\n",
            "Total pre-VTAC windows collected: 5904\n",
            "X shape: (5904, 22500)\n",
            "y shape: (5904,)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3409711745.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# Example: save to Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCUDB_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CUDB_preVTAC_X.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCUDB_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CUDB_preVTAC_y.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    575\u001b[0m                            pickle_kwargs=dict(fix_imports=fix_imports))\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             for chunk in numpy.nditer(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# CUDB Preprocessing: Pre-VTAC Windows for LSTM\n",
        "# ==================================================\n",
        "\n",
        "import wfdb\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "WINDOW_SECONDS = 90          # input window\n",
        "PREDICT_SECONDS = 60         # prediction horizon\n",
        "FS = 250                     # CUDB sampling rate (Hz)\n",
        "WINDOW_SAMPLES = WINDOW_SECONDS * FS\n",
        "PREDICT_SAMPLES = PREDICT_SECONDS * FS\n",
        "\n",
        "records = sorted(glob.glob(os.path.join(CUDB_PATH, \"*.dat\")))\n",
        "print(f\"Found {len(records)} CUDB records\\n\")\n",
        "\n",
        "# ------------------------------\n",
        "# Helper: map annotation symbol to rhythm\n",
        "# ------------------------------\n",
        "def classify_symbol(symbol):\n",
        "    \"\"\"Map CUDB annotation symbols to rhythm types\"\"\"\n",
        "    if symbol == '[':\n",
        "        return \"VT_START\"\n",
        "    if symbol == ']':\n",
        "        return \"VT_END\"\n",
        "    if symbol == '+':\n",
        "        return \"VFIB\"\n",
        "    return \"NORMAL\"\n",
        "\n",
        "# ------------------------------\n",
        "# Preprocess each record\n",
        "# ------------------------------\n",
        "all_windows = []  # store (ECG_window, time_to_VTAC) tuples\n",
        "\n",
        "for rec_path in records:\n",
        "    rec_name = os.path.basename(rec_path).replace(\".dat\", \"\")\n",
        "    print(f\"Processing {rec_name} ...\")\n",
        "\n",
        "    # Load ECG\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "        sig = record.p_signal[:,0]  # single lead\n",
        "        sig_len = len(sig)\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR reading record: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Load annotation\n",
        "    ann = None\n",
        "    for ext in [\"atr\",\"tr\"]:\n",
        "        try:\n",
        "            ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), ext)\n",
        "            break\n",
        "        except:\n",
        "            ann = None\n",
        "    if ann is None:\n",
        "        print(f\"  No annotation found for {rec_name}\")\n",
        "        continue\n",
        "\n",
        "    ann_samples = ann.sample\n",
        "    ann_symbols = ann.symbol\n",
        "\n",
        "    # Find VTAC start indices\n",
        "    vt_starts = [s for s, sym in zip(ann_samples, ann_symbols) if sym == '[']\n",
        "    if len(vt_starts) == 0:\n",
        "        print(f\"  No VTAC start found in {rec_name}, skipping\")\n",
        "        continue\n",
        "    vt_start_sample = vt_starts[0]  # only first VTAC per record\n",
        "\n",
        "    # ------------------------------\n",
        "    # Generate pre-VTAC windows\n",
        "    # ------------------------------\n",
        "    start_idx = 0\n",
        "    while (start_idx + WINDOW_SAMPLES + PREDICT_SAMPLES) <= vt_start_sample:\n",
        "        window_sig = sig[start_idx : start_idx + WINDOW_SAMPLES]\n",
        "        # compute time-to-VTAC in seconds\n",
        "        window_end = start_idx + WINDOW_SAMPLES\n",
        "        time_to_vt = (vt_start_sample - window_end) / FS\n",
        "        all_windows.append((window_sig, time_to_vt))\n",
        "        start_idx += FS  # stride 1 second\n",
        "\n",
        "    print(f\"  Created {len(all_windows)} windows so far\")\n",
        "\n",
        "print(f\"\\nTotal pre-VTAC windows collected: {len(all_windows)}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Convert to numpy arrays and save (optional)\n",
        "# ------------------------------\n",
        "X = np.array([w[0] for w in all_windows])\n",
        "y = np.array([w[1] for w in all_windows])\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# Example: save to Drive\n",
        "np.save(os.path.join(CUDB_PATH, \"CUDB_preVTAC_X.npy\"), X)\n",
        "np.save(os.path.join(CUDB_PATH, \"CUDB_preVTAC_y.npy\"), y)\n",
        "\n",
        "print(\"\\nPreprocessing complete. Arrays saved to Drive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyJ9Rs1IPVDJ"
      },
      "source": [
        "In this step, we will download the Normal Sinus Rhythm (NSR) dataset from PhysioNet, which contains ECG recordings of healthy individuals. This dataset will serve as the baseline normal ECG for training our LSTM next-step predictor. The model will learn the typical patterns of normal heart rhythms from these recordings. Later, during inference on CUDB data, the model will use this knowledge to ignore normal ECG segments and only highlight anomalous windows preceding VTAC events, allowing us to detect pre-VTAC patterns effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inafZM_SPV1i",
        "outputId": "763ee077-58b1-40ab-a431-691bc4d50301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Generating record list for: 16265\n",
            "Generating record list for: 16272\n",
            "Generating record list for: 16273\n",
            "Generating record list for: 16420\n",
            "Generating record list for: 16483\n",
            "Generating record list for: 16539\n",
            "Generating record list for: 16773\n",
            "Generating record list for: 16786\n",
            "Generating record list for: 16795\n",
            "Generating record list for: 17052\n",
            "Generating record list for: 17453\n",
            "Generating record list for: 18177\n",
            "Generating record list for: 18184\n",
            "Generating record list for: 19088\n",
            "Generating record list for: 19090\n",
            "Generating record list for: 19093\n",
            "Generating record list for: 19140\n",
            "Generating record list for: 19830\n",
            "Generating list of all files for: 16265\n",
            "Generating list of all files for: 16272\n",
            "Generating list of all files for: 16273\n",
            "Generating list of all files for: 16420\n",
            "Generating list of all files for: 16483\n",
            "Generating list of all files for: 16539\n",
            "Generating list of all files for: 16773\n",
            "Generating list of all files for: 16786\n",
            "Generating list of all files for: 16795\n",
            "Generating list of all files for: 17052\n",
            "Generating list of all files for: 17453\n",
            "Generating list of all files for: 18177\n",
            "Generating list of all files for: 18184\n",
            "Generating list of all files for: 19088\n",
            "Generating list of all files for: 19090\n",
            "Generating list of all files for: 19093\n",
            "Generating list of all files for: 19140\n",
            "Generating list of all files for: 19830\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "Download complete. Files are under: /content/drive/MyDrive/nsrdb/\n"
          ]
        }
      ],
      "source": [
        "# In Colab: Mount your Google Drive, then download NSRDB into a folder in Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# change this path if you want a different folder\n",
        "output_dir = '/content/drive/MyDrive/nsrdb/'\n",
        "\n",
        "!pip install wfdb --quiet\n",
        "\n",
        "import wfdb\n",
        "import os\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Download whole database\n",
        "wfdb.dl_database('nsrdb', output_dir, records='all', annotators='all', keep_subdirs=True, overwrite=False)\n",
        "\n",
        "print(\"Download complete. Files are under:\", output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4oTE2ooWhcY"
      },
      "source": [
        "<h1>Older autoencoder taking 150 second windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlomNlIPhw2d"
      },
      "source": [
        "Conversion of nsrdb into NPY files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r_wuTxGeAzp",
        "outputId": "773baa3e-cfb5-4f5a-93b5-fc9f1514f227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting first 90 seconds from each NSR record...\n",
            "\n",
            "Saved 1 window for 16272 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16272.npy\n",
            "Saved 1 window for 19830 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_19830.npy\n",
            "Saved 1 window for 16539 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16539.npy\n",
            "Saved 1 window for 16773 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16773.npy\n",
            "Saved 1 window for 16786 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16786.npy\n",
            "Saved 1 window for 16265 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16265.npy\n",
            "Saved 1 window for 16420 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16420.npy\n",
            "Saved 1 window for 17453 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_17453.npy\n",
            "Saved 1 window for 17052 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_17052.npy\n",
            "Saved 1 window for 16483 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16483.npy\n",
            "Saved 1 window for 19140 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_19140.npy\n",
            "Saved 1 window for 19093 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_19093.npy\n",
            "Saved 1 window for 16273 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16273.npy\n",
            "Saved 1 window for 18184 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_18184.npy\n",
            "Saved 1 window for 18177 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_18177.npy\n",
            "Saved 1 window for 19088 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_19088.npy\n",
            "Saved 1 window for 16795 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_16795.npy\n",
            "Saved 1 window for 19090 → /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_19090.npy\n",
            "\n",
            "Concatenating all NSR windows...\n",
            "Final NSR dataset saved: /content/drive/MyDrive/Models/NSR_windows_small/X_nsr_final.npy\n",
            "Final shape: (18, 37500)\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "NSR_PATH = \"/content/drive/MyDrive/nsrdb/\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Models/NSR_windows_small/\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "nsr_records = [\n",
        "    \"16272\",\"19830\",\"16539\",\"16773\",\"16786\",\"16265\",\"16420\",\"17453\",\n",
        "    \"17052\",\"16483\",\"19140\",\"19093\",\"16273\",\"18184\",\"18177\",\"19088\",\n",
        "    \"16795\",\"19090\"\n",
        "]\n",
        "\n",
        "FS = 250\n",
        "WINDOW_SEC = 150\n",
        "WINDOW_SIZE = FS * WINDOW_SEC\n",
        "\n",
        "saved_files = []\n",
        "\n",
        "print(\"Extracting first 150 seconds from each NSR record...\\n\")\n",
        "\n",
        "for rec_name in nsr_records:\n",
        "    try:\n",
        "        record_path = os.path.join(NSR_PATH, rec_name)\n",
        "        record = wfdb.rdrecord(record_path)\n",
        "        ecg = record.p_signal[:, 0]  # MLII lead\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {rec_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    if len(ecg) < WINDOW_SIZE:\n",
        "        print(f\"Skipping {rec_name}: ECG too short.\")\n",
        "        continue\n",
        "\n",
        "    # Take FIRST 150 seconds only\n",
        "    window = ecg[:WINDOW_SIZE].astype(np.float32)\n",
        "\n",
        "    out_file = os.path.join(OUTPUT_DIR, f\"X_nsr_{rec_name}.npy\")\n",
        "    np.save(out_file, window)\n",
        "    saved_files.append(out_file)\n",
        "\n",
        "    print(f\"Saved 1 window for {rec_name} → {out_file}\")\n",
        "\n",
        "# Step 2: Concatenate all windows into a single array\n",
        "print(\"\\nConcatenating all NSR windows...\")\n",
        "\n",
        "all_data = []\n",
        "for f in saved_files:\n",
        "    all_data.append(np.load(f))\n",
        "\n",
        "# Final shape → (num_records, 22500)\n",
        "X_final = np.stack(all_data, axis=0)\n",
        "\n",
        "final_file = os.path.join(OUTPUT_DIR, \"X_nsr_final.npy\")\n",
        "np.save(final_file, X_final)\n",
        "\n",
        "print(f\"Final NSR dataset saved: {final_file}\")\n",
        "print(f\"Final shape: {X_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEYiJutTdqDd"
      },
      "source": [
        "LSTM Autoencoder training on normal ECG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsu0VgFudpuZ",
        "outputId": "95180336-6af4-4cca-d4d1-114b7803a0a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 0.214424\n",
            "Epoch 2/20, Loss: 0.198809\n",
            "Epoch 3/20, Loss: 0.195857\n",
            "Epoch 4/20, Loss: 0.193262\n",
            "Epoch 5/20, Loss: 0.190715\n",
            "Epoch 6/20, Loss: 0.188204\n",
            "Epoch 7/20, Loss: 0.185732\n",
            "Epoch 8/20, Loss: 0.183238\n",
            "Epoch 9/20, Loss: 0.180723\n",
            "Epoch 10/20, Loss: 0.178192\n",
            "Epoch 11/20, Loss: 0.175629\n",
            "Epoch 12/20, Loss: 0.173043\n",
            "Epoch 13/20, Loss: 0.170352\n",
            "Epoch 14/20, Loss: 0.167667\n",
            "Epoch 15/20, Loss: 0.164939\n",
            "Epoch 16/20, Loss: 0.162244\n",
            "Epoch 17/20, Loss: 0.159596\n",
            "Epoch 18/20, Loss: 0.157047\n",
            "Epoch 19/20, Loss: 0.154673\n",
            "Epoch 20/20, Loss: 0.152493\n",
            "NSR LSTM autoencoder trained and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "SEQ_LEN = 150 * 250        # 150 seconds × 250 Hz\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 2             # Small batch size due to long sequences\n",
        "EPOCHS = 20\n",
        "LR = 0.001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Dataset ---\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # For autoencoder, input == target\n",
        "        return self.X[idx][:, np.newaxis], self.X[idx][:, np.newaxis]\n",
        "\n",
        "# --- LSTM Autoencoder ---\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc_out, (h, c) = self.encoder(x)\n",
        "        # Decoder\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# --- Load NSR windows ---\n",
        "X_nsr = np.load(\"/content/drive/MyDrive/Models/NSR_windows_small/X_nsr_final.npy\")\n",
        "dataset = ECGDataset(X_nsr)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# --- Initialize model, loss, optimizer ---\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# --- Training loop ---\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch_x.size(0)\n",
        "    avg_loss = total_loss / len(dataset)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# --- Save trained model ---\n",
        "MODEL_DIR = \"/content/drive/MyDrive/Models/NSR/\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"LSTM_NSR_autoencoder.pth\"))\n",
        "print(\"NSR LSTM autoencoder trained and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLCkOBoFIUXS"
      },
      "source": [
        "Running the model on the first file for the determination of hyperparameters to detect the abnormal ECG by setting a threshold of Errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHwUkGTgIWBi",
        "outputId": "a309f5ac-f38f-47b3-fab9-c26742e61a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using sample record: cu01\n",
            "\n",
            "Checkpoint found. Resuming...\n",
            "\n",
            "Processing windows...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|████▉     | 4486/8984 [16:12<18:23,  4.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint saved at index 85234 (95.0 percent)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 8972/8984 [32:24<00:02,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checkpoint saved at index 89720 (100.0 percent)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8984/8984 [32:27<00:00,  4.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved 89733 window errors to /content/drive/MyDrive/Models/Threshold_Exploration.csv\n",
            "\n",
            "=== Reconstruction Error Statistics ===\n",
            "Min:     0.180769\n",
            "Max:     0.379648\n",
            "Median:  0.293035\n",
            "95th:    0.377300\n",
            "98th:    0.377786\n",
            "99th:    0.378853\n",
            "\n",
            "Done without crashing. Enjoy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "SEQ_LEN = 150 * 250      # 150 seconds\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Paths ---\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder.pth\"\n",
        "ERRORS_CSV = \"/content/drive/MyDrive/Models/Threshold_Exploration.csv\"\n",
        "\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/Models/threshold_checkpoint.json\"\n",
        "\n",
        "# --- LSTM Autoencoder ---\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# --- Load model ---\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# --- Reconstruction error ---\n",
        "def reconstruction_error(x):\n",
        "    with torch.no_grad():\n",
        "        x_tensor = torch.tensor(x, dtype=torch.float32, device=DEVICE).unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x_tensor)\n",
        "        return ((recon.squeeze(-1) - x_tensor.squeeze(-1)) ** 2).mean().item()\n",
        "\n",
        "# --- Pick first record only ---\n",
        "records = sorted([f[:-4] for f in os.listdir(CUDB_PATH) if f.endswith(\".hea\")])\n",
        "rec_name = records[0]\n",
        "print(f\"Using sample record: {rec_name}\")\n",
        "\n",
        "# --- Load ECG ---\n",
        "record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "ecg = record.p_signal[:, 0]\n",
        "\n",
        "total_windows = len(ecg) - SEQ_LEN + 1\n",
        "\n",
        "\n",
        "# --- Load checkpoint if exists ---\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    print(\"\\nCheckpoint found. Resuming...\\n\")\n",
        "    with open(CHECKPOINT_PATH, \"r\") as f:\n",
        "        ckpt = json.load(f)\n",
        "\n",
        "    start_idx = ckpt[\"last_index\"] + 1\n",
        "    errors = ckpt[\"errors\"]\n",
        "else:\n",
        "    print(\"\\nNo checkpoint found. Starting fresh...\\n\")\n",
        "    start_idx = 0\n",
        "    errors = []\n",
        "\n",
        "\n",
        "# --- Calculate percent steps ---\n",
        "percent_step = max(1, total_windows // 20)   # 5 percent increments\n",
        "\n",
        "\n",
        "print(\"Processing windows...\\n\")\n",
        "\n",
        "for idx in tqdm(range(start_idx, total_windows)):\n",
        "\n",
        "    window = ecg[idx:idx + SEQ_LEN].astype(np.float32)\n",
        "    err = reconstruction_error(window)\n",
        "    errors.append(err)\n",
        "\n",
        "    # Save checkpoint every 5 percent progress\n",
        "    if idx % percent_step == 0:\n",
        "        ckpt_data = {\n",
        "            \"last_index\": idx,\n",
        "            \"errors\": errors,\n",
        "            \"total_windows\": total_windows,\n",
        "            \"record\": rec_name\n",
        "        }\n",
        "        with open(CHECKPOINT_PATH, \"w\") as f:\n",
        "            json.dump(ckpt_data, f)\n",
        "        print(f\"\\nCheckpoint saved at index {idx} ({(idx/total_windows)*100:.1f} percent)\\n\")\n",
        "\n",
        "\n",
        "# --- Final save ---\n",
        "df = pd.DataFrame({\"error\": errors})\n",
        "df.to_csv(ERRORS_CSV, index=False)\n",
        "\n",
        "# Remove checkpoint so resumed runs don’t restart from old data\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    os.remove(CHECKPOINT_PATH)\n",
        "\n",
        "print(f\"\\nSaved {len(errors)} window errors to {ERRORS_CSV}\")\n",
        "\n",
        "# --- Stats ---\n",
        "print(\"\\n=== Reconstruction Error Statistics ===\")\n",
        "print(f\"Min:     {df.error.min():.6f}\")\n",
        "print(f\"Max:     {df.error.max():.6f}\")\n",
        "print(f\"Median:  {df.error.median():.6f}\")\n",
        "print(f\"95th:    {df.error.quantile(0.95):.6f}\")\n",
        "print(f\"98th:    {df.error.quantile(0.98):.6f}\")\n",
        "print(f\"99th:    {df.error.quantile(0.99):.6f}\")\n",
        "\n",
        "print(\"\\nDone without crashing. Enjoy.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8lTvSVtWpNh"
      },
      "source": [
        "<h1>Newer autoencoder with 10 second window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H29bsZiVXIKf"
      },
      "source": [
        "nsrdb formation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-fOBUNIXK76",
        "outputId": "4635000b-fa76-450e-f21b-0a8c7077ef00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting first 10 seconds from each NSR record...\n",
            "\n",
            "Saved 1 window for 16272 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16272.npy\n",
            "Saved 1 window for 19830 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_19830.npy\n",
            "Saved 1 window for 16539 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16539.npy\n",
            "Saved 1 window for 16773 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16773.npy\n",
            "Saved 1 window for 16786 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16786.npy\n",
            "Saved 1 window for 16265 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16265.npy\n",
            "Saved 1 window for 16420 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16420.npy\n",
            "Saved 1 window for 17453 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_17453.npy\n",
            "Saved 1 window for 17052 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_17052.npy\n",
            "Saved 1 window for 16483 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16483.npy\n",
            "Saved 1 window for 19140 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_19140.npy\n",
            "Saved 1 window for 19093 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_19093.npy\n",
            "Saved 1 window for 16273 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16273.npy\n",
            "Saved 1 window for 18184 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_18184.npy\n",
            "Saved 1 window for 18177 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_18177.npy\n",
            "Saved 1 window for 19088 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_19088.npy\n",
            "Saved 1 window for 16795 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_16795.npy\n",
            "Saved 1 window for 19090 → /content/drive/MyDrive/Models/nsr_100s/X_nsr_19090.npy\n",
            "\n",
            "Concatenating all NSR windows...\n",
            "Final NSR dataset saved: /content/drive/MyDrive/Models/nsr_100s/X_nsr_final.npy\n",
            "Final shape: (18, 25000)\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "NSR_PATH = \"/content/drive/MyDrive/nsrdb/\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Models/nsr_100s/\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "nsr_records = [\n",
        "    \"16272\",\"19830\",\"16539\",\"16773\",\"16786\",\"16265\",\"16420\",\"17453\",\n",
        "    \"17052\",\"16483\",\"19140\",\"19093\",\"16273\",\"18184\",\"18177\",\"19088\",\n",
        "    \"16795\",\"19090\"\n",
        "]\n",
        "\n",
        "FS = 250\n",
        "WINDOW_SEC = 100\n",
        "WINDOW_SIZE = FS * WINDOW_SEC\n",
        "\n",
        "saved_files = []\n",
        "\n",
        "print(\"Extracting first 100 seconds from each NSR record...\\n\")\n",
        "\n",
        "for rec_name in nsr_records:\n",
        "    try:\n",
        "        record_path = os.path.join(NSR_PATH, rec_name)\n",
        "        record = wfdb.rdrecord(record_path)\n",
        "        ecg = record.p_signal[:, 0]  # MLII lead\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {rec_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    if len(ecg) < WINDOW_SIZE:\n",
        "        print(f\"Skipping {rec_name}: ECG too short.\")\n",
        "        continue\n",
        "\n",
        "    # Take FIRST 100 seconds only\n",
        "    window = ecg[:WINDOW_SIZE].astype(np.float32)\n",
        "\n",
        "    out_file = os.path.join(OUTPUT_DIR, f\"X_nsr_{rec_name}.npy\")\n",
        "    np.save(out_file, window)\n",
        "    saved_files.append(out_file)\n",
        "\n",
        "    print(f\"Saved 1 window for {rec_name} → {out_file}\")\n",
        "\n",
        "# Step 2: Concatenate all windows into a single array\n",
        "print(\"\\nConcatenating all NSR windows...\")\n",
        "\n",
        "all_data = []\n",
        "for f in saved_files:\n",
        "    all_data.append(np.load(f))\n",
        "\n",
        "# Final shape → (num_records, 22500)\n",
        "X_final = np.stack(all_data, axis=0)\n",
        "\n",
        "final_file = os.path.join(OUTPUT_DIR, \"X_nsr_final.npy\")\n",
        "np.save(final_file, X_final)\n",
        "\n",
        "print(f\"Final NSR dataset saved: {final_file}\")\n",
        "print(f\"Final shape: {X_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sibJrskFWujp"
      },
      "source": [
        "Training autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6COPdQoXWwWx",
        "outputId": "14586565-d6c9-46cf-be0f-0dd5d4c41568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 | Loss: 0.544702\n",
            "Epoch 2/30 | Loss: 0.394604\n",
            "Epoch 3/30 | Loss: 0.263594\n",
            "Epoch 4/30 | Loss: 0.179080\n",
            "Epoch 5/30 | Loss: 0.143273\n",
            "Epoch 6/30 | Loss: 0.138603\n",
            "Epoch 7/30 | Loss: 0.144896\n",
            "Epoch 8/30 | Loss: 0.150907\n",
            "Epoch 9/30 | Loss: 0.154313\n",
            "Epoch 10/30 | Loss: 0.155710\n",
            "Epoch 11/30 | Loss: 0.155729\n",
            "Epoch 12/30 | Loss: 0.154678\n",
            "Epoch 13/30 | Loss: 0.152659\n",
            "Epoch 14/30 | Loss: 0.149690\n",
            "Epoch 15/30 | Loss: 0.145828\n",
            "Epoch 16/30 | Loss: 0.141420\n",
            "Epoch 17/30 | Loss: 0.137753\n",
            "Epoch 18/30 | Loss: 0.138075\n",
            "Epoch 19/30 | Loss: 0.142776\n",
            "Epoch 20/30 | Loss: 0.141697\n",
            "Epoch 21/30 | Loss: 0.137408\n",
            "Epoch 22/30 | Loss: 0.135102\n",
            "Epoch 23/30 | Loss: 0.134796\n",
            "Epoch 24/30 | Loss: 0.135178\n",
            "Epoch 25/30 | Loss: 0.135464\n",
            "Epoch 26/30 | Loss: 0.135354\n",
            "Epoch 27/30 | Loss: 0.134789\n",
            "Epoch 28/30 | Loss: 0.133821\n",
            "Epoch 29/30 | Loss: 0.132577\n",
            "Epoch 30/30 | Loss: 0.131251\n",
            "10 second NSR LSTM autoencoder trained and saved.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Hyperparameters ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 32          # Now possible because sequences are shorter\n",
        "EPOCHS = 30\n",
        "LR = 0.001\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------ Dataset ------------------\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx][:, None]\n",
        "        return x, x\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load NSR Windows ------------------\n",
        "X_nsr = np.load(\"/content/drive/MyDrive/Models/nsr_100s/X_nsr_final.npy\")\n",
        "\n",
        "dataset = ECGDataset(X_nsr)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# ------------------ Training Setup ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ------------------ Training Loop ------------------\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        batch_x = batch_x.to(DEVICE)\n",
        "        batch_y = batch_y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataset)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# ------------------ Save Model ------------------\n",
        "MODEL_DIR = \"/content/drive/MyDrive/Models/NSR/\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "torch.save(\n",
        "    model.state_dict(),\n",
        "    os.path.join(MODEL_DIR, \"LSTM_NSR_autoencoder_10s.pth\")\n",
        ")\n",
        "\n",
        "print(\"10 second NSR LSTM autoencoder trained and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcCdfSNWKAth"
      },
      "source": [
        "Computing errors and compare:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkF93iETKGF-",
        "outputId": "9cbc8517-e7fb-4802-f0b8-1cf6fbaafc16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using sample CUDB record: cu01\n",
            "VTAC starts at sample: 53546\n",
            "VTAC starts at second: 214\n",
            "\n",
            "Computing reconstruction errors (1 window per second)...\n",
            "\n",
            "\n",
            "=== Reconstruction Error Comparison (SECONDS BASED) ===\n",
            "\n",
            "Normal rhythm windows (Every 5th second from the start of sample):\n",
            "Normal sec 01: 0.110882\n",
            "Normal sec 06: 0.113646\n",
            "Normal sec 11: 0.121762\n",
            "Normal sec 16: 0.110831\n",
            "Normal sec 21: 0.109084\n",
            "Normal sec 26: 0.124548\n",
            "Normal sec 31: 0.142751\n",
            "Normal sec 36: 0.141155\n",
            "Normal sec 41: 0.117428\n",
            "Normal sec 46: 0.113749\n",
            "Normal sec 51: 0.123342\n",
            "Normal sec 56: 0.134570\n",
            "Normal sec 61: 0.135457\n",
            "Normal sec 66: 0.136573\n",
            "Normal sec 71: 0.155528\n",
            "Normal sec 76: 0.142900\n",
            "Normal sec 81: 0.104496\n",
            "Normal sec 86: 0.107985\n",
            "Normal sec 91: 0.123310\n",
            "Normal sec 96: 0.131477\n",
            "Normal sec 101: 0.135297\n",
            "Normal sec 106: 0.124795\n",
            "Normal sec 111: 0.124332\n",
            "Normal sec 116: 0.126150\n",
            "Normal sec 121: 0.196156\n",
            "Normal sec 126: 0.702925\n",
            "Normal sec 131: 0.690753\n",
            "Normal sec 136: 0.165053\n",
            "Normal sec 141: 0.099636\n",
            "Normal sec 146: 0.112784\n",
            "Normal sec 151: 0.116049\n",
            "Normal sec 156: 0.115069\n",
            "Normal sec 161: 0.117197\n",
            "Normal sec 166: 0.119369\n",
            "Normal sec 171: 0.103522\n",
            "Normal sec 176: 0.108700\n",
            "Normal sec 181: 0.127876\n",
            "Normal sec 186: 0.119740\n",
            "Normal sec 191: 0.109912\n",
            "Normal sec 196: 0.119020\n",
            "Normal sec 201: 0.128839\n",
            "Normal sec 206: 0.273245\n",
            "\n",
            "VTAC windows (100 seconds AFTER VTAC onset):\n",
            "VTAC sec 01: 0.455086\n",
            "VTAC sec 02: 0.491060\n",
            "VTAC sec 03: 0.522543\n",
            "VTAC sec 04: 0.545144\n",
            "VTAC sec 05: 0.579894\n",
            "VTAC sec 06: 0.567700\n",
            "VTAC sec 07: 0.562569\n",
            "VTAC sec 08: 0.539534\n",
            "VTAC sec 09: 0.516498\n",
            "VTAC sec 10: 0.500762\n",
            "VTAC sec 11: 0.471192\n",
            "VTAC sec 12: 0.441984\n",
            "VTAC sec 13: 0.414789\n",
            "VTAC sec 14: 0.402875\n",
            "VTAC sec 15: 0.400428\n",
            "VTAC sec 16: 0.396377\n",
            "VTAC sec 17: 0.385216\n",
            "VTAC sec 18: 0.377681\n",
            "VTAC sec 19: 0.362412\n",
            "VTAC sec 20: 0.345924\n",
            "VTAC sec 21: 0.323746\n",
            "VTAC sec 22: 0.294547\n",
            "VTAC sec 23: 0.260055\n",
            "VTAC sec 24: 0.213418\n",
            "VTAC sec 25: 0.163864\n",
            "VTAC sec 26: 0.148597\n",
            "VTAC sec 27: 0.130551\n",
            "VTAC sec 28: 0.107503\n",
            "VTAC sec 29: 0.096017\n",
            "VTAC sec 30: 0.094562\n",
            "VTAC sec 31: 0.100835\n",
            "VTAC sec 32: 0.094147\n",
            "VTAC sec 33: 0.085605\n",
            "VTAC sec 34: 0.080584\n",
            "VTAC sec 35: 0.066195\n",
            "VTAC sec 36: 0.061354\n",
            "VTAC sec 37: 0.067744\n",
            "VTAC sec 38: 0.075794\n",
            "VTAC sec 39: 0.078289\n",
            "VTAC sec 40: 0.085957\n",
            "VTAC sec 41: 0.096124\n",
            "VTAC sec 42: 0.105130\n",
            "VTAC sec 43: 0.108331\n",
            "VTAC sec 44: 0.105157\n",
            "VTAC sec 45: 0.106325\n",
            "VTAC sec 46: 0.101611\n",
            "VTAC sec 47: 0.124274\n",
            "VTAC sec 48: 0.127885\n",
            "VTAC sec 49: 0.124507\n",
            "VTAC sec 50: 0.116096\n",
            "VTAC sec 51: 0.099402\n",
            "VTAC sec 52: 0.099424\n",
            "VTAC sec 53: 0.097616\n",
            "VTAC sec 54: 0.109867\n",
            "VTAC sec 55: 0.114413\n",
            "VTAC sec 56: 0.127417\n",
            "VTAC sec 57: 0.112055\n",
            "VTAC sec 58: 0.118357\n",
            "VTAC sec 59: 0.141381\n",
            "VTAC sec 60: 0.145316\n",
            "VTAC sec 61: 0.148300\n",
            "VTAC sec 62: 0.142339\n",
            "VTAC sec 63: 0.142947\n",
            "VTAC sec 64: 0.145231\n",
            "VTAC sec 65: 0.156894\n",
            "VTAC sec 66: 0.144235\n",
            "VTAC sec 67: 0.147069\n",
            "VTAC sec 68: 0.143319\n",
            "VTAC sec 69: 0.135311\n",
            "VTAC sec 70: 0.135742\n",
            "VTAC sec 71: 0.134951\n",
            "VTAC sec 72: 0.137299\n",
            "VTAC sec 73: 0.136880\n",
            "VTAC sec 74: 0.124523\n",
            "VTAC sec 75: 0.102686\n",
            "VTAC sec 76: 0.101681\n",
            "VTAC sec 77: 0.094302\n",
            "VTAC sec 78: 0.088645\n",
            "VTAC sec 79: 0.082444\n",
            "VTAC sec 80: 0.082610\n",
            "VTAC sec 81: 0.084893\n",
            "VTAC sec 82: 0.087833\n",
            "VTAC sec 83: 0.085890\n",
            "VTAC sec 84: 0.091338\n",
            "VTAC sec 85: 0.117895\n",
            "VTAC sec 86: 0.184904\n",
            "VTAC sec 87: 0.283416\n",
            "VTAC sec 88: 0.400224\n",
            "VTAC sec 89: 0.533841\n",
            "VTAC sec 90: 0.689329\n",
            "VTAC sec 91: 0.857733\n",
            "VTAC sec 92: 1.038873\n",
            "VTAC sec 93: 1.212592\n",
            "VTAC sec 94: 1.342817\n",
            "VTAC sec 95: 1.444381\n",
            "VTAC sec 96: 1.499849\n",
            "VTAC sec 97: 1.482821\n",
            "VTAC sec 98: 1.398950\n",
            "VTAC sec 99: 1.270245\n",
            "VTAC sec 100: 1.111297\n",
            "\n",
            "--- Summary Statistics ---\n",
            "Normal mean: 0.155664\n",
            "Normal max:  0.702925\n",
            "VTAC mean:   0.319362\n",
            "VTAC min:    0.061354\n",
            "\n",
            "Separation gap (VTAC min − Normal max): -0.641571\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250                       # Sampling frequency (Hz)\n",
        "SEQ_LEN_SEC = 10              # Window length in seconds\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS     # Window length in samples\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)  # (1, T, 1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Load First CUDB Record ------------------\n",
        "records = sorted([f[:-4] for f in os.listdir(CUDB_PATH) if f.endswith(\".hea\")])\n",
        "rec_name = records[0]\n",
        "\n",
        "print(f\"\\nUsing sample CUDB record: {rec_name}\")\n",
        "\n",
        "record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "ecg = record.p_signal[:, 0]\n",
        "\n",
        "# ------------------ Find VTAC Start ------------------\n",
        "vt_start_sample = None\n",
        "for sym, samp in zip(ann.symbol, ann.sample):\n",
        "    if sym == '[':\n",
        "        vt_start_sample = samp\n",
        "        break\n",
        "\n",
        "if vt_start_sample is None:\n",
        "    raise RuntimeError(\"No VTAC '[' annotation found in this record\")\n",
        "\n",
        "vt_start_second = vt_start_sample // FS\n",
        "\n",
        "print(f\"VTAC starts at sample: {vt_start_sample}\")\n",
        "print(f\"VTAC starts at second: {vt_start_second}\")\n",
        "\n",
        "# ------------------ Define Analysis Region (SECONDS) ------------------\n",
        "NORMAL_SEC_BEFORE = 210\n",
        "VT_SEC_AFTER = 100\n",
        "\n",
        "normal_seconds = list(range(vt_start_second - NORMAL_SEC_BEFORE, vt_start_second,5))\n",
        "vt_seconds = list(range(vt_start_second, vt_start_second + VT_SEC_AFTER))\n",
        "\n",
        "errors_normal = []\n",
        "errors_vt = []\n",
        "\n",
        "print(\"\\nComputing reconstruction errors (1 window per second)...\\n\")\n",
        "\n",
        "# ------------------ Normal Windows ------------------\n",
        "for sec in normal_seconds:\n",
        "    start_sample = sec * FS\n",
        "    end_sample = start_sample + SEQ_LEN\n",
        "\n",
        "    if start_sample < 0 or end_sample > len(ecg):\n",
        "        continue\n",
        "\n",
        "    window = ecg[start_sample:end_sample].astype(np.float32)\n",
        "    err = reconstruction_error(window)\n",
        "    errors_normal.append(err)\n",
        "\n",
        "# ------------------ VTAC Windows ------------------\n",
        "for sec in vt_seconds:\n",
        "    start_sample = sec * FS\n",
        "    end_sample = start_sample + SEQ_LEN\n",
        "\n",
        "    if end_sample > len(ecg):\n",
        "        continue\n",
        "\n",
        "    window = ecg[start_sample:end_sample].astype(np.float32)\n",
        "    err = reconstruction_error(window)\n",
        "    errors_vt.append(err)\n",
        "\n",
        "# ------------------ Print Comparison ------------------\n",
        "print(\"\\n=== Reconstruction Error Comparison (SECONDS BASED) ===\\n\")\n",
        "\n",
        "print(\"Normal rhythm windows (Every 5th second from the start of sample):\")\n",
        "for i, e in enumerate(errors_normal):\n",
        "    print(f\"Normal sec {(5*i)+1:02d}: {e:.6f}\")\n",
        "\n",
        "print(\"\\nVTAC windows (100 seconds AFTER VTAC onset):\")\n",
        "for i, e in enumerate(errors_vt):\n",
        "    print(f\"VTAC sec {i+1:02d}: {e:.6f}\")\n",
        "\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(f\"Normal mean: {np.mean(errors_normal):.6f}\")\n",
        "print(f\"Normal max:  {np.max(errors_normal):.6f}\")\n",
        "print(f\"VTAC mean:   {np.mean(errors_vt):.6f}\")\n",
        "print(f\"VTAC min:    {np.min(errors_vt):.6f}\")\n",
        "\n",
        "gap = np.min(errors_vt) - np.max(errors_normal)\n",
        "print(f\"\\nSeparation gap (VTAC min − Normal max): {gap:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUAaxGvdRvKe"
      },
      "source": [
        "Same code but for cu02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwAthfUPRu8d",
        "outputId": "68c06301-34b9-4a06-f197-9ca4e6863f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using sample CUDB record: cu03\n",
            "VTAC starts at sample: 116431\n",
            "VTAC starts at second: 465\n",
            "\n",
            "Computing reconstruction errors (1 window per second)...\n",
            "\n",
            "sec=465, min=-1.7300000190734863, max=1.8600000143051147, nan=False, inf=False\n",
            "sec=466, min=-1.7300000190734863, max=1.5700000524520874, nan=False, inf=False\n",
            "sec=467, min=-1.7200000286102295, max=1.5700000524520874, nan=False, inf=False\n",
            "sec=468, min=nan, max=nan, nan=True, inf=False\n",
            "sec=469, min=nan, max=nan, nan=True, inf=False\n",
            "sec=470, min=nan, max=nan, nan=True, inf=False\n",
            "sec=471, min=nan, max=nan, nan=True, inf=False\n",
            "sec=472, min=nan, max=nan, nan=True, inf=False\n",
            "sec=473, min=nan, max=nan, nan=True, inf=False\n",
            "sec=474, min=nan, max=nan, nan=True, inf=False\n",
            "sec=475, min=nan, max=nan, nan=True, inf=False\n",
            "sec=476, min=nan, max=nan, nan=True, inf=False\n",
            "sec=477, min=nan, max=nan, nan=True, inf=False\n",
            "sec=478, min=-0.625, max=1.684999942779541, nan=False, inf=False\n",
            "sec=479, min=-0.625, max=1.684999942779541, nan=False, inf=False\n",
            "sec=480, min=-0.6200000047683716, max=1.684999942779541, nan=False, inf=False\n",
            "sec=481, min=-0.6200000047683716, max=1.684999942779541, nan=False, inf=False\n",
            "sec=482, min=-0.6100000143051147, max=1.684999942779541, nan=False, inf=False\n",
            "sec=483, min=-0.6100000143051147, max=1.684999942779541, nan=False, inf=False\n",
            "sec=484, min=-0.6100000143051147, max=1.684999942779541, nan=False, inf=False\n",
            "sec=485, min=-0.6000000238418579, max=1.684999942779541, nan=False, inf=False\n",
            "sec=486, min=-0.6000000238418579, max=1.684999942779541, nan=False, inf=False\n",
            "sec=487, min=-0.6000000238418579, max=1.684999942779541, nan=False, inf=False\n",
            "sec=488, min=-0.7200000286102295, max=0.6850000023841858, nan=False, inf=False\n",
            "sec=489, min=-0.7200000286102295, max=1.0, nan=False, inf=False\n",
            "sec=490, min=-0.7200000286102295, max=1.3550000190734863, nan=False, inf=False\n",
            "sec=491, min=-1.100000023841858, max=1.3550000190734863, nan=False, inf=False\n",
            "sec=492, min=-1.100000023841858, max=1.3550000190734863, nan=False, inf=False\n",
            "sec=493, min=-1.100000023841858, max=1.3550000190734863, nan=False, inf=False\n",
            "sec=494, min=-1.100000023841858, max=2.194999933242798, nan=False, inf=False\n",
            "sec=495, min=-1.1399999856948853, max=2.194999933242798, nan=False, inf=False\n",
            "sec=496, min=-1.1399999856948853, max=2.194999933242798, nan=False, inf=False\n",
            "sec=497, min=-1.1399999856948853, max=2.194999933242798, nan=False, inf=False\n",
            "sec=498, min=-1.1399999856948853, max=2.194999933242798, nan=False, inf=False\n",
            "\n",
            "=== Reconstruction Error Comparison (SECONDS BASED) ===\n",
            "\n",
            "Normal rhythm windows (Every 5th second from the start of sample):\n",
            "Normal sec 16: 1.975261\n",
            "Normal sec 21: 0.592131\n",
            "Normal sec 26: 0.270563\n",
            "Normal sec 31: 0.181232\n",
            "Normal sec 36: 0.151489\n",
            "Normal sec 41: 0.144869\n",
            "Normal sec 46: 0.149232\n",
            "Normal sec 51: 0.155795\n",
            "Normal sec 56: 0.156049\n",
            "Normal sec 61: 0.359560\n",
            "Normal sec 66: 0.395126\n",
            "Normal sec 71: 0.193466\n",
            "Normal sec 76: 0.158208\n",
            "Normal sec 81: 0.151320\n",
            "Normal sec 86: 0.145981\n",
            "Normal sec 91: 0.145029\n",
            "Normal sec 96: 0.160626\n",
            "Normal sec 101: 0.161711\n",
            "Normal sec 106: 0.148226\n",
            "Normal sec 111: 0.145879\n",
            "Normal sec 116: 0.164135\n",
            "Normal sec 121: 0.212853\n",
            "Normal sec 126: 0.207708\n",
            "Normal sec 131: 0.167053\n",
            "Normal sec 136: 0.151349\n",
            "Normal sec 141: 0.140113\n",
            "Normal sec 146: 0.143745\n",
            "Normal sec 151: 0.142786\n",
            "Normal sec 156: 0.143292\n",
            "Normal sec 161: 0.143286\n",
            "Normal sec 166: 0.141424\n",
            "Normal sec 171: 0.142604\n",
            "Normal sec 176: 0.136273\n",
            "Normal sec 181: 0.141484\n",
            "Normal sec 186: 0.152713\n",
            "Normal sec 191: 0.148152\n",
            "Normal sec 196: 0.174589\n",
            "Normal sec 201: 0.171458\n",
            "Normal sec 206: 0.142492\n",
            "Normal sec 211: 0.145052\n",
            "Normal sec 216: 0.138776\n",
            "Normal sec 221: 0.142939\n",
            "Normal sec 226: 0.146451\n",
            "Normal sec 231: 0.148779\n",
            "Normal sec 236: 0.147501\n",
            "Normal sec 241: 0.136971\n",
            "Normal sec 246: 0.139561\n",
            "Normal sec 251: 0.143416\n",
            "Normal sec 256: 0.160182\n",
            "Normal sec 261: 2.028150\n",
            "Normal sec 266: 2.013649\n",
            "Normal sec 271: 0.156750\n",
            "Normal sec 276: 0.163270\n",
            "Normal sec 281: 0.163398\n",
            "Normal sec 286: 0.164536\n",
            "Normal sec 291: 0.162009\n",
            "Normal sec 296: 0.170933\n",
            "Normal sec 301: 0.167670\n",
            "Normal sec 306: 0.173679\n",
            "Normal sec 311: 0.172469\n",
            "Normal sec 316: 0.171328\n",
            "Normal sec 321: 0.200071\n",
            "Normal sec 326: 0.203725\n",
            "Normal sec 331: 0.180451\n",
            "Normal sec 336: 0.172584\n",
            "Normal sec 341: 0.176684\n",
            "Normal sec 346: 0.164909\n",
            "Normal sec 351: 0.155264\n",
            "Normal sec 356: 0.153130\n",
            "Normal sec 361: 0.156305\n",
            "Normal sec 366: 0.154786\n",
            "Normal sec 371: 0.163756\n",
            "Normal sec 376: 0.166856\n",
            "Normal sec 381: 0.164396\n",
            "Normal sec 386: 0.182121\n",
            "Normal sec 391: 0.184685\n",
            "Normal sec 396: 0.170577\n",
            "Normal sec 401: 0.163835\n",
            "Normal sec 406: 0.160395\n",
            "Normal sec 411: 0.169019\n",
            "Normal sec 416: 0.179268\n",
            "Normal sec 421: 0.174754\n",
            "Normal sec 426: 0.167044\n",
            "Normal sec 431: 0.164007\n",
            "Normal sec 436: 0.162730\n",
            "Normal sec 441: 0.184244\n",
            "Normal sec 446: 0.209477\n",
            "Normal sec 451: 0.232819\n",
            "Normal sec 456: 0.251341\n",
            "Normal sec 461: 0.363928\n",
            "\n",
            "VTAC windows (100 seconds AFTER VTAC onset):\n",
            "VTAC sec 01: 0.541432\n",
            "VTAC sec 02: 0.567031\n",
            "VTAC sec 03: 0.535527\n",
            "VTAC sec 04: nan\n",
            "VTAC sec 05: nan\n",
            "VTAC sec 06: nan\n",
            "VTAC sec 07: nan\n",
            "VTAC sec 08: nan\n",
            "VTAC sec 09: nan\n",
            "VTAC sec 10: nan\n",
            "VTAC sec 11: nan\n",
            "VTAC sec 12: nan\n",
            "VTAC sec 13: nan\n",
            "VTAC sec 14: 0.134212\n",
            "VTAC sec 15: 0.131495\n",
            "VTAC sec 16: 0.123289\n",
            "VTAC sec 17: 0.118480\n",
            "VTAC sec 18: 0.109340\n",
            "VTAC sec 19: 0.101014\n",
            "VTAC sec 20: 0.105087\n",
            "VTAC sec 21: 0.092264\n",
            "VTAC sec 22: 0.095345\n",
            "VTAC sec 23: 0.089786\n",
            "VTAC sec 24: 0.065147\n",
            "VTAC sec 25: 0.073859\n",
            "VTAC sec 26: 0.103960\n",
            "VTAC sec 27: 0.112394\n",
            "VTAC sec 28: 0.122049\n",
            "VTAC sec 29: 0.132870\n",
            "VTAC sec 30: 0.153332\n",
            "VTAC sec 31: 0.170721\n",
            "VTAC sec 32: 0.173823\n",
            "VTAC sec 33: 0.193146\n",
            "VTAC sec 34: 0.224882\n",
            "\n",
            "--- Summary Statistics ---\n",
            "Normal mean: 0.237777\n",
            "Normal max:  2.028150\n",
            "VTAC mean:   nan\n",
            "VTAC min:    nan\n",
            "\n",
            "Separation gap (VTAC min − Normal max): nan\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250                       # Sampling frequency (Hz)\n",
        "SEQ_LEN_SEC = 10              # Window length in seconds\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS     # Window length in samples\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)  # (1, T, 1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Load First CUDB Record ------------------\n",
        "records = sorted([f[:-4] for f in os.listdir(CUDB_PATH) if f.endswith(\".hea\")])\n",
        "rec_name = records[2]\n",
        "\n",
        "print(f\"\\nUsing sample CUDB record: {rec_name}\")\n",
        "\n",
        "record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "ecg = record.p_signal[:, 0]\n",
        "\n",
        "# ------------------ Find VTAC Start ------------------\n",
        "vt_start_sample = None\n",
        "for sym, samp in zip(ann.symbol, ann.sample):\n",
        "    if sym == '[':\n",
        "        vt_start_sample = samp\n",
        "        break\n",
        "\n",
        "if vt_start_sample is None:\n",
        "    raise RuntimeError(\"No VTAC '[' annotation found in this record\")\n",
        "\n",
        "vt_start_second = vt_start_sample // FS\n",
        "\n",
        "print(f\"VTAC starts at sample: {vt_start_sample}\")\n",
        "print(f\"VTAC starts at second: {vt_start_second}\")\n",
        "\n",
        "# ------------------ Define Analysis Region (SECONDS) ------------------\n",
        "NORMAL_SEC_BEFORE = 450\n",
        "VT_SEC_AFTER = 100\n",
        "\n",
        "normal_seconds = list(range(vt_start_second - NORMAL_SEC_BEFORE, vt_start_second,5))\n",
        "vt_seconds = list(range(vt_start_second, vt_start_second + VT_SEC_AFTER))\n",
        "\n",
        "errors_normal = []\n",
        "errors_vt = []\n",
        "\n",
        "print(\"\\nComputing reconstruction errors (1 window per second)...\\n\")\n",
        "\n",
        "# ------------------ Normal Windows ------------------\n",
        "for sec in normal_seconds:\n",
        "    start_sample = sec * FS\n",
        "    end_sample = start_sample + SEQ_LEN\n",
        "\n",
        "    if start_sample < 0 or end_sample > len(ecg):\n",
        "        continue\n",
        "\n",
        "    window = ecg[start_sample:end_sample].astype(np.float32)\n",
        "    err = reconstruction_error(window)\n",
        "    errors_normal.append(err)\n",
        "\n",
        "# ------------------ VTAC Windows ------------------\n",
        "for sec in vt_seconds:\n",
        "    start_sample = sec * FS\n",
        "    end_sample = start_sample + SEQ_LEN\n",
        "\n",
        "    if end_sample > len(ecg):\n",
        "        continue\n",
        "\n",
        "    window = ecg[start_sample:end_sample].astype(np.float32)\n",
        "    print(f\"sec={sec}, min={window.min()}, max={window.max()}, nan={np.isnan(window).any()}, inf={np.isinf(window).any()}\")\n",
        "    err = reconstruction_error(window)\n",
        "    errors_vt.append(err)\n",
        "\n",
        "# ------------------ Print Comparison ------------------\n",
        "print(\"\\n=== Reconstruction Error Comparison (SECONDS BASED) ===\\n\")\n",
        "\n",
        "print(\"Normal rhythm windows (Every 5th second from the start of sample):\")\n",
        "for i, e in enumerate(errors_normal):\n",
        "    print(f\"Normal sec {(5*i)+1+vt_start_second - NORMAL_SEC_BEFORE:02d}: {e:.6f}\")\n",
        "\n",
        "print(\"\\nVTAC windows (100 seconds AFTER VTAC onset):\")\n",
        "for i, e in enumerate(errors_vt):\n",
        "    print(f\"VTAC sec {i+1:02d}: {e:.6f}\")\n",
        "\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(f\"Normal mean: {np.mean(errors_normal):.6f}\")\n",
        "print(f\"Normal max:  {np.max(errors_normal):.6f}\")\n",
        "print(f\"VTAC mean:   {np.mean(errors_vt):.6f}\")\n",
        "print(f\"VTAC min:    {np.min(errors_vt):.6f}\")\n",
        "\n",
        "gap = np.min(errors_vt) - np.max(errors_normal)\n",
        "print(f\"\\nSeparation gap (VTAC min − Normal max): {gap:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUuOGXlFtq4Z"
      },
      "source": [
        "CU04"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc7B7sx6tsEU",
        "outputId": "85631caa-73bb-4ae7-df69-fc73ecffcd85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using sample CUDB record: cu04\n",
            "VTAC starts at sample: 38828\n",
            "VTAC starts at second: 155\n",
            "\n",
            "Computing reconstruction errors (1 window per second)...\n",
            "\n",
            "\n",
            "=== Reconstruction Error Comparison (SECONDS BASED) ===\n",
            "\n",
            "Normal rhythm windows (Every 5th second from the start of sample):\n",
            "Normal sec 16: 0.084469\n",
            "Normal sec 21: 0.090454\n",
            "Normal sec 26: 0.087101\n",
            "Normal sec 31: 0.106273\n",
            "Normal sec 36: 0.105585\n",
            "Normal sec 41: 0.090373\n",
            "Normal sec 46: 0.091751\n",
            "Normal sec 51: 0.091928\n",
            "Normal sec 56: 0.090845\n",
            "Normal sec 61: 0.079052\n",
            "Normal sec 66: 0.067533\n",
            "Normal sec 71: 0.080302\n",
            "Normal sec 76: 0.089481\n",
            "Normal sec 81: 0.073398\n",
            "Normal sec 86: 0.078411\n",
            "Normal sec 91: 0.079195\n",
            "Normal sec 96: 0.080681\n",
            "Normal sec 101: 0.079059\n",
            "Normal sec 106: 0.087641\n",
            "Normal sec 111: 0.090708\n",
            "Normal sec 116: 0.075922\n",
            "Normal sec 121: 0.111050\n",
            "Normal sec 126: 0.110641\n",
            "Normal sec 131: 0.120729\n",
            "Normal sec 136: 0.146187\n",
            "Normal sec 141: 0.176892\n",
            "Normal sec 146: 0.254189\n",
            "Normal sec 151: 0.170705\n",
            "\n",
            "VTAC windows (100 seconds AFTER VTAC onset):\n",
            "VTAC sec 01: 0.107826\n",
            "VTAC sec 02: 0.100005\n",
            "VTAC sec 03: 0.106951\n",
            "VTAC sec 04: 0.120356\n",
            "VTAC sec 05: 0.122979\n",
            "VTAC sec 06: 0.123315\n",
            "VTAC sec 07: 0.127516\n",
            "VTAC sec 08: 0.121673\n",
            "VTAC sec 09: 0.118711\n",
            "VTAC sec 10: 0.100358\n",
            "VTAC sec 11: 0.099707\n",
            "VTAC sec 12: 0.099981\n",
            "VTAC sec 13: 0.095116\n",
            "VTAC sec 14: 0.083031\n",
            "VTAC sec 15: 0.082296\n",
            "VTAC sec 16: 0.083580\n",
            "VTAC sec 17: 0.080591\n",
            "VTAC sec 18: 0.073161\n",
            "VTAC sec 19: 0.068658\n",
            "VTAC sec 20: 0.070020\n",
            "VTAC sec 21: 0.064311\n",
            "VTAC sec 22: 0.065148\n",
            "VTAC sec 23: 0.067730\n",
            "VTAC sec 24: 0.068387\n",
            "VTAC sec 25: 0.069349\n",
            "VTAC sec 26: 0.079431\n",
            "VTAC sec 27: 0.086092\n",
            "VTAC sec 28: 0.091857\n",
            "VTAC sec 29: 0.085755\n",
            "VTAC sec 30: 0.083235\n",
            "VTAC sec 31: 0.081151\n",
            "VTAC sec 32: 0.087947\n",
            "VTAC sec 33: 0.088704\n",
            "VTAC sec 34: 0.091635\n",
            "VTAC sec 35: 0.091048\n",
            "VTAC sec 36: 0.088070\n",
            "VTAC sec 37: 0.088152\n",
            "VTAC sec 38: 0.084626\n",
            "VTAC sec 39: 0.093236\n",
            "VTAC sec 40: 0.096085\n",
            "VTAC sec 41: 0.108802\n",
            "VTAC sec 42: 0.109254\n",
            "VTAC sec 43: 0.108403\n",
            "VTAC sec 44: 0.118533\n",
            "VTAC sec 45: 0.122803\n",
            "VTAC sec 46: 0.273566\n",
            "VTAC sec 47: 0.487966\n",
            "VTAC sec 48: 1.399761\n",
            "VTAC sec 49: 1.905016\n",
            "VTAC sec 50: 2.160502\n",
            "VTAC sec 51: 2.182159\n",
            "VTAC sec 52: 2.229245\n",
            "VTAC sec 53: 2.261486\n",
            "VTAC sec 54: 2.266490\n",
            "VTAC sec 55: 2.298203\n",
            "VTAC sec 56: 2.144643\n",
            "VTAC sec 57: 1.962120\n",
            "VTAC sec 58: 1.068895\n",
            "VTAC sec 59: 0.562476\n",
            "VTAC sec 60: 0.314949\n",
            "VTAC sec 61: 0.295368\n",
            "VTAC sec 62: 0.263850\n",
            "VTAC sec 63: 0.233419\n",
            "VTAC sec 64: 0.227542\n",
            "VTAC sec 65: 0.238468\n",
            "VTAC sec 66: 0.244278\n",
            "VTAC sec 67: 0.218012\n",
            "VTAC sec 68: 0.202846\n",
            "VTAC sec 69: 0.202994\n",
            "VTAC sec 70: 0.204096\n",
            "VTAC sec 71: 0.206425\n",
            "VTAC sec 72: 0.194134\n",
            "VTAC sec 73: 0.194227\n",
            "VTAC sec 74: 0.184273\n",
            "VTAC sec 75: 0.146523\n",
            "VTAC sec 76: 0.147611\n",
            "VTAC sec 77: 0.137205\n",
            "VTAC sec 78: 0.143414\n",
            "VTAC sec 79: 0.446591\n",
            "VTAC sec 80: 0.836900\n",
            "VTAC sec 81: 1.330277\n",
            "VTAC sec 82: 1.546673\n",
            "VTAC sec 83: 1.593369\n",
            "VTAC sec 84: 1.667347\n",
            "VTAC sec 85: 1.680387\n",
            "VTAC sec 86: 1.693309\n",
            "VTAC sec 87: 1.701585\n",
            "VTAC sec 88: 1.735032\n",
            "VTAC sec 89: 1.466085\n",
            "VTAC sec 90: 1.107440\n",
            "VTAC sec 91: 0.628169\n",
            "VTAC sec 92: 0.415382\n",
            "VTAC sec 93: 0.396440\n",
            "VTAC sec 94: 0.431769\n",
            "VTAC sec 95: 0.464998\n",
            "VTAC sec 96: 0.484183\n",
            "VTAC sec 97: 0.494469\n",
            "VTAC sec 98: 0.478872\n",
            "VTAC sec 99: 0.475901\n",
            "VTAC sec 100: 0.460707\n",
            "\n",
            "--- Summary Statistics ---\n",
            "Normal mean: 0.103234\n",
            "Normal max:  0.254189\n",
            "VTAC mean:   0.529436\n",
            "VTAC min:    0.064311\n",
            "\n",
            "Separation gap (VTAC min − Normal max): -0.189878\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250                       # Sampling frequency (Hz)\n",
        "SEQ_LEN_SEC = 10              # Window length in seconds\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS     # Window length in samples\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)  # (1, T, 1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Load First CUDB Record ------------------\n",
        "records = sorted([f[:-4] for f in os.listdir(CUDB_PATH) if f.endswith(\".hea\")])\n",
        "rec_name = records[3]\n",
        "\n",
        "print(f\"\\nUsing sample CUDB record: {rec_name}\")\n",
        "\n",
        "record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "ecg = record.p_signal[:, 0]\n",
        "\n",
        "# ------------------ Find VTAC Start ------------------\n",
        "vt_start_sample = None\n",
        "for sym, samp in zip(ann.symbol, ann.sample):\n",
        "    if sym == '[':\n",
        "        vt_start_sample = samp\n",
        "        break\n",
        "\n",
        "if vt_start_sample is None:\n",
        "    raise RuntimeError(\"No VTAC '[' annotation found in this record\")\n",
        "\n",
        "vt_start_second = vt_start_sample // FS\n",
        "\n",
        "print(f\"VTAC starts at sample: {vt_start_sample}\")\n",
        "print(f\"VTAC starts at second: {vt_start_second}\")\n",
        "\n",
        "# ------------------ Define Analysis Region (SECONDS) ------------------\n",
        "NORMAL_SEC_BEFORE = 140\n",
        "VT_SEC_AFTER = 100\n",
        "\n",
        "normal_seconds = list(range(vt_start_second - NORMAL_SEC_BEFORE, vt_start_second,5))\n",
        "vt_seconds = list(range(vt_start_second, vt_start_second + VT_SEC_AFTER))\n",
        "\n",
        "errors_normal = []\n",
        "errors_vt = []\n",
        "\n",
        "print(\"\\nComputing reconstruction errors (1 window per second)...\\n\")\n",
        "\n",
        "# ------------------ Normal Windows ------------------\n",
        "for sec in normal_seconds:\n",
        "    start_sample = sec * FS\n",
        "    end_sample = start_sample + SEQ_LEN\n",
        "\n",
        "    if start_sample < 0 or end_sample > len(ecg):\n",
        "        continue\n",
        "\n",
        "    window = ecg[start_sample:end_sample].astype(np.float32)\n",
        "    err = reconstruction_error(window)\n",
        "    errors_normal.append(err)\n",
        "\n",
        "# ------------------ VTAC Windows ------------------\n",
        "for sec in vt_seconds:\n",
        "    start_sample = sec * FS\n",
        "    end_sample = start_sample + SEQ_LEN\n",
        "\n",
        "    if end_sample > len(ecg):\n",
        "        continue\n",
        "\n",
        "    window = ecg[start_sample:end_sample].astype(np.float32)\n",
        "    err = reconstruction_error(window)\n",
        "    errors_vt.append(err)\n",
        "\n",
        "# ------------------ Print Comparison ------------------\n",
        "print(\"\\n=== Reconstruction Error Comparison (SECONDS BASED) ===\\n\")\n",
        "\n",
        "print(\"Normal rhythm windows (Every 5th second from the start of sample):\")\n",
        "for i, e in enumerate(errors_normal):\n",
        "    print(f\"Normal sec {(5*i)+1+vt_start_second - NORMAL_SEC_BEFORE:02d}: {e:.6f}\")\n",
        "\n",
        "print(\"\\nVTAC windows (100 seconds AFTER VTAC onset):\")\n",
        "for i, e in enumerate(errors_vt):\n",
        "    print(f\"VTAC sec {i+1:02d}: {e:.6f}\")\n",
        "\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(f\"Normal mean: {np.mean(errors_normal):.6f}\")\n",
        "print(f\"Normal max:  {np.max(errors_normal):.6f}\")\n",
        "print(f\"VTAC mean:   {np.mean(errors_vt):.6f}\")\n",
        "print(f\"VTAC min:    {np.min(errors_vt):.6f}\")\n",
        "\n",
        "gap = np.min(errors_vt) - np.max(errors_normal)\n",
        "print(f\"\\nSeparation gap (VTAC min − Normal max): {gap:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxJvINf126q0"
      },
      "source": [
        "CU05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mztsBtQg27nE",
        "outputId": "060e1495-c3fb-4d61-f832-64373bdd70b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using sample CUDB record: cu05\n",
            "VTAC starts at sample: 89692\n",
            "VTAC starts at second: 358\n",
            "\n",
            "Computing reconstruction errors (1 window per second)...\n",
            "\n",
            "\n",
            "=== Reconstruction Error Comparison (SECONDS BASED) ===\n",
            "\n",
            "Normal rhythm windows (Every 5th second from the start of sample):\n",
            "Normal sec 19: 0.173416\n",
            "Normal sec 24: 0.178039\n",
            "Normal sec 29: 0.283933\n",
            "Normal sec 34: 0.280233\n",
            "Normal sec 39: 0.229333\n",
            "Normal sec 44: 0.246109\n",
            "Normal sec 49: 0.225213\n",
            "Normal sec 54: 0.200392\n",
            "Normal sec 59: 0.201146\n",
            "Normal sec 64: 0.186454\n",
            "Normal sec 69: 0.195518\n",
            "Normal sec 74: 0.192837\n",
            "Normal sec 79: 0.258187\n",
            "Normal sec 84: 0.774505\n",
            "Normal sec 89: 0.739374\n",
            "Normal sec 94: 0.238098\n",
            "Normal sec 99: 0.160245\n",
            "Normal sec 104: 0.151998\n",
            "Normal sec 109: 0.152446\n",
            "Normal sec 114: 0.191862\n",
            "Normal sec 119: 0.212021\n",
            "Normal sec 124: 0.178774\n",
            "Normal sec 129: 0.255104\n",
            "Normal sec 134: 0.277484\n",
            "Normal sec 139: 0.808593\n",
            "Normal sec 144: 0.882127\n",
            "Normal sec 149: 0.304647\n",
            "Normal sec 154: 0.227879\n",
            "Normal sec 159: 0.176788\n",
            "Normal sec 164: 0.153027\n",
            "Normal sec 169: 0.201257\n",
            "Normal sec 174: 0.277968\n",
            "Normal sec 179: 0.258795\n",
            "Normal sec 184: 0.298569\n",
            "Normal sec 189: 1.355327\n",
            "Normal sec 194: 1.519265\n",
            "Normal sec 199: 0.533674\n",
            "Normal sec 204: 0.319244\n",
            "Normal sec 209: 0.428965\n",
            "Normal sec 214: 0.697456\n",
            "Normal sec 219: 0.639498\n",
            "Normal sec 224: 0.302536\n",
            "Normal sec 229: 0.139463\n",
            "Normal sec 234: 0.125563\n",
            "Normal sec 239: 0.129429\n",
            "Normal sec 244: 0.143715\n",
            "Normal sec 249: 0.169561\n",
            "Normal sec 254: 0.235347\n",
            "Normal sec 259: 0.238940\n",
            "Normal sec 264: 0.168678\n",
            "Normal sec 269: 0.140189\n",
            "Normal sec 274: 0.155851\n",
            "Normal sec 279: 0.167032\n",
            "Normal sec 284: 0.177662\n",
            "Normal sec 289: 0.264636\n",
            "Normal sec 294: 0.290075\n",
            "Normal sec 299: 0.184991\n",
            "Normal sec 304: 0.158715\n",
            "Normal sec 309: 0.192726\n",
            "Normal sec 314: 0.195766\n",
            "Normal sec 319: 0.186523\n",
            "Normal sec 324: 0.162916\n",
            "Normal sec 329: 0.150902\n",
            "Normal sec 334: 0.149237\n",
            "Normal sec 339: 0.190459\n",
            "Normal sec 344: 0.244000\n",
            "Normal sec 349: 0.328860\n",
            "Normal sec 354: 0.437180\n",
            "\n",
            "VTAC windows (100 seconds AFTER VTAC onset):\n",
            "VTAC sec 01: 0.435315\n",
            "VTAC sec 02: 0.419956\n",
            "VTAC sec 03: 0.359175\n",
            "VTAC sec 04: 0.342576\n",
            "VTAC sec 05: 0.299338\n",
            "VTAC sec 06: 0.312902\n",
            "VTAC sec 07: 0.404716\n",
            "VTAC sec 08: 0.591731\n",
            "VTAC sec 09: 0.798164\n",
            "VTAC sec 10: 0.961091\n",
            "VTAC sec 11: 1.148716\n",
            "VTAC sec 12: 1.203529\n",
            "VTAC sec 13: 1.264893\n",
            "VTAC sec 14: 1.394748\n",
            "VTAC sec 15: 1.416777\n",
            "VTAC sec 16: 1.430195\n",
            "VTAC sec 17: 1.389060\n",
            "VTAC sec 18: 1.161311\n",
            "VTAC sec 19: 0.975821\n",
            "VTAC sec 20: 0.899700\n",
            "VTAC sec 21: 0.804613\n",
            "VTAC sec 22: 0.848403\n",
            "VTAC sec 23: 0.793140\n",
            "VTAC sec 24: 0.663888\n",
            "VTAC sec 25: 0.691067\n",
            "VTAC sec 26: 0.804012\n",
            "VTAC sec 27: 0.799196\n",
            "VTAC sec 28: 0.840202\n",
            "VTAC sec 29: 0.823701\n",
            "VTAC sec 30: 0.788569\n",
            "VTAC sec 31: 0.747814\n",
            "VTAC sec 32: 0.709464\n",
            "VTAC sec 33: 0.760636\n",
            "VTAC sec 34: 0.816498\n",
            "VTAC sec 35: 0.798806\n",
            "VTAC sec 36: 0.701870\n",
            "VTAC sec 37: 0.664693\n",
            "VTAC sec 38: 0.665014\n",
            "VTAC sec 39: 0.728085\n",
            "VTAC sec 40: 0.721855\n",
            "VTAC sec 41: 0.662506\n",
            "VTAC sec 42: 0.620977\n",
            "VTAC sec 43: 0.684541\n",
            "VTAC sec 44: 0.689484\n",
            "VTAC sec 45: 0.707052\n",
            "VTAC sec 46: 0.715543\n",
            "VTAC sec 47: 0.747713\n",
            "VTAC sec 48: 0.770999\n",
            "VTAC sec 49: 0.794877\n",
            "VTAC sec 50: 0.787417\n",
            "VTAC sec 51: 0.840196\n",
            "VTAC sec 52: 0.872227\n",
            "VTAC sec 53: 0.799352\n",
            "VTAC sec 54: 0.754248\n",
            "VTAC sec 55: 0.798049\n",
            "VTAC sec 56: 0.799885\n",
            "VTAC sec 57: 0.787426\n",
            "VTAC sec 58: 0.853020\n",
            "VTAC sec 59: 0.805303\n",
            "VTAC sec 60: 0.783646\n",
            "VTAC sec 61: 0.847365\n",
            "VTAC sec 62: 0.950113\n",
            "VTAC sec 63: 0.952253\n",
            "VTAC sec 64: 0.953577\n",
            "VTAC sec 65: 0.907943\n",
            "VTAC sec 66: 0.892908\n",
            "VTAC sec 67: 0.910883\n",
            "VTAC sec 68: 0.788520\n",
            "VTAC sec 69: 0.727554\n",
            "VTAC sec 70: 0.708210\n",
            "VTAC sec 71: 0.667520\n",
            "VTAC sec 72: 0.587178\n",
            "VTAC sec 73: 0.561859\n",
            "VTAC sec 74: 0.631419\n",
            "VTAC sec 75: 0.651513\n",
            "VTAC sec 76: 0.659154\n",
            "VTAC sec 77: 0.679326\n",
            "VTAC sec 78: 0.709359\n",
            "VTAC sec 79: 1.091395\n",
            "VTAC sec 80: 1.665762\n",
            "VTAC sec 81: nan\n",
            "VTAC sec 82: nan\n",
            "VTAC sec 83: nan\n",
            "VTAC sec 84: nan\n",
            "VTAC sec 85: nan\n",
            "VTAC sec 86: nan\n",
            "VTAC sec 87: nan\n",
            "VTAC sec 88: nan\n",
            "VTAC sec 89: nan\n",
            "VTAC sec 90: nan\n",
            "VTAC sec 91: 1.955705\n",
            "VTAC sec 92: 1.055168\n",
            "VTAC sec 93: 0.614533\n",
            "VTAC sec 94: 0.466135\n",
            "VTAC sec 95: 0.364256\n",
            "VTAC sec 96: 0.298454\n",
            "VTAC sec 97: 0.212607\n",
            "VTAC sec 98: 0.179253\n",
            "VTAC sec 99: 0.154650\n",
            "VTAC sec 100: 0.146007\n",
            "\n",
            "--- Summary Statistics ---\n",
            "Normal mean: 0.304364\n",
            "Normal max:  1.519265\n",
            "VTAC mean:   nan\n",
            "VTAC min:    nan\n",
            "\n",
            "Separation gap (VTAC min − Normal max): nan\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250                       # Sampling frequency (Hz)\n",
        "SEQ_LEN_SEC = 10              # Window length in seconds\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS     # Window length in samples\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)  # (1, T, 1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Load First CUDB Record ------------------\n",
        "records = sorted([f[:-4] for f in os.listdir(CUDB_PATH) if f.endswith(\".hea\")])\n",
        "rec_name = records[4]\n",
        "\n",
        "print(f\"\\nUsing sample CUDB record: {rec_name}\")\n",
        "\n",
        "record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "ecg = record.p_signal[:, 0]\n",
        "\n",
        "# ------------------ Find VTAC Start ------------------\n",
        "vt_start_sample = None\n",
        "for sym, samp in zip(ann.symbol, ann.sample):\n",
        "    if sym == '[':\n",
        "        vt_start_sample = samp\n",
        "        break\n",
        "\n",
        "if vt_start_sample is None:\n",
        "    raise RuntimeError(\"No VTAC '[' annotation found in this record\")\n",
        "\n",
        "vt_start_second = vt_start_sample // FS\n",
        "\n",
        "print(f\"VTAC starts at sample: {vt_start_sample}\")\n",
        "print(f\"VTAC starts at second: {vt_start_second}\")\n",
        "\n",
        "# ------------------ Define Analysis Region (SECONDS) ------------------\n",
        "NORMAL_SEC_BEFORE = 340\n",
        "VT_SEC_AFTER = 100\n",
        "\n",
        "normal_seconds = list(range(vt_start_second - NORMAL_SEC_BEFORE, vt_start_second,5))\n",
        "vt_seconds = list(range(vt_start_second, vt_start_second + VT_SEC_AFTER))\n",
        "\n",
        "errors_normal = []\n",
        "errors_vt = []\n",
        "\n",
        "print(\"\\nComputing reconstruction errors (1 window per second)...\\n\")\n",
        "\n",
        "# ------------------ Normal Windows ------------------\n",
        "for sec in normal_seconds:\n",
        "    start_sample = sec * FS\n",
        "    end_sample = start_sample + SEQ_LEN\n",
        "\n",
        "    if start_sample < 0 or end_sample > len(ecg):\n",
        "        continue\n",
        "\n",
        "    window = ecg[start_sample:end_sample].astype(np.float32)\n",
        "    err = reconstruction_error(window)\n",
        "    errors_normal.append(err)\n",
        "\n",
        "# ------------------ VTAC Windows ------------------\n",
        "for sec in vt_seconds:\n",
        "    start_sample = sec * FS\n",
        "    end_sample = start_sample + SEQ_LEN\n",
        "\n",
        "    if end_sample > len(ecg):\n",
        "        continue\n",
        "\n",
        "    window = ecg[start_sample:end_sample].astype(np.float32)\n",
        "    err = reconstruction_error(window)\n",
        "    errors_vt.append(err)\n",
        "\n",
        "# ------------------ Print Comparison ------------------\n",
        "print(\"\\n=== Reconstruction Error Comparison (SECONDS BASED) ===\\n\")\n",
        "\n",
        "print(\"Normal rhythm windows (Every 5th second from the start of sample):\")\n",
        "for i, e in enumerate(errors_normal):\n",
        "    print(f\"Normal sec {(5*i)+1+vt_start_second - NORMAL_SEC_BEFORE:02d}: {e:.6f}\")\n",
        "\n",
        "print(\"\\nVTAC windows (100 seconds AFTER VTAC onset):\")\n",
        "for i, e in enumerate(errors_vt):\n",
        "    print(f\"VTAC sec {i+1:02d}: {e:.6f}\")\n",
        "\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(f\"Normal mean: {np.mean(errors_normal):.6f}\")\n",
        "print(f\"Normal max:  {np.max(errors_normal):.6f}\")\n",
        "print(f\"VTAC mean:   {np.mean(errors_vt):.6f}\")\n",
        "print(f\"VTAC min:    {np.min(errors_vt):.6f}\")\n",
        "\n",
        "gap = np.min(errors_vt) - np.max(errors_normal)\n",
        "print(f\"\\nSeparation gap (VTAC min − Normal max): {gap:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfVXt14980x6"
      },
      "source": [
        "<h1>16/12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVM92Imf44hB"
      },
      "source": [
        "Actual test For all CUDB files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "598McfPc44RT",
        "outputId": "fb2a9663-ca0a-4b1b-aa59-1483927065ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Processing CUDB record: cu01\n",
            "==============================\n",
            "Annotated VTAC start time: 214 seconds\n",
            "Annotated VFIB start time: 214 seconds\n",
            "Model detected sustained abnormality at second: 136\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu02\n",
            "==============================\n",
            "No VTAC annotation found\n",
            "Annotated VFIB start time: 192 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu03\n",
            "==============================\n",
            "Annotated VTAC start time: 465 seconds\n",
            "Annotated VFIB start time: 465 seconds\n",
            "Model detected sustained abnormality at second: 16\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu04\n",
            "==============================\n",
            "Annotated VTAC start time: 155 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 211\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu05\n",
            "==============================\n",
            "Annotated VTAC start time: 358 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 89\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu06\n",
            "==============================\n",
            "Annotated VTAC start time: 184 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu07\n",
            "==============================\n",
            "Annotated VTAC start time: 182 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 151\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu08\n",
            "==============================\n",
            "Annotated VTAC start time: 426 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu09\n",
            "==============================\n",
            "Annotated VTAC start time: 239 seconds\n",
            "Annotated VFIB start time: 100 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu10\n",
            "==============================\n",
            "Annotated VTAC start time: 316 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 225\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu11\n",
            "==============================\n",
            "Annotated VTAC start time: 371 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu12\n",
            "==============================\n",
            "Annotated VTAC start time: 261 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu13\n",
            "==============================\n",
            "Annotated VTAC start time: 427 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 268\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu14\n",
            "==============================\n",
            "No VTAC annotation found\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 39\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu15\n",
            "==============================\n",
            "Annotated VTAC start time: 405 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 74\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu16\n",
            "==============================\n",
            "Annotated VTAC start time: 254 seconds\n",
            "Annotated VFIB start time: 168 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu17\n",
            "==============================\n",
            "Annotated VTAC start time: 382 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 393\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu18\n",
            "==============================\n",
            "Annotated VTAC start time: 334 seconds\n",
            "Annotated VFIB start time: 40 seconds\n",
            "Model detected sustained abnormality at second: 334\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu19\n",
            "==============================\n",
            "Annotated VTAC start time: 409 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 24\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu20\n",
            "==============================\n",
            "Annotated VTAC start time: 244 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 240\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu21\n",
            "==============================\n",
            "Annotated VTAC start time: 0 seconds\n",
            "Annotated VFIB start time: 52 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu22\n",
            "==============================\n",
            "Annotated VTAC start time: 338 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 340\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu23\n",
            "==============================\n",
            "Annotated VTAC start time: 334 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 336\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu24\n",
            "==============================\n",
            "Annotated VTAC start time: 356 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 369\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu25\n",
            "==============================\n",
            "Annotated VTAC start time: 421 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 27\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu26\n",
            "==============================\n",
            "Annotated VTAC start time: 145 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 170\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu27\n",
            "==============================\n",
            "Annotated VTAC start time: 451 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 123\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu28\n",
            "==============================\n",
            "Annotated VTAC start time: 496 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu29\n",
            "==============================\n",
            "Annotated VTAC start time: 378 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 57\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu30\n",
            "==============================\n",
            "Annotated VTAC start time: 27 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 26\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu31\n",
            "==============================\n",
            "Annotated VTAC start time: 494 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu32\n",
            "==============================\n",
            "Annotated VTAC start time: 443 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 80\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu33\n",
            "==============================\n",
            "Annotated VTAC start time: 405 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 457\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu34\n",
            "==============================\n",
            "Annotated VTAC start time: 118 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 116\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu35\n",
            "==============================\n",
            "Annotated VTAC start time: 483 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 223\n",
            "\n",
            "===================================\n",
            "           FINAL SUMMARY            \n",
            "===================================\n",
            "Record: cu01\n",
            "  VTAC Annotation: 214 sec\n",
            "  VFIB Annotation: 214 sec\n",
            "  Model Detection: 136 sec\n",
            "  Difference from VTAC: -78 sec\n",
            "  Difference from VFIB: -78 sec\n",
            "-----------------------------------\n",
            "Record: cu02\n",
            "  VTAC Annotation: None sec\n",
            "  VFIB Annotation: 192 sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: None sec\n",
            "  Difference from VFIB: -182 sec\n",
            "-----------------------------------\n",
            "Record: cu03\n",
            "  VTAC Annotation: 465 sec\n",
            "  VFIB Annotation: 465 sec\n",
            "  Model Detection: 16 sec\n",
            "  Difference from VTAC: -449 sec\n",
            "  Difference from VFIB: -449 sec\n",
            "-----------------------------------\n",
            "Record: cu04\n",
            "  VTAC Annotation: 155 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 211 sec\n",
            "  Difference from VTAC: 56 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu05\n",
            "  VTAC Annotation: 358 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 89 sec\n",
            "  Difference from VTAC: -269 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu06\n",
            "  VTAC Annotation: 184 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: -174 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu07\n",
            "  VTAC Annotation: 182 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 151 sec\n",
            "  Difference from VTAC: -31 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu08\n",
            "  VTAC Annotation: 426 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: -416 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu09\n",
            "  VTAC Annotation: 239 sec\n",
            "  VFIB Annotation: 100 sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: -229 sec\n",
            "  Difference from VFIB: -90 sec\n",
            "-----------------------------------\n",
            "Record: cu10\n",
            "  VTAC Annotation: 316 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 225 sec\n",
            "  Difference from VTAC: -91 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu11\n",
            "  VTAC Annotation: 371 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: -361 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu12\n",
            "  VTAC Annotation: 261 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: -251 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu13\n",
            "  VTAC Annotation: 427 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 268 sec\n",
            "  Difference from VTAC: -159 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu14\n",
            "  VTAC Annotation: None sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 39 sec\n",
            "  Difference from VTAC: None sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu15\n",
            "  VTAC Annotation: 405 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 74 sec\n",
            "  Difference from VTAC: -331 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu16\n",
            "  VTAC Annotation: 254 sec\n",
            "  VFIB Annotation: 168 sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: -244 sec\n",
            "  Difference from VFIB: -158 sec\n",
            "-----------------------------------\n",
            "Record: cu17\n",
            "  VTAC Annotation: 382 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 393 sec\n",
            "  Difference from VTAC: 11 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu18\n",
            "  VTAC Annotation: 334 sec\n",
            "  VFIB Annotation: 40 sec\n",
            "  Model Detection: 334 sec\n",
            "  Difference from VTAC: 0 sec\n",
            "  Difference from VFIB: 294 sec\n",
            "-----------------------------------\n",
            "Record: cu19\n",
            "  VTAC Annotation: 409 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 24 sec\n",
            "  Difference from VTAC: -385 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu20\n",
            "  VTAC Annotation: 244 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 240 sec\n",
            "  Difference from VTAC: -4 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu21\n",
            "  VTAC Annotation: 0 sec\n",
            "  VFIB Annotation: 52 sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: 10 sec\n",
            "  Difference from VFIB: -42 sec\n",
            "-----------------------------------\n",
            "Record: cu22\n",
            "  VTAC Annotation: 338 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 340 sec\n",
            "  Difference from VTAC: 2 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu23\n",
            "  VTAC Annotation: 334 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 336 sec\n",
            "  Difference from VTAC: 2 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu24\n",
            "  VTAC Annotation: 356 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 369 sec\n",
            "  Difference from VTAC: 13 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu25\n",
            "  VTAC Annotation: 421 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 27 sec\n",
            "  Difference from VTAC: -394 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu26\n",
            "  VTAC Annotation: 145 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 170 sec\n",
            "  Difference from VTAC: 25 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu27\n",
            "  VTAC Annotation: 451 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 123 sec\n",
            "  Difference from VTAC: -328 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu28\n",
            "  VTAC Annotation: 496 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: -486 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu29\n",
            "  VTAC Annotation: 378 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 57 sec\n",
            "  Difference from VTAC: -321 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu30\n",
            "  VTAC Annotation: 27 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 26 sec\n",
            "  Difference from VTAC: -1 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu31\n",
            "  VTAC Annotation: 494 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 10 sec\n",
            "  Difference from VTAC: -484 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu32\n",
            "  VTAC Annotation: 443 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 80 sec\n",
            "  Difference from VTAC: -363 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu33\n",
            "  VTAC Annotation: 405 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 457 sec\n",
            "  Difference from VTAC: 52 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu34\n",
            "  VTAC Annotation: 118 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 116 sec\n",
            "  Difference from VTAC: -2 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n",
            "Record: cu35\n",
            "  VTAC Annotation: 483 sec\n",
            "  VFIB Annotation: None sec\n",
            "  Model Detection: 223 sec\n",
            "  Difference from VTAC: -260 sec\n",
            "  Difference from VFIB: None sec\n",
            "-----------------------------------\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Predict Abnormal Segments ------------------\n",
        "def predict_abnormal(ecg, fs=FS, seq_len=SEQ_LEN, threshold=ERROR_THRESHOLD, consecutive=CONSECUTIVE_SEC):\n",
        "    errors = []\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // fs\n",
        "\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * fs\n",
        "        end = start + seq_len\n",
        "        if end > n_samples:\n",
        "            break\n",
        "        window = ecg[start:end].astype(np.float32)\n",
        "        err = reconstruction_error(window)\n",
        "        errors.append(err)\n",
        "\n",
        "    errors = np.array(errors)\n",
        "    predictions = np.zeros_like(errors, dtype=int)\n",
        "\n",
        "    for i in range(len(errors) - consecutive + 1):\n",
        "        if np.all(errors[i:i+consecutive] > threshold):\n",
        "            predictions[i:i+consecutive] = 1\n",
        "\n",
        "    return errors, predictions\n",
        "\n",
        "# ==================================================\n",
        "#                BATCH EXECUTION\n",
        "# ==================================================\n",
        "\n",
        "summary = []\n",
        "\n",
        "for idx in range(1, 36):\n",
        "    rec_name = f\"cu{idx:02d}\"\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"Processing CUDB record: {rec_name}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "        ecg = record.p_signal[:, 0]\n",
        "        ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "    except Exception:\n",
        "        print(\"Failed to load record or annotation\")\n",
        "        continue\n",
        "\n",
        "    # ---------- Annotations ----------\n",
        "    vtac_annot_sec = None\n",
        "    vfib_annot_sec = None\n",
        "\n",
        "    for sym, samp in zip(ann.symbol, ann.sample):\n",
        "        if sym == '[' and vtac_annot_sec is None:\n",
        "            vtac_annot_sec = samp // FS\n",
        "        if sym == '+' and vfib_annot_sec is None:\n",
        "            vfib_annot_sec = samp // FS\n",
        "\n",
        "    if vtac_annot_sec is not None:\n",
        "        print(f\"Annotated VTAC start time: {vtac_annot_sec} seconds\")\n",
        "    else:\n",
        "        print(\"No VTAC annotation found\")\n",
        "\n",
        "    if vfib_annot_sec is not None:\n",
        "        print(f\"Annotated VFIB start time: {vfib_annot_sec} seconds\")\n",
        "    else:\n",
        "        print(\"No VFIB annotation found\")\n",
        "\n",
        "    # ---------- Model Prediction ----------\n",
        "    errors, predictions = predict_abnormal(ecg)\n",
        "\n",
        "    vtac_start_sec = None\n",
        "    for i in range(len(errors) - CONSECUTIVE_SEC + 1):\n",
        "        if np.all(errors[i:i+CONSECUTIVE_SEC] > ERROR_THRESHOLD):\n",
        "            vtac_start_sec = i + SEQ_LEN_SEC\n",
        "            break\n",
        "\n",
        "    if vtac_start_sec is not None:\n",
        "        print(f\"Model detected sustained abnormality at second: {vtac_start_sec}\")\n",
        "    else:\n",
        "        print(\"Model did not detect sustained abnormality\")\n",
        "\n",
        "    # ---------- Compute Differences ----------\n",
        "    vtac_diff_sec = vtac_start_sec - vtac_annot_sec if vtac_start_sec is not None and vtac_annot_sec is not None else None\n",
        "    vfib_diff_sec = vtac_start_sec - vfib_annot_sec if vtac_start_sec is not None and vfib_annot_sec is not None else None\n",
        "\n",
        "    summary.append({\n",
        "        \"record\": rec_name,\n",
        "        \"vtac_annotation_sec\": vtac_annot_sec,\n",
        "        \"vfib_annotation_sec\": vfib_annot_sec,\n",
        "        \"model_detect_sec\": vtac_start_sec,\n",
        "        \"vtac_diff_sec\": vtac_diff_sec,\n",
        "        \"vfib_diff_sec\": vfib_diff_sec\n",
        "    })\n",
        "\n",
        "# ==================================================\n",
        "#                 FINAL SUMMARY\n",
        "# ==================================================\n",
        "print(\"\\n===================================\")\n",
        "print(\"           FINAL SUMMARY            \")\n",
        "print(\"===================================\")\n",
        "\n",
        "for s in summary:\n",
        "    print(f\"Record: {s['record']}\")\n",
        "    print(f\"  VTAC Annotation: {s['vtac_annotation_sec']} sec\")\n",
        "    print(f\"  VFIB Annotation: {s['vfib_annotation_sec']} sec\")\n",
        "    print(f\"  Model Detection: {s['model_detect_sec']} sec\")\n",
        "    print(f\"  Difference from VTAC: {s['vtac_diff_sec']} sec\")\n",
        "    print(f\"  Difference from VFIB: {s['vfib_diff_sec']} sec\")\n",
        "    print(\"-----------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SCB_WjX9uDx",
        "outputId": "968ff48e-e084-4820-9ad5-4dc3ce7babda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 595708\n",
            "-rw------- 1 root root   304326 Dec  7 14:38 16265.atr\n",
            "-rw------- 1 root root 35192832 Dec  7 14:38 16265.dat\n",
            "-rw------- 1 root root      118 Dec  7 14:36 16265.hea\n",
            "-rw------- 1 root root   268342 Dec  7 14:41 16272.atr\n",
            "-rw------- 1 root root 34560000 Dec  7 14:41 16272.dat\n",
            "-rw------- 1 root root      116 Dec  7 14:38 16272.hea\n",
            "-rw------- 1 root root   250552 Dec  7 14:38 16273.atr\n",
            "-rw------- 1 root root 34062336 Dec  7 14:38 16273.dat\n",
            "-rw------- 1 root root      117 Dec  7 14:41 16273.hea\n",
            "-rw------- 1 root root   314364 Dec  7 14:41 16420.atr\n",
            "-rw------- 1 root root 33153024 Dec  7 14:41 16420.dat\n",
            "-rw------- 1 root root      116 Dec  7 14:38 16420.hea\n",
            "-rw------- 1 root root   257534 Dec  7 14:45 16483.atr\n",
            "-rw------- 1 root root 35880960 Dec  7 14:44 16483.dat\n",
            "-rw------- 1 root root      117 Dec  7 14:41 16483.hea\n",
            "-rw------- 1 root root   274178 Dec  7 14:48 16539.atr\n",
            "-rw------- 1 root root 33982464 Dec  7 14:48 16539.dat\n",
            "-rw------- 1 root root      117 Dec  7 14:45 16539.hea\n",
            "-rw------- 1 root root   461400 Dec  7 14:44 16773.atr\n",
            "-rw------- 1 root root 33140736 Dec  7 14:50 16773.dat\n",
            "-rw------- 1 root root      118 Dec  7 14:48 16773.hea\n",
            "-rw------- 1 root root   206454 Dec  7 14:46 16786.atr\n",
            "-rw------- 1 root root 33853440 Dec  7 14:46 16786.dat\n",
            "-rw------- 1 root root      115 Dec  7 14:44 16786.hea\n",
            "-rw------- 1 root root   257590 Dec  7 14:48 16795.atr\n",
            "-rw------- 1 root root 32600064 Dec  7 14:48 16795.dat\n",
            "-rw------- 1 root root      114 Dec  7 14:46 16795.hea\n",
            "-rw------- 1 root root   218098 Dec  7 14:50 17052.atr\n",
            "-rw------- 1 root root 31979520 Dec  7 14:50 17052.dat\n",
            "-rw------- 1 root root      117 Dec  7 14:48 17052.hea\n",
            "-rw------- 1 root root   261466 Dec  7 14:53 17453.atr\n",
            "-rw------- 1 root root 33705984 Dec  7 14:53 17453.dat\n",
            "-rw------- 1 root root      116 Dec  7 14:50 17453.hea\n",
            "-rw------- 1 root root   413624 Dec  7 14:53 18177.atr\n",
            "-rw------- 1 root root 35880960 Dec  7 14:53 18177.dat\n",
            "-rw------- 1 root root      117 Dec  7 14:53 18177.hea\n",
            "-rw------- 1 root root   232250 Dec  7 14:55 18184.atr\n",
            "-rw------- 1 root root 32827392 Dec  7 14:55 18184.dat\n",
            "-rw------- 1 root root      116 Dec  7 14:53 18184.hea\n",
            "-rw------- 1 root root   390826 Dec  7 14:55 19088.atr\n",
            "-rw------- 1 root root 32901120 Dec  7 14:55 19088.dat\n",
            "-rw------- 1 root root      116 Dec  7 14:55 19088.hea\n",
            "-rw------- 1 root root   243326 Dec  7 14:58 19090.atr\n",
            "-rw------- 1 root root 33429504 Dec  7 14:58 19090.dat\n",
            "-rw------- 1 root root      116 Dec  7 14:55 19090.hea\n",
            "-rw------- 1 root root   236284 Dec  7 14:55 19093.atr\n",
            "-rw------- 1 root root 32117760 Dec  7 15:00 19093.dat\n",
            "-rw------- 1 root root      117 Dec  7 14:58 19093.hea\n",
            "-rw------- 1 root root   259694 Dec  7 14:58 19140.atr\n",
            "-rw------- 1 root root 33417216 Dec  7 14:58 19140.dat\n",
            "-rw------- 1 root root      116 Dec  7 14:55 19140.hea\n",
            "-rw------- 1 root root   349972 Dec  7 15:00 19830.atr\n",
            "-rw------- 1 root root 32105472 Dec  7 15:00 19830.dat\n",
            "-rw------- 1 root root      117 Dec  7 14:58 19830.hea\n"
          ]
        }
      ],
      "source": [
        "!ls -l /content/drive/MyDrive/nsrdb/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr1J5_s184k5"
      },
      "source": [
        "Validation for false positives on NSR database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYWDiBI19Kcg",
        "outputId": "c3f0cb89-17c0-4701-f021-0c11ca7be24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16265\n",
            "==============================\n",
            "Total sustained false positive events detected: 49\n",
            "Event start times (seconds): [42, 73, 83, 109, 119, 129, 139, 149, 159, 169, 179, 189, 199, 209, 219, 229, 239, 249, 259, 269, 279, 289, 299, 309, 319, 329, 341, 351, 367, 377, 397, 407, 417, 427, 437, 458, 768, 1230, 1482, 1492, 1502, 1523, 1533, 1543, 1670, 1739, 1749, 1775, 1785]\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16272\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16273\n",
            "==============================\n",
            "Total sustained false positive events detected: 22\n",
            "Event start times (seconds): [27, 68, 78, 138, 271, 281, 304, 314, 324, 334, 344, 354, 364, 374, 384, 394, 404, 414, 1337, 1350, 1369, 1379]\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16420\n",
            "==============================\n",
            "Total sustained false positive events detected: 1\n",
            "Event start times (seconds): [102]\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16483\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16539\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16773\n",
            "==============================\n",
            "Total sustained false positive events detected: 14\n",
            "Event start times (seconds): [30, 73, 83, 147, 157, 167, 177, 187, 1069, 1079, 1089, 1157, 1521, 1730]\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16786\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 16795\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 17052\n",
            "==============================\n",
            "Total sustained false positive events detected: 1\n",
            "Event start times (seconds): [378]\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 17453\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 18177\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 18184\n",
            "==============================\n",
            "Total sustained false positive events detected: 2\n",
            "Event start times (seconds): [1567, 1577]\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 19088\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 19090\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 19093\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 19140\n",
            "==============================\n",
            "Total sustained false positive events detected: 0\n",
            "Event start times (seconds): []\n",
            "\n",
            "==============================\n",
            "Processing NSRDB record: 19830\n",
            "==============================\n",
            "Total sustained false positive events detected: 28\n",
            "Event start times (seconds): [957, 967, 1327, 1337, 1347, 1366, 1376, 1386, 1396, 1406, 1416, 1426, 1436, 1466, 1476, 1489, 1499, 1509, 1526, 1536, 1546, 1573, 1583, 1604, 1614, 1629, 1639, 1788]\n",
            "\n",
            "===================================\n",
            "      NSRDB VALIDATION SUMMARY      \n",
            "===================================\n",
            "{'record': '16265', 'sustained_false_positive_count': 49, 'false_positive_event_starts': [42, 73, 83, 109, 119, 129, 139, 149, 159, 169, 179, 189, 199, 209, 219, 229, 239, 249, 259, 269, 279, 289, 299, 309, 319, 329, 341, 351, 367, 377, 397, 407, 417, 427, 437, 458, 768, 1230, 1482, 1492, 1502, 1523, 1533, 1543, 1670, 1739, 1749, 1775, 1785], 'total_seconds_checked': 1800}\n",
            "{'record': '16272', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '16273', 'sustained_false_positive_count': 22, 'false_positive_event_starts': [27, 68, 78, 138, 271, 281, 304, 314, 324, 334, 344, 354, 364, 374, 384, 394, 404, 414, 1337, 1350, 1369, 1379], 'total_seconds_checked': 1800}\n",
            "{'record': '16420', 'sustained_false_positive_count': 1, 'false_positive_event_starts': [102], 'total_seconds_checked': 1800}\n",
            "{'record': '16483', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '16539', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '16773', 'sustained_false_positive_count': 14, 'false_positive_event_starts': [30, 73, 83, 147, 157, 167, 177, 187, 1069, 1079, 1089, 1157, 1521, 1730], 'total_seconds_checked': 1800}\n",
            "{'record': '16786', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '16795', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '17052', 'sustained_false_positive_count': 1, 'false_positive_event_starts': [378], 'total_seconds_checked': 1800}\n",
            "{'record': '17453', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '18177', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '18184', 'sustained_false_positive_count': 2, 'false_positive_event_starts': [1567, 1577], 'total_seconds_checked': 1800}\n",
            "{'record': '19088', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '19090', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '19093', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '19140', 'sustained_false_positive_count': 0, 'false_positive_event_starts': [], 'total_seconds_checked': 1800}\n",
            "{'record': '19830', 'sustained_false_positive_count': 28, 'false_positive_event_starts': [957, 967, 1327, 1337, 1347, 1366, 1376, 1386, 1396, 1406, 1416, 1426, 1436, 1466, 1476, 1489, 1499, 1509, 1526, 1536, 1546, 1573, 1583, 1604, 1614, 1629, 1639, 1788], 'total_seconds_checked': 1800}\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "NSRDB_PATH = \"/content/drive/MyDrive/nsrdb/\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "MAX_MINUTES = 30   # Only take first 30 minutes\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Predict Abnormal Segments ------------------\n",
        "def predict_abnormal(ecg, fs=FS, seq_len=SEQ_LEN, threshold=ERROR_THRESHOLD, consecutive=CONSECUTIVE_SEC):\n",
        "    errors = []\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // fs\n",
        "\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * fs\n",
        "        end = start + seq_len\n",
        "        if end > n_samples:\n",
        "            break\n",
        "        window = ecg[start:end].astype(np.float32)\n",
        "        err = reconstruction_error(window)\n",
        "        errors.append(err)\n",
        "\n",
        "    errors = np.array(errors)\n",
        "    predictions = np.zeros_like(errors, dtype=int)\n",
        "\n",
        "    for i in range(len(errors) - consecutive + 1):\n",
        "        if np.all(errors[i:i+consecutive] > threshold):\n",
        "            predictions[i:i+consecutive] = 1\n",
        "\n",
        "    return errors, predictions\n",
        "\n",
        "# ------------------ Count Sustained Abnormal Events ------------------\n",
        "def count_sustained_abnormal(predictions, consecutive=CONSECUTIVE_SEC):\n",
        "    events = []\n",
        "    i = 0\n",
        "    while i < len(predictions) - consecutive + 1:\n",
        "        if np.all(predictions[i:i+consecutive] == 1):\n",
        "            events.append(i + SEQ_LEN_SEC)  # mark start second of event\n",
        "            i += consecutive  # skip this segment to avoid double counting\n",
        "        else:\n",
        "            i += 1\n",
        "    return events\n",
        "\n",
        "# ==================================================\n",
        "#                NSRDB VALIDATION\n",
        "# ==================================================\n",
        "summary = []\n",
        "\n",
        "for fname in sorted(os.listdir(NSRDB_PATH)):\n",
        "    if not fname.endswith(\".hea\"):\n",
        "        continue\n",
        "\n",
        "    rec_name = fname.replace(\".hea\", \"\")\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"Processing NSRDB record: {rec_name}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(NSRDB_PATH, rec_name))\n",
        "        ecg = record.p_signal[:, 0]  # take lead 0\n",
        "        max_samples = MAX_MINUTES * 60 * FS\n",
        "        ecg = ecg[:max_samples]  # take first 30 minutes\n",
        "    except Exception:\n",
        "        print(\"Failed to load record\")\n",
        "        continue\n",
        "\n",
        "    errors, predictions = predict_abnormal(ecg)\n",
        "    false_positive_events = count_sustained_abnormal(predictions)\n",
        "\n",
        "    print(f\"Total sustained false positive events detected: {len(false_positive_events)}\")\n",
        "    print(f\"Event start times (seconds): {false_positive_events}\")\n",
        "\n",
        "    summary.append({\n",
        "        \"record\": rec_name,\n",
        "        \"sustained_false_positive_count\": len(false_positive_events),\n",
        "        \"false_positive_event_starts\": false_positive_events,\n",
        "        \"total_seconds_checked\": len(ecg)//FS\n",
        "    })\n",
        "\n",
        "# ==================================================\n",
        "#                 FINAL SUMMARY\n",
        "# ==================================================\n",
        "print(\"\\n===================================\")\n",
        "print(\"      NSRDB VALIDATION SUMMARY      \")\n",
        "print(\"===================================\")\n",
        "for s in summary:\n",
        "    print(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MU61-Ql6yDE"
      },
      "source": [
        "Mean difference between predicted VTAC time and actual time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVrnd-k665T6",
        "outputId": "1dbf613e-4278-47bf-edda-9b15f7a1701f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean VTAC prediction difference: -180.00 seconds\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Predict Sustained Abnormality ------------------\n",
        "def detect_vtac_start(ecg):\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "    errors = []\n",
        "\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples:\n",
        "            break\n",
        "\n",
        "        window = ecg[start:end].astype(np.float32)\n",
        "        err = reconstruction_error(window)\n",
        "        errors.append(err)\n",
        "\n",
        "    errors = np.array(errors)\n",
        "\n",
        "    for i in range(len(errors) - CONSECUTIVE_SEC + 1):\n",
        "        if np.all(errors[i:i + CONSECUTIVE_SEC] > ERROR_THRESHOLD):\n",
        "            return i + SEQ_LEN_SEC  # detected VTAC start (seconds)\n",
        "\n",
        "    return None\n",
        "\n",
        "# ==================================================\n",
        "#           MAIN EVALUATION LOOP\n",
        "# ==================================================\n",
        "\n",
        "time_differences = []\n",
        "\n",
        "for idx in range(1, 36):\n",
        "    rec_name = f\"cu{idx:02d}\"\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "        ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    ecg = record.p_signal[:, 0]\n",
        "\n",
        "    # -------- VTAC Annotation --------\n",
        "    vtac_annot_sec = None\n",
        "    for sym, samp in zip(ann.symbol, ann.sample):\n",
        "        if sym == '[':\n",
        "            vtac_annot_sec = samp // FS\n",
        "            break\n",
        "\n",
        "    if vtac_annot_sec is None:\n",
        "        continue  # no VTAC → skip record\n",
        "\n",
        "    # -------- Model Detection --------\n",
        "    vtac_pred_sec = detect_vtac_start(ecg)\n",
        "\n",
        "    if vtac_pred_sec is None:\n",
        "        continue  # model never triggered → skip\n",
        "\n",
        "    # -------- Time Difference --------\n",
        "    diff_sec = vtac_pred_sec - vtac_annot_sec\n",
        "    time_differences.append(diff_sec)\n",
        "\n",
        "# ==================================================\n",
        "#               FINAL RESULT\n",
        "# ==================================================\n",
        "\n",
        "if len(time_differences) == 0:\n",
        "    print(\"No valid VTAC detections found.\")\n",
        "else:\n",
        "    mean_diff = np.mean(time_differences)\n",
        "    print(f\"Mean VTAC prediction difference: {mean_diff:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXnSxnyYgzmO"
      },
      "source": [
        "Further Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqyDOc3Ug1Sn",
        "outputId": "8c39036e-9ded-4870-cc03-d6ca77fcb155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== VALIDATION SUMMARY ==========\n",
            "Total records evaluated: 35\n",
            "True Positives (early or on-time): 20\n",
            "Late Detections: 11\n",
            "Missed Detections: 2\n",
            "False Positives: 2\n",
            "---------------------------------------\n",
            "Sensitivity: 0.909\n",
            "False Positive Rate: 0.057\n",
            "---------------------------------------\n",
            "Mean Detection Time Difference (sec): -99.2258064516129\n",
            "Std Detection Time Difference (sec): 183.9359477709949\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "\n",
        "HR_MIN = 120\n",
        "SLOPE_WINDOW = 10\n",
        "\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Heart Rate Estimation ------------------\n",
        "def estimate_heart_rate(ecg_segment):\n",
        "    diff = np.diff(ecg_segment)\n",
        "    squared = diff ** 2\n",
        "    threshold = np.mean(squared) + 0.5 * np.std(squared)\n",
        "    peaks = np.where(squared > threshold)[0]\n",
        "\n",
        "    if len(peaks) < 2:\n",
        "        return 0\n",
        "\n",
        "    rr_intervals = np.diff(peaks) / FS\n",
        "    rr_intervals = rr_intervals[rr_intervals > 0.25]  # remove implausible HR > 240 bpm\n",
        "\n",
        "    if len(rr_intervals) == 0:\n",
        "        return 0\n",
        "\n",
        "    return 60 / np.mean(rr_intervals)\n",
        "\n",
        "# ------------------ Error Slope ------------------\n",
        "def error_slope(errors):\n",
        "    y = errors[-SLOPE_WINDOW:]\n",
        "    x = np.arange(len(y))\n",
        "    slope = np.polyfit(x, y, 1)[0]\n",
        "    return slope\n",
        "\n",
        "# ------------------ Prediction ------------------\n",
        "def predict_vtac(ecg):\n",
        "    errors = []\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples:\n",
        "            break\n",
        "\n",
        "        window = ecg[start:end].astype(np.float32)\n",
        "        errors.append(reconstruction_error(window))\n",
        "\n",
        "    errors = np.array(errors)\n",
        "\n",
        "    for i in range(len(errors) - CONSECUTIVE_SEC + 1):\n",
        "        if np.all(errors[i:i+CONSECUTIVE_SEC] > ERROR_THRESHOLD):\n",
        "\n",
        "            detect_sec = i + SEQ_LEN_SEC\n",
        "\n",
        "            # ---------- Stage 2 checks ----------\n",
        "            ecg_segment = ecg[detect_sec*FS : (detect_sec+10)*FS]\n",
        "            hr = estimate_heart_rate(ecg_segment)\n",
        "            slope = error_slope(errors[i:i+CONSECUTIVE_SEC])\n",
        "\n",
        "            if hr >= HR_MIN and slope > 0:\n",
        "                return detect_sec\n",
        "\n",
        "    return None\n",
        "\n",
        "# ==================================================\n",
        "#               VALIDATION LOOP\n",
        "# ==================================================\n",
        "\n",
        "time_diffs = []\n",
        "TP = FP = FN = LD = 0\n",
        "total_records = 0\n",
        "\n",
        "for idx in range(1, 36):\n",
        "    rec_name = f\"cu{idx:02d}\"\n",
        "    total_records += 1\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "        ecg = record.p_signal[:, 0]\n",
        "        ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    vtac_annot_sec = None\n",
        "    for sym, samp in zip(ann.symbol, ann.sample):\n",
        "        if sym == '[':\n",
        "            vtac_annot_sec = samp // FS\n",
        "            break\n",
        "\n",
        "    vtac_pred_sec = predict_vtac(ecg)\n",
        "\n",
        "    if vtac_annot_sec is not None and vtac_pred_sec is not None:\n",
        "        diff = vtac_pred_sec - vtac_annot_sec\n",
        "        time_diffs.append(diff)\n",
        "        TP += int(diff <= 0)\n",
        "        LD += int(diff > 0)\n",
        "\n",
        "    elif vtac_annot_sec is not None and vtac_pred_sec is None:\n",
        "        FN += 1\n",
        "\n",
        "    elif vtac_annot_sec is None and vtac_pred_sec is not None:\n",
        "        FP += 1\n",
        "\n",
        "# ==================================================\n",
        "#               FINAL METRICS\n",
        "# ==================================================\n",
        "\n",
        "mean_diff = np.mean(time_diffs) if time_diffs else None\n",
        "std_diff = np.std(time_diffs) if time_diffs else None\n",
        "sensitivity = TP / (TP + FN) if (TP + FN) else 0\n",
        "fp_rate = FP / total_records\n",
        "\n",
        "print(\"\\n========== VALIDATION SUMMARY ==========\")\n",
        "print(f\"Total records evaluated: {total_records}\")\n",
        "print(f\"True Positives (early or on-time): {TP}\")\n",
        "print(f\"Late Detections: {LD}\")\n",
        "print(f\"Missed Detections: {FN}\")\n",
        "print(f\"False Positives: {FP}\")\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
        "print(f\"False Positive Rate: {fp_rate:.3f}\")\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Mean Detection Time Difference (sec): {mean_diff}\")\n",
        "print(f\"Std Detection Time Difference (sec): {std_diff}\")\n",
        "print(\"=======================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be8-L3F3lbLV"
      },
      "source": [
        "NSR Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v7KTZvblcvh",
        "outputId": "e5199bca-7fdf-44f4-945c-a2ee6827b8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing NSR record: 16265\n",
            "Processing NSR record: 16272\n",
            "Processing NSR record: 16273\n",
            "Processing NSR record: 16420\n",
            "Processing NSR record: 16483\n",
            "Processing NSR record: 16539\n",
            "Processing NSR record: 16773\n",
            "Processing NSR record: 16786\n",
            "Processing NSR record: 16795\n",
            "Processing NSR record: 17052\n",
            "Processing NSR record: 17453\n",
            "Processing NSR record: 18177\n",
            "Processing NSR record: 18184\n",
            "Processing NSR record: 19088\n",
            "Processing NSR record: 19090\n",
            "Processing NSR record: 19093\n",
            "Processing NSR record: 19140\n",
            "Processing NSR record: 19830\n",
            "\n",
            "========== NSRDB FALSE POSITIVE SUMMARY ==========\n",
            "Total records evaluated: 18\n",
            "Records with ≥1 false alarm: 7\n",
            "Total false positive events: 45\n",
            "Mean FP per 30 min: 2.500\n",
            "False positives per hour: 5.000\n",
            "=================================================\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = FS * SEQ_LEN_SEC\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "MAX_MINUTES = 30\n",
        "\n",
        "HR_MIN = 120\n",
        "SLOPE_WINDOW = 10\n",
        "\n",
        "NSRDB_PATH = \"/content/drive/MyDrive/nsrdb\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ Model ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc, _ = self.encoder(x)\n",
        "        dec, _ = self.decoder(enc)\n",
        "        return dec\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Heart Rate Estimation ------------------\n",
        "def estimate_heart_rate(ecg_segment):\n",
        "    diff = np.diff(ecg_segment)\n",
        "    energy = diff ** 2\n",
        "    threshold = np.mean(energy) + 0.5 * np.std(energy)\n",
        "    peaks = np.where(energy > threshold)[0]\n",
        "\n",
        "    if len(peaks) < 2:\n",
        "        return 0\n",
        "\n",
        "    rr = np.diff(peaks) / FS\n",
        "    rr = rr[rr > 0.25]\n",
        "\n",
        "    if len(rr) == 0:\n",
        "        return 0\n",
        "\n",
        "    return 60 / np.mean(rr)\n",
        "\n",
        "# ------------------ Error Slope ------------------\n",
        "def error_slope(errors):\n",
        "    y = errors[-SLOPE_WINDOW:]\n",
        "    x = np.arange(len(y))\n",
        "    return np.polyfit(x, y, 1)[0]\n",
        "\n",
        "# ------------------ Prediction ------------------\n",
        "def predict_abnormal(ecg):\n",
        "    errors = []\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples:\n",
        "            break\n",
        "\n",
        "        window = ecg[start:end]\n",
        "        errors.append(reconstruction_error(window))\n",
        "\n",
        "    errors = np.array(errors)\n",
        "    predictions = np.zeros_like(errors, dtype=int)\n",
        "\n",
        "    for i in range(len(errors) - CONSECUTIVE_SEC + 1):\n",
        "        if np.all(errors[i:i + CONSECUTIVE_SEC] > ERROR_THRESHOLD):\n",
        "\n",
        "            detect_sec = i + SEQ_LEN_SEC\n",
        "\n",
        "            ecg_segment = ecg[detect_sec * FS : (detect_sec + 10) * FS]\n",
        "            hr = estimate_heart_rate(ecg_segment)\n",
        "            slope = error_slope(errors[i:i + CONSECUTIVE_SEC])\n",
        "\n",
        "            if hr >= HR_MIN and slope > 0:\n",
        "                predictions[i:i + CONSECUTIVE_SEC] = 1\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# ==================================================\n",
        "#              NSRDB FALSE POSITIVE TEST\n",
        "# ==================================================\n",
        "total_records = 0\n",
        "records_with_fp = 0\n",
        "total_fp_events = 0\n",
        "\n",
        "for file in sorted(os.listdir(NSRDB_PATH)):\n",
        "    if not file.endswith(\".hea\"):\n",
        "        continue\n",
        "\n",
        "    record_name = file.replace(\".hea\", \"\")\n",
        "    print(f\"Processing NSR record: {record_name}\")\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(NSRDB_PATH, record_name))\n",
        "        ecg = record.p_signal[:, 0]\n",
        "        ecg = ecg[:MAX_MINUTES * 60 * FS]\n",
        "    except Exception:\n",
        "        print(\"  Failed to load record\")\n",
        "        continue\n",
        "\n",
        "    predictions = predict_abnormal(ecg)\n",
        "\n",
        "    fp_events = 0\n",
        "    i = 0\n",
        "    while i < len(predictions):\n",
        "        if predictions[i] == 1:\n",
        "            fp_events += 1\n",
        "            while i < len(predictions) and predictions[i] == 1:\n",
        "                i += 1\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    total_records += 1\n",
        "    total_fp_events += fp_events\n",
        "    if fp_events > 0:\n",
        "        records_with_fp += 1\n",
        "\n",
        "# ==================================================\n",
        "#                 FINAL METRICS\n",
        "# ==================================================\n",
        "hours_tested = total_records * (MAX_MINUTES / 60)\n",
        "\n",
        "print(\"\\n========== NSRDB FALSE POSITIVE SUMMARY ==========\")\n",
        "print(f\"Total records evaluated: {total_records}\")\n",
        "print(f\"Records with ≥1 false alarm: {records_with_fp}\")\n",
        "print(f\"Total false positive events: {total_fp_events}\")\n",
        "print(f\"Mean FP per 30 min: {total_fp_events / total_records:.3f}\")\n",
        "print(f\"False positives per hour: {total_fp_events / hours_tested:.3f}\")\n",
        "print(\"=================================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y772noRuziV"
      },
      "source": [
        "Newer method for reduction in FPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPjjFaqIu3ZP",
        "outputId": "f9c577bf-ae07-4906-f89a-5b56d672f42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing NSR record: 16265\n",
            "Processing NSR record: 16272\n",
            "Processing NSR record: 16273\n",
            "Processing NSR record: 16420\n",
            "Processing NSR record: 16483\n",
            "Processing NSR record: 16539\n",
            "Processing NSR record: 16773\n",
            "Processing NSR record: 16786\n",
            "Processing NSR record: 16795\n",
            "Processing NSR record: 17052\n",
            "Processing NSR record: 17453\n",
            "Processing NSR record: 18177\n",
            "Processing NSR record: 18184\n",
            "Processing NSR record: 19088\n",
            "Processing NSR record: 19090\n",
            "Processing NSR record: 19093\n",
            "Processing NSR record: 19140\n",
            "Processing NSR record: 19830\n",
            "\n",
            "========== NSRDB FALSE POSITIVE SUMMARY ==========\n",
            "Total records evaluated: 18\n",
            "Records with ≥1 false alarm: 7\n",
            "Total false positive events: 42\n",
            "Mean FP per 30 min: 2.333\n",
            "False positives per hour: 4.667\n",
            "=================================================\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = FS * SEQ_LEN_SEC\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "MAX_MINUTES = 30\n",
        "\n",
        "# RR regularity threshold (NSR suppressor)\n",
        "RR_STD_THRESHOLD = 0.04\n",
        "\n",
        "NSRDB_PATH = \"/content/drive/MyDrive/nsrdb\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ Model ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc, _ = self.encoder(x)\n",
        "        dec, _ = self.decoder(enc)\n",
        "        return dec\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ RR Regularity (Stage 2 veto) ------------------\n",
        "def rr_irregularity(ecg_segment):\n",
        "    diff = np.diff(ecg_segment)\n",
        "    energy = diff ** 2\n",
        "\n",
        "    threshold = np.percentile(energy, 95)\n",
        "    peaks = np.where(energy > threshold)[0]\n",
        "\n",
        "    if len(peaks) < 6:\n",
        "        return 0.0\n",
        "\n",
        "    rr = np.diff(peaks) / FS\n",
        "    return np.std(rr)\n",
        "\n",
        "# ------------------ Prediction ------------------\n",
        "def predict_abnormal(ecg):\n",
        "    errors = []\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples:\n",
        "            break\n",
        "\n",
        "        window = ecg[start:end]\n",
        "        errors.append(reconstruction_error(window))\n",
        "\n",
        "    errors = np.array(errors)\n",
        "    predictions = np.zeros_like(errors, dtype=int)\n",
        "\n",
        "    for i in range(len(errors) - CONSECUTIVE_SEC + 1):\n",
        "        if np.all(errors[i:i + CONSECUTIVE_SEC] > ERROR_THRESHOLD):\n",
        "\n",
        "            # ---------- Stage 2: NSR veto ----------\n",
        "            veto = False\n",
        "            for j in range(i, i + CONSECUTIVE_SEC):\n",
        "                seg_start = j * FS\n",
        "                seg_end = seg_start + SEQ_LEN\n",
        "\n",
        "                if seg_end > len(ecg):\n",
        "                    continue\n",
        "\n",
        "                rr_std = rr_irregularity(ecg[seg_start:seg_end])\n",
        "\n",
        "                # If rhythm is too regular, suppress alarm\n",
        "                if rr_std < RR_STD_THRESHOLD:\n",
        "                    veto = True\n",
        "                    break\n",
        "\n",
        "            if not veto:\n",
        "                predictions[i:i + CONSECUTIVE_SEC] = 1\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# ==================================================\n",
        "#              NSRDB FALSE POSITIVE TEST\n",
        "# ==================================================\n",
        "total_records = 0\n",
        "records_with_fp = 0\n",
        "total_fp_events = 0\n",
        "\n",
        "for file in sorted(os.listdir(NSRDB_PATH)):\n",
        "    if not file.endswith(\".hea\"):\n",
        "        continue\n",
        "\n",
        "    record_name = file.replace(\".hea\", \"\")\n",
        "    print(f\"Processing NSR record: {record_name}\")\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(NSRDB_PATH, record_name))\n",
        "        ecg = record.p_signal[:, 0]\n",
        "        ecg = ecg[:MAX_MINUTES * 60 * FS]\n",
        "    except Exception:\n",
        "        print(\"  Failed to load record\")\n",
        "        continue\n",
        "\n",
        "    predictions = predict_abnormal(ecg)\n",
        "\n",
        "    # Count sustained false positive events\n",
        "    fp_events = 0\n",
        "    i = 0\n",
        "    while i < len(predictions):\n",
        "        if predictions[i] == 1:\n",
        "            fp_events += 1\n",
        "            while i < len(predictions) and predictions[i] == 1:\n",
        "                i += 1\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    total_records += 1\n",
        "    total_fp_events += fp_events\n",
        "    if fp_events > 0:\n",
        "        records_with_fp += 1\n",
        "\n",
        "# ==================================================\n",
        "#                 FINAL METRICS\n",
        "# ==================================================\n",
        "hours_tested = total_records * (MAX_MINUTES / 60)\n",
        "\n",
        "print(\"\\n========== NSRDB FALSE POSITIVE SUMMARY ==========\")\n",
        "print(f\"Total records evaluated: {total_records}\")\n",
        "print(f\"Records with ≥1 false alarm: {records_with_fp}\")\n",
        "print(f\"Total false positive events: {total_fp_events}\")\n",
        "print(f\"Mean FP per 30 min: {total_fp_events / total_records:.3f}\")\n",
        "print(f\"False positives per hour: {total_fp_events / hours_tested:.3f}\")\n",
        "print(\"=================================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crTS-1hdmiw1"
      },
      "source": [
        "Slight alteration by gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCequZrNmihe",
        "outputId": "73a7a92f-439e-4c6e-b69e-fb66ba2dca2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from /content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\n",
            "Record     | Annotated  | Predicted  | Diff (s)   | Threshold \n",
            "-----------------------------------------------------------------\n",
            "cu01       | 214        | 218        | 4          | 0.1927\n",
            "cu03       | 465        | MISSED     | ---        | 1.5000\n",
            "cu04       | 155        | 141        | -14        | 0.1201\n",
            "cu05       | 358        | 90         | -268       | 0.3478\n",
            "cu06       | 184        | 291        | 107        | 0.6156\n",
            "cu07       | 182        | 146        | -36        | 0.2045\n",
            "cu08       | 426        | 26         | -400       | 1.5000\n",
            "cu09       | 239        | 447        | 208        | 0.5777\n",
            "cu10       | 316        | 225        | -91        | 0.2233\n",
            "cu11       | 371        | MISSED     | ---        | 0.7480\n",
            "cu12       | 261        | 264        | 3          | 0.8221\n",
            "cu13       | 427        | 261        | -166       | 0.2752\n",
            "cu15       | 405        | 225        | -180       | 0.5293\n",
            "cu16       | 254        | 258        | 4          | 0.4033\n",
            "cu17       | 382        | 385        | 3          | 0.1000\n",
            "cu18       | 334        | 334        | 0          | 0.2956\n",
            "cu19       | 409        | 343        | -66        | 0.3881\n",
            "cu20       | 244        | MISSED     | ---        | nan\n",
            "cu21       | 0          | MISSED     | ---        | 1.5000\n",
            "cu22       | 338        | 339        | 1          | 0.2043\n",
            "cu23       | 334        | 336        | 2          | 0.2356\n",
            "cu24       | 356        | 361        | 5          | 0.1734\n",
            "cu25       | 421        | MISSED     | ---        | 0.7033\n",
            "cu26       | 145        | 170        | 25         | 0.3410\n",
            "cu27       | 451        | MISSED     | ---        | nan\n",
            "cu28       | 496        | MISSED     | ---        | 0.6728\n",
            "cu29       | 378        | 382        | 4          | 0.7414\n",
            "cu30       | 27         | MISSED     | ---        | nan\n",
            "cu31       | 494        | MISSED     | ---        | nan\n",
            "cu32       | 443        | MISSED     | ---        | 0.3240\n",
            "cu33       | 405        | 390        | -15        | 0.1011\n",
            "cu34       | 118        | 116        | -2         | 0.2660\n",
            "cu35       | 483        | MISSED     | ---        | nan\n",
            "-----------------------------------------------------------------\n",
            "Summary: Detected 22/33 events.\n",
            "Mean Prediction Difference: -39.64 seconds\n",
            "Result: EARLY detection (on average)\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Default settings (used as fallback)\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    print(f\"Model loaded from {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"Error: Model path not found at {MODEL_PATH}\")\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ Adaptive Detection Logic ------------------\n",
        "def detect_vtac_start_adaptive(ecg):\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "    errors = []\n",
        "\n",
        "    # --- Step 1: Calculate Errors for the whole signal ---\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples:\n",
        "            break\n",
        "\n",
        "        window = ecg[start:end].astype(np.float32)\n",
        "\n",
        "        # --- Normalization Fix ---\n",
        "        # We subtract the mean to remove \"Baseline Drift\" (wandering signal).\n",
        "        # We avoid dividing by Std Dev unless we are sure the model was trained that way.\n",
        "        mu = np.mean(window)\n",
        "        window_norm = window - mu\n",
        "\n",
        "        err = reconstruction_error(window_norm)\n",
        "        errors.append(err)\n",
        "\n",
        "    errors = np.array(errors)\n",
        "\n",
        "    # --- Step 2: Adaptive Threshold Calibration ---\n",
        "    # We look at the first 60 seconds (assumed Normal) to find this patient's \"normal noise level\"\n",
        "    calibration_duration = 60\n",
        "\n",
        "    if len(errors) > calibration_duration:\n",
        "        baseline_errors = errors[:calibration_duration]\n",
        "        baseline_mean = np.mean(baseline_errors)\n",
        "        baseline_std = np.std(baseline_errors)\n",
        "\n",
        "        # Threshold = Mean + 4 standard deviations of the noise\n",
        "        dynamic_threshold = baseline_mean + (4 * baseline_std)\n",
        "\n",
        "        # Safety: Ensure threshold isn't impossibly low or high\n",
        "        dynamic_threshold = max(dynamic_threshold, 0.10)\n",
        "        dynamic_threshold = min(dynamic_threshold, 1.5)\n",
        "    else:\n",
        "        dynamic_threshold = ERROR_THRESHOLD # Fallback\n",
        "\n",
        "    # --- Step 3: Detection with Persistence ---\n",
        "    # We look for a sequence of errors that stays above the threshold for CONSECUTIVE_SEC\n",
        "    for i in range(len(errors) - CONSECUTIVE_SEC + 1):\n",
        "        if np.all(errors[i:i + CONSECUTIVE_SEC] > dynamic_threshold):\n",
        "            # Found it! Return the time (seconds) and the threshold used\n",
        "            return i + SEQ_LEN_SEC, dynamic_threshold\n",
        "\n",
        "    # If loop finishes without returning, no VTAC was found\n",
        "    return None, dynamic_threshold\n",
        "\n",
        "# ==================================================\n",
        "#           MAIN EVALUATION LOOP\n",
        "# ==================================================\n",
        "\n",
        "time_differences = []\n",
        "detected_count = 0\n",
        "total_annotated = 0\n",
        "\n",
        "print(f\"{'Record':<10} | {'Annotated':<10} | {'Predicted':<10} | {'Diff (s)':<10} | {'Threshold':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for idx in range(1, 36):\n",
        "    rec_name = f\"cu{idx:02d}\"\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "        ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "    except Exception:\n",
        "        # print(f\"Could not load {rec_name}\")\n",
        "        continue\n",
        "\n",
        "    ecg = record.p_signal[:, 0]\n",
        "\n",
        "    # -------- 1. Get Annotation --------\n",
        "    vtac_annot_sec = None\n",
        "    for sym, samp in zip(ann.symbol, ann.sample):\n",
        "        if sym == '[':\n",
        "            vtac_annot_sec = samp // FS\n",
        "            break\n",
        "\n",
        "    if vtac_annot_sec is None:\n",
        "        continue  # Skip records with no VTAC annotation\n",
        "\n",
        "    total_annotated += 1\n",
        "\n",
        "    # -------- 2. Run Model Detection --------\n",
        "    # Note: We unpack the tuple (time, threshold)\n",
        "    vtac_pred_sec, thresh_used = detect_vtac_start_adaptive(ecg)\n",
        "\n",
        "    # -------- 3. Handle Null Prediction --------\n",
        "    if vtac_pred_sec is None:\n",
        "        print(f\"{rec_name:<10} | {vtac_annot_sec:<10} | {'MISSED':<10} | {'---':<10} | {thresh_used:.4f}\")\n",
        "        continue\n",
        "\n",
        "    # -------- 4. Calculate Difference --------\n",
        "    diff_sec = vtac_pred_sec - vtac_annot_sec\n",
        "    time_differences.append(diff_sec)\n",
        "    detected_count += 1\n",
        "\n",
        "    print(f\"{rec_name:<10} | {vtac_annot_sec:<10} | {vtac_pred_sec:<10} | {diff_sec:<10} | {thresh_used:.4f}\")\n",
        "\n",
        "# ==================================================\n",
        "#               FINAL RESULT\n",
        "# ==================================================\n",
        "print(\"-\" * 65)\n",
        "if len(time_differences) == 0:\n",
        "    print(\"No valid VTAC detections found.\")\n",
        "else:\n",
        "    mean_diff = np.mean(time_differences)\n",
        "    print(f\"Summary: Detected {detected_count}/{total_annotated} events.\")\n",
        "    print(f\"Mean Prediction Difference: {mean_diff:.2f} seconds\")\n",
        "    if mean_diff < 0:\n",
        "        print(\"Result: EARLY detection (on average)\")\n",
        "    else:\n",
        "        print(\"Result: LATE detection (on average)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is6HlvN-G4Rt"
      },
      "source": [
        "Gemini edit on NSR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0YswI2pG6k5",
        "outputId": "29c282d2-c56f-49a9-e150-0498ef03f8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from /content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\n",
            "Record     | FP Events  | Threshold Used \n",
            "---------------------------------------------\n",
            "16265      | 3          | 0.3436\n",
            "16272      | 0          | 0.1500\n",
            "16273      | 0          | 0.6155\n",
            "16420      | 1          | 0.1500\n",
            "16483      | 10         | 0.1500\n",
            "16539      | 0          | 0.1500\n",
            "16773      | 3          | 0.3602\n",
            "16786      | 7          | 0.2290\n",
            "16795      | 0          | 0.1500\n",
            "17052      | 1          | 0.1500\n",
            "17453      | 0          | 0.1956\n",
            "18177      | 1          | 0.1500\n",
            "18184      | 22         | 0.1742\n",
            "19088      | 0          | 0.3666\n",
            "19090      | 0          | 0.1883\n",
            "19093      | 0          | 0.3018\n",
            "19140      | 0          | 0.1904\n",
            "19830      | 9          | 0.2805\n",
            "\n",
            "========== NSRDB FALSE POSITIVE SUMMARY ==========\n",
            "Total records evaluated: 18\n",
            "Records with ≥1 false alarm: 9\n",
            "Total false positive events: 57\n",
            "Mean FP per 30 min: 3.167\n",
            "False positives per hour: 6.333\n",
            "=================================================\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = FS * SEQ_LEN_SEC\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Default Fallback (only used if signal is too short to calibrate)\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "MAX_MINUTES = 30\n",
        "\n",
        "# RR regularity threshold (Stage 2 veto)\n",
        "RR_STD_THRESHOLD = 0.04\n",
        "\n",
        "NSRDB_PATH = \"/content/drive/MyDrive/nsrdb\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ Model ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    print(f\"Model loaded from {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"Error: Model path not found at {MODEL_PATH}\")\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Reconstruction Error ------------------\n",
        "def reconstruction_error(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        return ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "\n",
        "# ------------------ RR Regularity (Stage 2 veto) ------------------\n",
        "def rr_irregularity(ecg_segment):\n",
        "    # Simple energy-based peak detection\n",
        "    diff = np.diff(ecg_segment)\n",
        "    energy = diff ** 2\n",
        "\n",
        "    # Dynamic threshold for peak detection\n",
        "    threshold = np.percentile(energy, 95)\n",
        "    peaks = np.where(energy > threshold)[0]\n",
        "\n",
        "    if len(peaks) < 6:\n",
        "        return 0.0\n",
        "\n",
        "    rr = np.diff(peaks) / FS\n",
        "    return np.std(rr)\n",
        "\n",
        "# ------------------ Adaptive Prediction Logic ------------------\n",
        "def predict_abnormal_adaptive(ecg):\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "    errors = []\n",
        "\n",
        "    # --- Step 1: Compute Errors with Normalization ---\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples:\n",
        "            break\n",
        "\n",
        "        window = ecg[start:end].astype(np.float32)\n",
        "\n",
        "        # FIX 1: Normalize window to remove baseline drift\n",
        "        mu = np.mean(window)\n",
        "        window_norm = window - mu\n",
        "\n",
        "        errors.append(reconstruction_error(window_norm))\n",
        "\n",
        "    errors = np.array(errors)\n",
        "    predictions = np.zeros_like(errors, dtype=int)\n",
        "\n",
        "    # --- Step 2: Adaptive Threshold Calibration ---\n",
        "    # Use first 60 seconds of THIS PATIENT to set the alarm level\n",
        "    calibration_duration = 60\n",
        "\n",
        "    if len(errors) > calibration_duration:\n",
        "        baseline_errors = errors[:calibration_duration]\n",
        "        baseline_mean = np.mean(baseline_errors)\n",
        "        baseline_std = np.std(baseline_errors)\n",
        "\n",
        "        # Threshold = Mean + 4 standard deviations\n",
        "        dynamic_threshold = baseline_mean + (4 * baseline_std)\n",
        "\n",
        "        # Safety limits\n",
        "        dynamic_threshold = max(dynamic_threshold, 0.15)\n",
        "        dynamic_threshold = min(dynamic_threshold, 1.2)\n",
        "    else:\n",
        "        dynamic_threshold = ERROR_THRESHOLD  # Fallback\n",
        "\n",
        "    # --- Step 3: Detection Loop ---\n",
        "    for i in range(len(errors) - CONSECUTIVE_SEC + 1):\n",
        "        if np.all(errors[i:i + CONSECUTIVE_SEC] > dynamic_threshold):\n",
        "\n",
        "            # ---------- Stage 2: NSR veto ----------\n",
        "            veto = False\n",
        "            for j in range(i, i + CONSECUTIVE_SEC):\n",
        "                seg_start = j * FS\n",
        "                seg_end = seg_start + SEQ_LEN\n",
        "\n",
        "                if seg_end > len(ecg):\n",
        "                    continue\n",
        "\n",
        "                rr_std = rr_irregularity(ecg[seg_start:seg_end])\n",
        "\n",
        "                # If rhythm is too regular, suppress alarm\n",
        "                if rr_std < RR_STD_THRESHOLD:\n",
        "                    veto = True\n",
        "                    break\n",
        "\n",
        "            if not veto:\n",
        "                predictions[i:i + CONSECUTIVE_SEC] = 1\n",
        "\n",
        "    return predictions, dynamic_threshold\n",
        "\n",
        "# ==================================================\n",
        "#              NSRDB FALSE POSITIVE TEST\n",
        "# ==================================================\n",
        "total_records = 0\n",
        "records_with_fp = 0\n",
        "total_fp_events = 0\n",
        "\n",
        "print(f\"{'Record':<10} | {'FP Events':<10} | {'Threshold Used':<15}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "for file in sorted(os.listdir(NSRDB_PATH)):\n",
        "    if not file.endswith(\".hea\"):\n",
        "        continue\n",
        "\n",
        "    record_name = file.replace(\".hea\", \"\")\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(NSRDB_PATH, record_name))\n",
        "        ecg = record.p_signal[:, 0]\n",
        "        ecg = ecg[:MAX_MINUTES * 60 * FS]\n",
        "    except Exception:\n",
        "        # print(f\"Failed to load {record_name}\")\n",
        "        continue\n",
        "\n",
        "    # Run Adaptive Prediction\n",
        "    predictions, thresh_used = predict_abnormal_adaptive(ecg)\n",
        "\n",
        "    # Count sustained false positive events\n",
        "    fp_events = 0\n",
        "    i = 0\n",
        "    while i < len(predictions):\n",
        "        if predictions[i] == 1:\n",
        "            fp_events += 1\n",
        "            # Skip the rest of this event (don't count every second as a new event)\n",
        "            while i < len(predictions) and predictions[i] == 1:\n",
        "                i += 1\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    total_records += 1\n",
        "    total_fp_events += fp_events\n",
        "\n",
        "    if fp_events > 0:\n",
        "        records_with_fp += 1\n",
        "\n",
        "    print(f\"{record_name:<10} | {fp_events:<10} | {thresh_used:.4f}\")\n",
        "\n",
        "# ==================================================\n",
        "#                 FINAL METRICS\n",
        "# ==================================================\n",
        "if total_records > 0:\n",
        "    hours_tested = total_records * (MAX_MINUTES / 60)\n",
        "    fp_per_hour = total_fp_events / hours_tested\n",
        "\n",
        "    print(\"\\n========== NSRDB FALSE POSITIVE SUMMARY ==========\")\n",
        "    print(f\"Total records evaluated: {total_records}\")\n",
        "    print(f\"Records with ≥1 false alarm: {records_with_fp}\")\n",
        "    print(f\"Total false positive events: {total_fp_events}\")\n",
        "    print(f\"Mean FP per 30 min: {total_fp_events / total_records:.3f}\")\n",
        "    print(f\"False positives per hour: {fp_per_hour:.3f}\")\n",
        "    print(\"=================================================\")\n",
        "else:\n",
        "    print(\"No records processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQIdAB5TaxGk"
      },
      "source": [
        "<h1>25/12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83JAKu9saVIV"
      },
      "source": [
        "<h3>New pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn55vhfnaYJ_"
      },
      "source": [
        "Gemini code instead of GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFggWV3-aU64",
        "outputId": "ff67b26b-4cb9-447b-e1ba-6f5aef3e516b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from /content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\n",
            "Record     | Annotated  | Predicted  | Diff (s)   | Status\n",
            "-----------------------------------------------------------------\n",
            "cu01       | 214        | 231        | 17.00      | LATE\n",
            "cu03       | 465        | 480        | 15.00      | LATE\n",
            "cu04       | 155        | 253        | 98.00      | LATE\n",
            "cu05       | 358        | 99         | -259.00    | EARLY\n",
            "cu06       | 184        | 41         | -143.00    | EARLY\n",
            "cu07       | 182        | 161        | -21.00     | EARLY\n",
            "cu08       | 426        | 62         | -364.00    | EARLY\n",
            "cu09       | 239        | 40         | -199.00    | EARLY\n",
            "cu10       | 316        | 235        | -81.00     | EARLY\n",
            "cu11       | 371        | 40         | -331.00    | EARLY\n",
            "cu12       | 261        | 42         | -219.00    | EARLY\n",
            "cu13       | 427        | 426        | -1.00      | EARLY\n",
            "cu15       | 405        | 85         | -320.00    | EARLY\n",
            "cu16       | 254        | 47         | -207.00    | EARLY\n",
            "cu17       | 382        | 403        | 21.00      | LATE\n",
            "cu18       | 334        | 344        | 10.00      | LATE\n",
            "cu19       | 409        | 271        | -138.00    | EARLY\n",
            "cu20       | 244        | 64         | -180.00    | EARLY\n",
            "cu21       | 0          | 54         | 54.00      | LATE\n",
            "cu22       | 338        | 350        | 12.00      | LATE\n",
            "cu23       | 334        | 346        | 12.00      | LATE\n",
            "cu24       | 356        | 379        | 23.00      | LATE\n",
            "cu25       | 421        | 40         | -381.00    | EARLY\n",
            "cu26       | 145        | 183        | 38.00      | LATE\n",
            "cu27       | 451        | 279        | -172.00    | EARLY\n",
            "cu28       | 496        | 41         | -455.00    | EARLY\n",
            "cu29       | 378        | 67         | -311.00    | EARLY\n",
            "cu30       | 27         | 110        | 83.00      | LATE\n",
            "cu31       | 494        | 42         | -452.00    | EARLY\n",
            "cu32       | 443        | 451        | 8.00       | LATE\n",
            "cu33       | 405        | 482        | 77.00      | LATE\n",
            "cu34       | 118        | 134        | 16.00      | LATE\n",
            "cu35       | 483        | 131        | -352.00    | EARLY\n",
            "-----------------------------------------------------------------\n",
            "Total Detected: 33\n",
            "Mean Prediction Time: -124.30 seconds (-2.07 min)\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import detrend, savgol_filter\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Thresholds\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "WARMUP_SEC = 30         # SKIP the first 30 seconds (Filter settling time)\n",
        "\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    print(f\"Model loaded from {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"Error: Model path not found at {MODEL_PATH}\")\n",
        "model.eval()\n",
        "\n",
        "# ------------------ 1. CLEANING ------------------\n",
        "def clean_signal(window):\n",
        "    # FIX: Replace NaNs with 0.0\n",
        "    sig = np.array(window, dtype=np.float32)\n",
        "    sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # A. Remove Sweat (Detrend)\n",
        "    sig = detrend(sig, type='linear')\n",
        "\n",
        "    # B. Remove Fuzz (Smoothing)\n",
        "    try:\n",
        "        sig = savgol_filter(sig, window_length=11, polyorder=3)\n",
        "    except:\n",
        "        pass\n",
        "    return sig\n",
        "\n",
        "# ------------------ 2. GATEKEEPER ------------------\n",
        "def is_mechanically_sound(window):\n",
        "    if np.std(window) < 0.001: return False # Flatline\n",
        "    if np.mean(np.abs(window) > 3.0) > 0.1: return False # Saturation\n",
        "    return True\n",
        "\n",
        "# ------------------ 3. RECONSTRUCTION ------------------\n",
        "def get_error_and_morphology(window):\n",
        "    # FIX: ONLY MEAN SUBTRACTION (No division by STD)\n",
        "    # This preserves the original Millivolt amplitude your model was trained on.\n",
        "    mu = np.mean(window)\n",
        "    norm_window = window - mu\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(norm_window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        recon_np = recon.squeeze().cpu().numpy()\n",
        "        input_np = x.squeeze().cpu().numpy()\n",
        "\n",
        "    mse = ((recon_np - input_np) ** 2).mean()\n",
        "\n",
        "    # --- MORPHOLOGY CHECK ---\n",
        "    if mse > ERROR_THRESHOLD:\n",
        "        diff = np.abs(input_np - recon_np)\n",
        "\n",
        "        # We use a fixed voltage threshold (e.g., 0.5mV) to find the QRS\n",
        "        # instead of Z-score, since we are now in raw amplitude space.\n",
        "        active_mask = np.abs(input_np) > 0.4\n",
        "\n",
        "        if np.sum(active_mask) > 0:\n",
        "            error_in_qrs = np.mean(diff[active_mask])\n",
        "            error_total = np.mean(diff)\n",
        "            is_structural = error_in_qrs > error_total\n",
        "        else:\n",
        "            is_structural = False\n",
        "\n",
        "        return mse, is_structural\n",
        "\n",
        "    return mse, True\n",
        "\n",
        "# ------------------ 4. PREDICT ------------------\n",
        "def detect_vtac_start_clinical(ecg):\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "\n",
        "    ecg = np.nan_to_num(ecg, nan=0.0)\n",
        "    error_history = []\n",
        "\n",
        "    # FIX: START LOOP AFTER WARMUP\n",
        "    start_sec = max(SEQ_LEN_SEC, WARMUP_SEC)\n",
        "\n",
        "    # Pad history for the warmup period so indices match\n",
        "    for _ in range(start_sec):\n",
        "        error_history.append(0.0)\n",
        "\n",
        "    for sec in range(start_sec, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples: break\n",
        "\n",
        "        raw_window = ecg[start:end]\n",
        "        clean_window = clean_signal(raw_window)\n",
        "\n",
        "        if not is_mechanically_sound(clean_window):\n",
        "            error_history.append(0.0)\n",
        "            continue\n",
        "\n",
        "        mse, is_structural = get_error_and_morphology(clean_window)\n",
        "\n",
        "        if mse > ERROR_THRESHOLD and not is_structural:\n",
        "            mse = 0.0\n",
        "\n",
        "        error_history.append(mse)\n",
        "\n",
        "    error_history = np.array(error_history)\n",
        "\n",
        "    for i in range(len(error_history) - CONSECUTIVE_SEC + 1):\n",
        "        # Skip if we are still in the padding/warmup zone\n",
        "        if i < WARMUP_SEC: continue\n",
        "\n",
        "        recent_errors = error_history[i : i + CONSECUTIVE_SEC]\n",
        "\n",
        "        if np.all(recent_errors > ERROR_THRESHOLD):\n",
        "            # Check for sudden jump\n",
        "            if i > 5:\n",
        "                prev_baseline = np.mean(error_history[i-5 : i])\n",
        "                current_level = np.mean(recent_errors)\n",
        "                if (current_level - prev_baseline) > 0.5:\n",
        "                    continue\n",
        "\n",
        "            return i + SEQ_LEN_SEC\n",
        "\n",
        "    return None\n",
        "\n",
        "# ==================================================\n",
        "#           MAIN EVALUATION LOOP\n",
        "# ==================================================\n",
        "\n",
        "print(f\"{'Record':<10} | {'Annotated':<10} | {'Predicted':<10} | {'Diff (s)':<10} | {'Status'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx in range(1, 2):\n",
        "    rec_name = f\"cu{idx:02d}\"\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "        ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    ecg = record.p_signal[:, 0]\n",
        "\n",
        "    vtac_annot_sec = None\n",
        "    for sym, samp in zip(ann.symbol, ann.sample):\n",
        "        if sym == '[':\n",
        "            vtac_annot_sec = samp // FS\n",
        "            break\n",
        "\n",
        "    if vtac_annot_sec is None: continue\n",
        "\n",
        "    vtac_pred_sec = detect_vtac_start_clinical(ecg)\n",
        "\n",
        "    if vtac_pred_sec is None:\n",
        "        print(f\"{rec_name:<10} | {vtac_annot_sec:<10} | {'---':<10} | {'---':<10} | MISSED\")\n",
        "    else:\n",
        "        diff_sec = vtac_pred_sec - vtac_annot_sec\n",
        "        results.append(diff_sec)\n",
        "\n",
        "        status = \"EARLY\" if diff_sec < 0 else \"LATE\"\n",
        "        print(f\"{rec_name:<10} | {vtac_annot_sec:<10} | {vtac_pred_sec:<10} | {diff_sec:<10.2f} | {status}\")\n",
        "\n",
        "# ==================================================\n",
        "#               FINAL SUMMARY\n",
        "# ==================================================\n",
        "print(\"-\" * 65)\n",
        "if len(results) == 0:\n",
        "    print(\"No valid detections.\")\n",
        "else:\n",
        "    mean_diff = np.mean(results)\n",
        "    std_diff = np.std(results)\n",
        "    print(f\"Total Detected: {len(results)}\")\n",
        "    print(f\"Mean Prediction Time: {mean_diff:.2f} seconds ({mean_diff/60:.2f} min)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation on NSR"
      ],
      "metadata": {
        "id": "J3LU7DNGfDWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import detrend, savgol_filter\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Clinical Thresholds\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "WARMUP_SEC = 30         # Filter settling time\n",
        "\n",
        "# Evaluation Settings\n",
        "MAX_MINUTES = 30        # Analyze first 30 mins of each record\n",
        "NSRDB_PATH = \"/content/drive/MyDrive/nsrdb\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    print(f\"Model loaded from {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"Error: Model path not found at {MODEL_PATH}\")\n",
        "model.eval()\n",
        "\n",
        "# ------------------ 1. CLEANING ------------------\n",
        "def clean_signal(window):\n",
        "    sig = np.array(window, dtype=np.float32)\n",
        "    sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sig = detrend(sig, type='linear')\n",
        "    try:\n",
        "        sig = savgol_filter(sig, window_length=11, polyorder=3)\n",
        "    except:\n",
        "        pass\n",
        "    return sig\n",
        "\n",
        "# ------------------ 2. GATEKEEPER ------------------\n",
        "def is_mechanically_sound(window):\n",
        "    if np.std(window) < 0.001: return False\n",
        "    if np.mean(np.abs(window) > 3.0) > 0.1: return False\n",
        "    return True\n",
        "\n",
        "# ------------------ 3. RECONSTRUCTION ------------------\n",
        "def get_error_and_morphology(window):\n",
        "    mu = np.mean(window)\n",
        "    norm_window = window - mu\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(norm_window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        recon_np = recon.squeeze().cpu().numpy()\n",
        "        input_np = x.squeeze().cpu().numpy()\n",
        "\n",
        "    mse = ((recon_np - input_np) ** 2).mean()\n",
        "\n",
        "    if mse > ERROR_THRESHOLD:\n",
        "        diff = np.abs(input_np - recon_np)\n",
        "        active_mask = np.abs(input_np) > 0.4\n",
        "\n",
        "        if np.sum(active_mask) > 0:\n",
        "            error_in_qrs = np.mean(diff[active_mask])\n",
        "            error_total = np.mean(diff)\n",
        "            is_structural = error_in_qrs > error_total\n",
        "        else:\n",
        "            is_structural = False\n",
        "        return mse, is_structural\n",
        "\n",
        "    return mse, True\n",
        "\n",
        "# ------------------ 4. PREDICT & COUNT ------------------\n",
        "def count_false_positives(ecg):\n",
        "    \"\"\"\n",
        "    Scans the entire signal and counts DISTINCT false positive events.\n",
        "    \"\"\"\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "\n",
        "    ecg = np.nan_to_num(ecg, nan=0.0)\n",
        "    error_history = []\n",
        "\n",
        "    # Warmup padding\n",
        "    start_sec = max(SEQ_LEN_SEC, WARMUP_SEC)\n",
        "    for _ in range(start_sec):\n",
        "        error_history.append(0.0)\n",
        "\n",
        "    # --- Pass 1: Calculate Errors for the whole file ---\n",
        "    for sec in range(start_sec, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples: break\n",
        "\n",
        "        raw_window = ecg[start:end]\n",
        "        clean_window = clean_signal(raw_window)\n",
        "\n",
        "        if not is_mechanically_sound(clean_window):\n",
        "            error_history.append(0.0)\n",
        "            continue\n",
        "\n",
        "        mse, is_structural = get_error_and_morphology(clean_window)\n",
        "\n",
        "        if mse > ERROR_THRESHOLD and not is_structural:\n",
        "            mse = 0.0\n",
        "\n",
        "        error_history.append(mse)\n",
        "\n",
        "    # --- Pass 2: Count Distinct Events ---\n",
        "    error_history = np.array(error_history)\n",
        "    fp_events = 0\n",
        "    i = WARMUP_SEC\n",
        "\n",
        "    # We use a while loop so we can skip ahead when an event is found\n",
        "    while i < len(error_history) - CONSECUTIVE_SEC + 1:\n",
        "\n",
        "        recent_errors = error_history[i : i + CONSECUTIVE_SEC]\n",
        "\n",
        "        # Check for persistent error\n",
        "        if np.all(recent_errors > ERROR_THRESHOLD):\n",
        "\n",
        "            # Artifact Rejection (Sudden Jump check)\n",
        "            is_artifact = False\n",
        "            if i > 5:\n",
        "                prev_baseline = np.mean(error_history[i-5 : i])\n",
        "                current_level = np.mean(recent_errors)\n",
        "                if (current_level - prev_baseline) > 0.5:\n",
        "                    is_artifact = True\n",
        "\n",
        "            if not is_artifact:\n",
        "                # Valid FP Event Found!\n",
        "                fp_events += 1\n",
        "\n",
        "                # SKIP LOGIC: Jump forward to avoid counting this same event multiple times.\n",
        "                # We skip the length of the event (CONSECUTIVE_SEC)\n",
        "                i += CONSECUTIVE_SEC\n",
        "                continue\n",
        "\n",
        "        # If no event, move to next second\n",
        "        i += 1\n",
        "\n",
        "    return fp_events\n",
        "\n",
        "# ==================================================\n",
        "#           NSRDB FALSE POSITIVE TEST\n",
        "# ==================================================\n",
        "\n",
        "print(f\"{'Record':<10} | {'Duration (m)':<12} | {'FP Events':<10}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "total_records = 0\n",
        "total_fp_events = 0\n",
        "total_hours_analyzed = 0.0\n",
        "\n",
        "try:\n",
        "    files = sorted([f.replace('.hea', '') for f in os.listdir(NSRDB_PATH) if f.endswith('.hea')])\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: NSRDB path not found at {NSRDB_PATH}\")\n",
        "    files = []\n",
        "\n",
        "for rec_name in files:\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(NSRDB_PATH, rec_name), sampto=MAX_MINUTES*60*FS)\n",
        "        ecg = record.p_signal[:, 0]\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    total_records += 1\n",
        "\n",
        "    # Calculate actual duration in hours (some records might be shorter than MAX_MINUTES)\n",
        "    duration_hours = (len(ecg) / FS) / 3600.0\n",
        "    total_hours_analyzed += duration_hours\n",
        "\n",
        "    # Count Events\n",
        "    events = count_false_positives(ecg)\n",
        "    total_fp_events += events\n",
        "\n",
        "    status_str = \"CLEAN\" if events == 0 else str(events)\n",
        "    print(f\"{rec_name:<10} | {duration_hours*60:<12.1f} | {status_str:<10}\")\n",
        "\n",
        "# ==================================================\n",
        "#               FINAL SUMMARY\n",
        "# ==================================================\n",
        "print(\"-\" * 60)\n",
        "if total_records > 0 and total_hours_analyzed > 0:\n",
        "    fp_per_hour = total_fp_events / total_hours_analyzed\n",
        "\n",
        "    print(f\"Total Records:      {total_records}\")\n",
        "    print(f\"Total Hours:        {total_hours_analyzed:.2f}\")\n",
        "    print(f\"Total FP Events:    {total_fp_events}\")\n",
        "    print(f\"False Pos / Hour:   {fp_per_hour:.2f}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    if fp_per_hour < 1.0:\n",
        "        print(\"[EXCELLENT] Clinical grade performance (< 1 FP/hour).\")\n",
        "    elif fp_per_hour < 5.0:\n",
        "        print(\"[GOOD] Acceptable for ward monitoring, maybe noisy for ICU.\")\n",
        "    else:\n",
        "        print(\"[HIGH] Too many alarms. Needs threshold tuning.\")\n",
        "else:\n",
        "    print(\"No records processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI4Nad6IfE4V",
        "outputId": "459e346e-5343-4347-95b7-b40e24859987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\n",
            "Record     | Duration (m) | FP Events \n",
            "---------------------------------------------\n",
            "16265      | 30.0         | 2         \n",
            "16272      | 30.0         | CLEAN     \n",
            "16273      | 30.0         | 1         \n",
            "16420      | 30.0         | 1         \n",
            "16483      | 30.0         | CLEAN     \n",
            "16539      | 30.0         | CLEAN     \n",
            "16773      | 30.0         | 1         \n",
            "16786      | 30.0         | CLEAN     \n",
            "16795      | 30.0         | CLEAN     \n",
            "17052      | 30.0         | CLEAN     \n",
            "17453      | 30.0         | CLEAN     \n",
            "18177      | 30.0         | CLEAN     \n",
            "18184      | 30.0         | CLEAN     \n",
            "19088      | 30.0         | CLEAN     \n",
            "19090      | 30.0         | CLEAN     \n",
            "19093      | 30.0         | CLEAN     \n",
            "19140      | 30.0         | CLEAN     \n",
            "19830      | 30.0         | 23        \n",
            "------------------------------------------------------------\n",
            "Total Records:      18\n",
            "Total Hours:        9.00\n",
            "Total FP Events:    28\n",
            "False Pos / Hour:   3.11\n",
            "------------------------------------------------------------\n",
            "[GOOD] Acceptable for ward monitoring, maybe noisy for ICU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing on VFDB now"
      ],
      "metadata": {
        "id": "hoGxolrFoMvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the db"
      ],
      "metadata": {
        "id": "93zaJ75uoPVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import os\n",
        "\n",
        "# Define where you want to save it\n",
        "VFDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/VFDB\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(VFDB_PATH):\n",
        "    os.makedirs(VFDB_PATH)\n",
        "\n",
        "print(f\"Downloading VFDB to {VFDB_PATH}...\")\n",
        "\n",
        "try:\n",
        "    # 'vfdb' is the PhysioNet identifier\n",
        "    wfdb.dl_database('vfdb', VFDB_PATH)\n",
        "    print(\"Download complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsIEFUHVoQ4g",
        "outputId": "a6ac5b7f-3886-46ea-a326-36b5c5d0b3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading VFDB to /content/drive/MyDrive/ECG_Datasets/VFDB...\n",
            "Generating record list for: 418\n",
            "Generating record list for: 419\n",
            "Generating record list for: 420\n",
            "Generating record list for: 421\n",
            "Generating record list for: 422\n",
            "Generating record list for: 423\n",
            "Generating record list for: 424\n",
            "Generating record list for: 425\n",
            "Generating record list for: 426\n",
            "Generating record list for: 427\n",
            "Generating record list for: 428\n",
            "Generating record list for: 429\n",
            "Generating record list for: 430\n",
            "Generating record list for: 602\n",
            "Generating record list for: 605\n",
            "Generating record list for: 607\n",
            "Generating record list for: 609\n",
            "Generating record list for: 610\n",
            "Generating record list for: 611\n",
            "Generating record list for: 612\n",
            "Generating record list for: 614\n",
            "Generating record list for: 615\n",
            "Generating list of all files for: 418\n",
            "Generating list of all files for: 419\n",
            "Generating list of all files for: 420\n",
            "Generating list of all files for: 421\n",
            "Generating list of all files for: 422\n",
            "Generating list of all files for: 423\n",
            "Generating list of all files for: 424\n",
            "Generating list of all files for: 425\n",
            "Generating list of all files for: 426\n",
            "Generating list of all files for: 427\n",
            "Generating list of all files for: 428\n",
            "Generating list of all files for: 429\n",
            "Generating list of all files for: 430\n",
            "Generating list of all files for: 602\n",
            "Generating list of all files for: 605\n",
            "Generating list of all files for: 607\n",
            "Generating list of all files for: 609\n",
            "Generating list of all files for: 610\n",
            "Generating list of all files for: 611\n",
            "Generating list of all files for: 612\n",
            "Generating list of all files for: 614\n",
            "Generating list of all files for: 615\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now testing"
      ],
      "metadata": {
        "id": "vJNQ0KrvowvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import detrend, savgol_filter\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Clinical Thresholds\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "WARMUP_SEC = 30\n",
        "\n",
        "# Dataset Paths\n",
        "VFDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/VFDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    print(f\"Model loaded from {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"Error: Model path not found at {MODEL_PATH}\")\n",
        "model.eval()\n",
        "\n",
        "# ------------------ 1. CLEANING ------------------\n",
        "def clean_signal(window):\n",
        "    sig = np.array(window, dtype=np.float32)\n",
        "    sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sig = detrend(sig, type='linear')\n",
        "    try:\n",
        "        sig = savgol_filter(sig, window_length=11, polyorder=3)\n",
        "    except:\n",
        "        pass\n",
        "    return sig\n",
        "\n",
        "# ------------------ 2. GATEKEEPER ------------------\n",
        "def is_mechanically_sound(window):\n",
        "    if np.std(window) < 0.001: return False\n",
        "    if np.mean(np.abs(window) > 3.0) > 0.1: return False\n",
        "    return True\n",
        "\n",
        "# ------------------ 3. RECONSTRUCTION ------------------\n",
        "def get_error_and_morphology(window):\n",
        "    mu = np.mean(window)\n",
        "    norm_window = window - mu\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(norm_window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        recon_np = recon.squeeze().cpu().numpy()\n",
        "        input_np = x.squeeze().cpu().numpy()\n",
        "\n",
        "    mse = ((recon_np - input_np) ** 2).mean()\n",
        "\n",
        "    if mse > ERROR_THRESHOLD:\n",
        "        diff = np.abs(input_np - recon_np)\n",
        "        active_mask = np.abs(input_np) > 0.4\n",
        "\n",
        "        if np.sum(active_mask) > 0:\n",
        "            error_in_qrs = np.mean(diff[active_mask])\n",
        "            error_total = np.mean(diff)\n",
        "            is_structural = error_in_qrs > error_total\n",
        "        else:\n",
        "            is_structural = False\n",
        "\n",
        "        return mse, is_structural\n",
        "\n",
        "    return mse, True\n",
        "\n",
        "# ------------------ 4. PREDICT ------------------\n",
        "def detect_vtac_start_clinical(ecg):\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "\n",
        "    ecg = np.nan_to_num(ecg, nan=0.0)\n",
        "    error_history = []\n",
        "\n",
        "    start_sec = max(SEQ_LEN_SEC, WARMUP_SEC)\n",
        "    for _ in range(start_sec):\n",
        "        error_history.append(0.0)\n",
        "\n",
        "    for sec in range(start_sec, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples: break\n",
        "\n",
        "        raw_window = ecg[start:end]\n",
        "        clean_window = clean_signal(raw_window)\n",
        "\n",
        "        if not is_mechanically_sound(clean_window):\n",
        "            error_history.append(0.0)\n",
        "            continue\n",
        "\n",
        "        mse, is_structural = get_error_and_morphology(clean_window)\n",
        "\n",
        "        if mse > ERROR_THRESHOLD and not is_structural:\n",
        "            mse = 0.0\n",
        "\n",
        "        error_history.append(mse)\n",
        "\n",
        "    error_history = np.array(error_history)\n",
        "\n",
        "    for i in range(len(error_history) - CONSECUTIVE_SEC + 1):\n",
        "        if i < WARMUP_SEC: continue\n",
        "\n",
        "        recent_errors = error_history[i : i + CONSECUTIVE_SEC]\n",
        "\n",
        "        if np.all(recent_errors > ERROR_THRESHOLD):\n",
        "            if i > 5:\n",
        "                prev_baseline = np.mean(error_history[i-5 : i])\n",
        "                current_level = np.mean(recent_errors)\n",
        "                if (current_level - prev_baseline) > 0.5:\n",
        "                    continue\n",
        "\n",
        "            return i + SEQ_LEN_SEC\n",
        "\n",
        "    return None\n",
        "\n",
        "# ==================================================\n",
        "#           MAIN EVALUATION LOOP (VFDB)\n",
        "# ==================================================\n",
        "\n",
        "print(f\"{'Record':<10} | {'Annotated':<10} | {'Predicted':<10} | {'Diff (s)':<10} | {'Status'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "results = []\n",
        "\n",
        "# 1. FILE CHECK\n",
        "try:\n",
        "    if not os.path.exists(VFDB_PATH):\n",
        "        print(f\"CRITICAL: VFDB Folder not found at {VFDB_PATH}\")\n",
        "        files = []\n",
        "    else:\n",
        "        files = sorted([f.replace('.dat', '') for f in os.listdir(VFDB_PATH) if f.endswith('.dat')])\n",
        "        if len(files) == 0:\n",
        "            print(f\"CRITICAL: No .dat files found in {VFDB_PATH}. Did the download finish?\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading directory: {e}\")\n",
        "    files = []\n",
        "\n",
        "for rec_name in files:\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(VFDB_PATH, rec_name))\n",
        "        ann = wfdb.rdann(os.path.join(VFDB_PATH, rec_name), 'atr')\n",
        "    except Exception:\n",
        "        # print(f\"Skipping {rec_name}, load failed.\")\n",
        "        continue\n",
        "\n",
        "    # VFDB has 2 leads. Lead 0 is usually ECG.\n",
        "    ecg = record.p_signal[:, 0]\n",
        "\n",
        "    # Fallback: If Lead 0 is flat, try Lead 1\n",
        "    if np.std(ecg) < 0.05 and record.p_signal.shape[1] > 1:\n",
        "        ecg = record.p_signal[:, 1]\n",
        "\n",
        "    # -------- UPDATED VTAC ANNOTATION LOGIC --------\n",
        "    vtac_annot_sec = None\n",
        "\n",
        "    # VFDB uses text notes like \"(VT\" or \"(VFL\" in addition to symbols\n",
        "    for i in range(len(ann.sample)):\n",
        "        sym = ann.symbol[i]\n",
        "\n",
        "        # Check text note if available\n",
        "        aux = ann.aux_note[i] if hasattr(ann, 'aux_note') else \"\"\n",
        "\n",
        "        # Condition 1: Strict VF symbol '['\n",
        "        if sym == '[':\n",
        "            vtac_annot_sec = ann.sample[i] // FS\n",
        "            break\n",
        "\n",
        "        # Condition 2: Rhythm Change '+' with Text Note\n",
        "        if sym == '+' and ('(VT' in aux or '(VFL' in aux):\n",
        "            vtac_annot_sec = ann.sample[i] // FS\n",
        "            break\n",
        "\n",
        "    if vtac_annot_sec is None:\n",
        "        # print(f\"Skipping {rec_name}, no VT/VF annotation found.\")\n",
        "        continue\n",
        "\n",
        "    # -------- Clinical Detection --------\n",
        "    vtac_pred_sec = detect_vtac_start_clinical(ecg)\n",
        "\n",
        "    # -------- Result Logging --------\n",
        "    if vtac_pred_sec is None:\n",
        "        print(f\"{rec_name:<10} | {vtac_annot_sec:<10} | {'---':<10} | {'---':<10} | MISSED\")\n",
        "    else:\n",
        "        diff_sec = vtac_pred_sec - vtac_annot_sec\n",
        "        results.append(diff_sec)\n",
        "\n",
        "        status = \"EARLY\" if diff_sec < 0 else \"LATE\"\n",
        "        print(f\"{rec_name:<10} | {vtac_annot_sec:<10} | {vtac_pred_sec:<10} | {diff_sec:<10.2f} | {status}\")\n",
        "\n",
        "# ==================================================\n",
        "#               FINAL SUMMARY\n",
        "# ==================================================\n",
        "print(\"-\" * 65)\n",
        "if len(results) == 0:\n",
        "    print(\"No valid detections.\")\n",
        "else:\n",
        "    mean_diff = np.mean(results)\n",
        "    std_diff = np.std(results)\n",
        "    print(f\"Total Detected: {len(results)}\")\n",
        "    print(f\"Mean Prediction Time: {mean_diff:.2f} seconds ({mean_diff/60:.2f} min)\")\n",
        "    print(f\"Standard Deviation: {std_diff:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aT_A9O0ox3z",
        "outputId": "6020f8d0-1165-4022-86c5-bcaa358eceed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\n",
            "Record     | Annotated  | Predicted  | Diff (s)   | Status\n",
            "-----------------------------------------------------------------\n",
            "418        | 398        | 410        | 12.00      | LATE\n",
            "419        | 40         | 671        | 631.00     | LATE\n",
            "420        | 1430       | 211        | -1219.00   | EARLY\n",
            "421        | 894        | 158        | -736.00    | EARLY\n",
            "422        | 1332       | 293        | -1039.00   | EARLY\n",
            "423        | 898        | 202        | -696.00    | EARLY\n",
            "424        | 1475       | 1844       | 369.00     | LATE\n",
            "425        | 929        | ---        | ---        | MISSED\n",
            "426        | 1667       | 1199       | -468.00    | EARLY\n",
            "427        | 648        | ---        | ---        | MISSED\n",
            "428        | 1687       | ---        | ---        | MISSED\n",
            "429        | 402        | 892        | 490.00     | LATE\n",
            "430        | 207        | ---        | ---        | MISSED\n",
            "602        | 485        | 65         | -420.00    | EARLY\n",
            "605        | 1647       | 1667       | 20.00      | LATE\n",
            "607        | 133        | 1694       | 1561.00    | LATE\n",
            "609        | 1021       | 41         | -980.00    | EARLY\n",
            "610        | 1839       | 41         | -1798.00   | EARLY\n",
            "611        | 1196       | 42         | -1154.00   | EARLY\n",
            "612        | 1707       | 56         | -1651.00   | EARLY\n",
            "614        | 1423       | 112        | -1311.00   | EARLY\n",
            "615        | 326        | 40         | -286.00    | EARLY\n",
            "-----------------------------------------------------------------\n",
            "Total Detected: 18\n",
            "Mean Prediction Time: -481.94 seconds (-8.03 min)\n",
            "Standard Deviation: 852.09 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now validating on NSR of Fantasia db`"
      ],
      "metadata": {
        "id": "GGUHG-SptT7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading"
      ],
      "metadata": {
        "id": "RYPW3aS1tYI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import os\n",
        "\n",
        "FANTASIA_PATH = \"/content/drive/MyDrive/ECG_Datasets/Fantasia\"\n",
        "\n",
        "if not os.path.exists(FANTASIA_PATH):\n",
        "    os.makedirs(FANTASIA_PATH)\n",
        "\n",
        "print(f\"Downloading Fantasia Database to {FANTASIA_PATH}...\")\n",
        "\n",
        "try:\n",
        "    # 'fantasia' is the PhysioNet identifier\n",
        "    wfdb.dl_database('fantasia', FANTASIA_PATH)\n",
        "    print(\"Download complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZcsgWpFtZZ0",
        "outputId": "f05f2a87-2241-4907-d76e-daf227280bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Fantasia Database to /content/drive/MyDrive/ECG_Datasets/Fantasia...\n",
            "Generating record list for: f1o01\n",
            "Generating record list for: f1o02\n",
            "Generating record list for: f1o03\n",
            "Generating record list for: f1o04\n",
            "Generating record list for: f1o05\n",
            "Generating record list for: f1o06\n",
            "Generating record list for: f1o07\n",
            "Generating record list for: f1o08\n",
            "Generating record list for: f1o09\n",
            "Generating record list for: f1o10\n",
            "Generating record list for: f1y01\n",
            "Generating record list for: f1y02\n",
            "Generating record list for: f1y03\n",
            "Generating record list for: f1y04\n",
            "Generating record list for: f1y05\n",
            "Generating record list for: f1y06\n",
            "Generating record list for: f1y07\n",
            "Generating record list for: f1y08\n",
            "Generating record list for: f1y09\n",
            "Generating record list for: f1y10\n",
            "Generating record list for: f2o01\n",
            "Generating record list for: f2o02\n",
            "Generating record list for: f2o03\n",
            "Generating record list for: f2o04\n",
            "Generating record list for: f2o05\n",
            "Generating record list for: f2o06\n",
            "Generating record list for: f2o07\n",
            "Generating record list for: f2o08\n",
            "Generating record list for: f2o09\n",
            "Generating record list for: f2o10\n",
            "Generating record list for: f2y01\n",
            "Generating record list for: f2y02\n",
            "Generating record list for: f2y03\n",
            "Generating record list for: f2y04\n",
            "Generating record list for: f2y05\n",
            "Generating record list for: f2y06\n",
            "Generating record list for: f2y07\n",
            "Generating record list for: f2y08\n",
            "Generating record list for: f2y09\n",
            "Generating record list for: f2y10\n",
            "Generating list of all files for: f1o01\n",
            "Generating list of all files for: f1o02\n",
            "Generating list of all files for: f1o03\n",
            "Generating list of all files for: f1o04\n",
            "Generating list of all files for: f1o05\n",
            "Generating list of all files for: f1o06\n",
            "Generating list of all files for: f1o07\n",
            "Generating list of all files for: f1o08\n",
            "Generating list of all files for: f1o09\n",
            "Generating list of all files for: f1o10\n",
            "Generating list of all files for: f1y01\n",
            "Generating list of all files for: f1y02\n",
            "Generating list of all files for: f1y03\n",
            "Generating list of all files for: f1y04\n",
            "Generating list of all files for: f1y05\n",
            "Generating list of all files for: f1y06\n",
            "Generating list of all files for: f1y07\n",
            "Generating list of all files for: f1y08\n",
            "Generating list of all files for: f1y09\n",
            "Generating list of all files for: f1y10\n",
            "Generating list of all files for: f2o01\n",
            "Generating list of all files for: f2o02\n",
            "Generating list of all files for: f2o03\n",
            "Generating list of all files for: f2o04\n",
            "Generating list of all files for: f2o05\n",
            "Generating list of all files for: f2o06\n",
            "Generating list of all files for: f2o07\n",
            "Generating list of all files for: f2o08\n",
            "Generating list of all files for: f2o09\n",
            "Generating list of all files for: f2o10\n",
            "Generating list of all files for: f2y01\n",
            "Generating list of all files for: f2y02\n",
            "Generating list of all files for: f2y03\n",
            "Generating list of all files for: f2y04\n",
            "Generating list of all files for: f2y05\n",
            "Generating list of all files for: f2y06\n",
            "Generating list of all files for: f2y07\n",
            "Generating list of all files for: f2y08\n",
            "Generating list of all files for: f2y09\n",
            "Generating list of all files for: f2y10\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "8ryv4z0fthPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import detrend, savgol_filter\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Clinical Thresholds\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "WARMUP_SEC = 30\n",
        "\n",
        "# Dataset Settings\n",
        "MAX_MINUTES = 30        # Analyze first 30 mins\n",
        "FANTASIA_PATH = \"/content/drive/MyDrive/ECG_Datasets/Fantasia\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    print(f\"Model loaded from {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"Error: Model path not found at {MODEL_PATH}\")\n",
        "model.eval()\n",
        "\n",
        "# ------------------ 1. CLEANING ------------------\n",
        "def clean_signal(window):\n",
        "    sig = np.array(window, dtype=np.float32)\n",
        "    sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sig = detrend(sig, type='linear')\n",
        "    try:\n",
        "        sig = savgol_filter(sig, window_length=11, polyorder=3)\n",
        "    except:\n",
        "        pass\n",
        "    return sig\n",
        "\n",
        "# ------------------ 2. GATEKEEPER ------------------\n",
        "def is_mechanically_sound(window):\n",
        "    if np.std(window) < 0.001: return False\n",
        "    if np.mean(np.abs(window) > 3.0) > 0.1: return False\n",
        "    return True\n",
        "\n",
        "# ------------------ 3. RECONSTRUCTION ------------------\n",
        "def get_error_and_morphology(window):\n",
        "    mu = np.mean(window)\n",
        "    norm_window = window - mu\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(norm_window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        recon_np = recon.squeeze().cpu().numpy()\n",
        "        input_np = x.squeeze().cpu().numpy()\n",
        "\n",
        "    mse = ((recon_np - input_np) ** 2).mean()\n",
        "\n",
        "    if mse > ERROR_THRESHOLD:\n",
        "        diff = np.abs(input_np - recon_np)\n",
        "        active_mask = np.abs(input_np) > 0.4\n",
        "\n",
        "        if np.sum(active_mask) > 0:\n",
        "            error_in_qrs = np.mean(diff[active_mask])\n",
        "            error_total = np.mean(diff)\n",
        "            is_structural = error_in_qrs > error_total\n",
        "        else:\n",
        "            is_structural = False\n",
        "\n",
        "        return mse, is_structural\n",
        "\n",
        "    return mse, True\n",
        "\n",
        "# ------------------ 4. COUNT FALSE POSITIVES ------------------\n",
        "def count_false_positives(ecg):\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "\n",
        "    ecg = np.nan_to_num(ecg, nan=0.0)\n",
        "    error_history = []\n",
        "\n",
        "    start_sec = max(SEQ_LEN_SEC, WARMUP_SEC)\n",
        "    for _ in range(start_sec):\n",
        "        error_history.append(0.0)\n",
        "\n",
        "    # Pass 1: Calc Errors\n",
        "    for sec in range(start_sec, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples: break\n",
        "\n",
        "        raw_window = ecg[start:end]\n",
        "        clean_window = clean_signal(raw_window)\n",
        "\n",
        "        if not is_mechanically_sound(clean_window):\n",
        "            error_history.append(0.0)\n",
        "            continue\n",
        "\n",
        "        mse, is_structural = get_error_and_morphology(clean_window)\n",
        "\n",
        "        if mse > ERROR_THRESHOLD and not is_structural:\n",
        "            mse = 0.0\n",
        "\n",
        "        error_history.append(mse)\n",
        "\n",
        "    # Pass 2: Count Events\n",
        "    error_history = np.array(error_history)\n",
        "    fp_events = 0\n",
        "    i = WARMUP_SEC\n",
        "\n",
        "    while i < len(error_history) - CONSECUTIVE_SEC + 1:\n",
        "        recent_errors = error_history[i : i + CONSECUTIVE_SEC]\n",
        "\n",
        "        if np.all(recent_errors > ERROR_THRESHOLD):\n",
        "            is_artifact = False\n",
        "            if i > 5:\n",
        "                prev_baseline = np.mean(error_history[i-5 : i])\n",
        "                current_level = np.mean(recent_errors)\n",
        "                if (current_level - prev_baseline) > 0.5:\n",
        "                    is_artifact = True\n",
        "\n",
        "            if not is_artifact:\n",
        "                fp_events += 1\n",
        "                i += CONSECUTIVE_SEC\n",
        "                continue\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return fp_events\n",
        "\n",
        "# ==================================================\n",
        "#           FANTASIA EVALUATION LOOP\n",
        "# ==================================================\n",
        "\n",
        "print(f\"{'Record':<10} | {'Group':<10} | {'FP Events':<10}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "total_records = 0\n",
        "total_fp_events = 0\n",
        "total_hours_analyzed = 0.0\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(FANTASIA_PATH):\n",
        "        print(\"Error: Fantasia folder not found.\")\n",
        "        files = []\n",
        "    else:\n",
        "        files = sorted([f.replace('.dat', '') for f in os.listdir(FANTASIA_PATH) if f.endswith('.dat')])\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    files = []\n",
        "\n",
        "for rec_name in files:\n",
        "    try:\n",
        "        # Load record\n",
        "        record = wfdb.rdrecord(os.path.join(FANTASIA_PATH, rec_name), sampto=MAX_MINUTES*60*FS)\n",
        "\n",
        "        # Fantasia has 'ECG' and 'RESP'. We need to find the ECG channel.\n",
        "        ecg_idx = 0\n",
        "        if 'ECG' in record.sig_name:\n",
        "            ecg_idx = record.sig_name.index('ECG')\n",
        "        else:\n",
        "            # Heuristic: ECG usually has higher variance than Respiration\n",
        "            std0 = np.std(record.p_signal[:, 0])\n",
        "            std1 = np.std(record.p_signal[:, 1])\n",
        "            ecg_idx = 0 if std0 > std1 else 1\n",
        "\n",
        "        ecg = record.p_signal[:, ecg_idx]\n",
        "\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    total_records += 1\n",
        "\n",
        "    duration_hours = (len(ecg) / FS) / 3600.0\n",
        "    total_hours_analyzed += duration_hours\n",
        "\n",
        "    # Identify Group based on filename (f1y = young, f1o = old)\n",
        "    group = \"Young\" if 'y' in rec_name else \"Elderly\"\n",
        "\n",
        "    # Count Events\n",
        "    events = count_false_positives(ecg)\n",
        "    total_fp_events += events\n",
        "\n",
        "    status_str = \"CLEAN\" if events == 0 else str(events)\n",
        "    print(f\"{rec_name:<10} | {group:<10} | {status_str:<10}\")\n",
        "\n",
        "# ==================================================\n",
        "#               FINAL SUMMARY\n",
        "# ==================================================\n",
        "\n",
        "\n",
        "print(\"-\" * 60)\n",
        "if total_records > 0 and total_hours_analyzed > 0:\n",
        "    fp_per_hour = total_fp_events / total_hours_analyzed\n",
        "\n",
        "    print(f\"Total Records:      {total_records}\")\n",
        "    print(f\"Total Hours:        {total_hours_analyzed:.2f}\")\n",
        "    print(f\"Total FP Events:    {total_fp_events}\")\n",
        "    print(f\"False Pos / Hour:   {fp_per_hour:.2f}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(\"Interpretation:\")\n",
        "    print(\"If 'Elderly' records have significantly more FPs than 'Young',\")\n",
        "    print(\"it means the model is sensitive to age-related benign changes.\")\n",
        "else:\n",
        "    print(\"No records processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIsUZVAWtiFG",
        "outputId": "311bf02f-8c3f-4545-c827-9d237160a3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\n",
            "Record     | Group      | FP Events \n",
            "---------------------------------------------\n",
            "f1o01      | Elderly    | CLEAN     \n",
            "f1o02      | Elderly    | CLEAN     \n",
            "f1o03      | Elderly    | CLEAN     \n",
            "f1o04      | Elderly    | 2         \n",
            "f1o05      | Elderly    | CLEAN     \n",
            "f1o06      | Elderly    | CLEAN     \n",
            "f1o07      | Elderly    | CLEAN     \n",
            "f1o08      | Elderly    | CLEAN     \n",
            "f1o09      | Elderly    | CLEAN     \n",
            "f1o10      | Elderly    | CLEAN     \n",
            "f1y01      | Young      | CLEAN     \n",
            "f1y02      | Young      | CLEAN     \n",
            "f1y03      | Young      | CLEAN     \n",
            "f1y04      | Young      | CLEAN     \n",
            "f1y05      | Young      | 1         \n",
            "f1y06      | Young      | CLEAN     \n",
            "f1y07      | Young      | 45        \n",
            "f1y08      | Young      | CLEAN     \n",
            "f1y09      | Young      | CLEAN     \n",
            "f1y10      | Young      | CLEAN     \n",
            "f2o01      | Elderly    | CLEAN     \n",
            "f2o02      | Elderly    | CLEAN     \n",
            "f2o03      | Elderly    | CLEAN     \n",
            "f2o04      | Elderly    | CLEAN     \n",
            "f2o05      | Elderly    | CLEAN     \n",
            "f2o06      | Elderly    | CLEAN     \n",
            "f2o07      | Elderly    | CLEAN     \n",
            "f2o08      | Elderly    | CLEAN     \n",
            "f2o09      | Elderly    | CLEAN     \n",
            "f2o10      | Elderly    | CLEAN     \n",
            "f2y01      | Young      | CLEAN     \n",
            "f2y02      | Young      | CLEAN     \n",
            "f2y03      | Young      | CLEAN     \n",
            "f2y04      | Young      | CLEAN     \n",
            "f2y05      | Young      | CLEAN     \n",
            "f2y06      | Young      | CLEAN     \n",
            "f2y07      | Young      | CLEAN     \n",
            "f2y08      | Young      | CLEAN     \n",
            "f2y09      | Young      | 3         \n",
            "f2y10      | Young      | 3         \n",
            "------------------------------------------------------------\n",
            "Total Records:      40\n",
            "Total Hours:        20.00\n",
            "Total FP Events:    54\n",
            "False Pos / Hour:   2.70\n",
            "------------------------------------------------------------\n",
            "Interpretation:\n",
            "If 'Elderly' records have significantly more FPs than 'Young',\n",
            "it means the model is sensitive to age-related benign changes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further testing"
      ],
      "metadata": {
        "id": "uwbOsre74Zbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloads"
      ],
      "metadata": {
        "id": "B5CsLPpw4ncL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "MITDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/MITDB\"\n",
        "PTBDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/PTBDB\"\n",
        "\n",
        "for path in [MITDB_PATH, PTBDB_PATH]:\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "print(\"Downloading MIT-BIH Arrhythmia Database (MITDB)...\")\n",
        "try:\n",
        "    wfdb.dl_database('mitdb', MITDB_PATH)\n",
        "    print(\"MITDB Download complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading MITDB: {e}\")\n",
        "\n",
        "print(\"\\nDownloading PTB Diagnostic Database (PTBDB)...\")\n",
        "try:\n",
        "    # This is a large database. We download the index first to find healthy controls later,\n",
        "    # or just download the whole thing. It's safer to download all to ensure we get the headers.\n",
        "    wfdb.dl_database('ptbdb', PTBDB_PATH)\n",
        "    print(\"PTBDB Download complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading PTBDB: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BYqmv6LH4mKf",
        "outputId": "5ede8b4f-8caf-4b37-b39d-7ac96affef26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MIT-BIH Arrhythmia Database (MITDB)...\n",
            "Generating record list for: 100\n",
            "Generating record list for: 101\n",
            "Generating record list for: 102\n",
            "Generating record list for: 103\n",
            "Generating record list for: 104\n",
            "Generating record list for: 105\n",
            "Generating record list for: 106\n",
            "Generating record list for: 107\n",
            "Generating record list for: 108\n",
            "Generating record list for: 109\n",
            "Generating record list for: 111\n",
            "Generating record list for: 112\n",
            "Generating record list for: 113\n",
            "Generating record list for: 114\n",
            "Generating record list for: 115\n",
            "Generating record list for: 116\n",
            "Generating record list for: 117\n",
            "Generating record list for: 118\n",
            "Generating record list for: 119\n",
            "Generating record list for: 121\n",
            "Generating record list for: 122\n",
            "Generating record list for: 123\n",
            "Generating record list for: 124\n",
            "Generating record list for: 200\n",
            "Generating record list for: 201\n",
            "Generating record list for: 202\n",
            "Generating record list for: 203\n",
            "Generating record list for: 205\n",
            "Generating record list for: 207\n",
            "Generating record list for: 208\n",
            "Generating record list for: 209\n",
            "Generating record list for: 210\n",
            "Generating record list for: 212\n",
            "Generating record list for: 213\n",
            "Generating record list for: 214\n",
            "Generating record list for: 215\n",
            "Generating record list for: 217\n",
            "Generating record list for: 219\n",
            "Generating record list for: 220\n",
            "Generating record list for: 221\n",
            "Generating record list for: 222\n",
            "Generating record list for: 223\n",
            "Generating record list for: 228\n",
            "Generating record list for: 230\n",
            "Generating record list for: 231\n",
            "Generating record list for: 232\n",
            "Generating record list for: 233\n",
            "Generating record list for: 234\n",
            "Generating list of all files for: 100\n",
            "Generating list of all files for: 101\n",
            "Generating list of all files for: 102\n",
            "Generating list of all files for: 103\n",
            "Generating list of all files for: 104\n",
            "Generating list of all files for: 105\n",
            "Generating list of all files for: 106\n",
            "Generating list of all files for: 107\n",
            "Generating list of all files for: 108\n",
            "Generating list of all files for: 109\n",
            "Generating list of all files for: 111\n",
            "Generating list of all files for: 112\n",
            "Generating list of all files for: 113\n",
            "Generating list of all files for: 114\n",
            "Generating list of all files for: 115\n",
            "Generating list of all files for: 116\n",
            "Generating list of all files for: 117\n",
            "Generating list of all files for: 118\n",
            "Generating list of all files for: 119\n",
            "Generating list of all files for: 121\n",
            "Generating list of all files for: 122\n",
            "Generating list of all files for: 123\n",
            "Generating list of all files for: 124\n",
            "Generating list of all files for: 200\n",
            "Generating list of all files for: 201\n",
            "Generating list of all files for: 202\n",
            "Generating list of all files for: 203\n",
            "Generating list of all files for: 205\n",
            "Generating list of all files for: 207\n",
            "Generating list of all files for: 208\n",
            "Generating list of all files for: 209\n",
            "Generating list of all files for: 210\n",
            "Generating list of all files for: 212\n",
            "Generating list of all files for: 213\n",
            "Generating list of all files for: 214\n",
            "Generating list of all files for: 215\n",
            "Generating list of all files for: 217\n",
            "Generating list of all files for: 219\n",
            "Generating list of all files for: 220\n",
            "Generating list of all files for: 221\n",
            "Generating list of all files for: 222\n",
            "Generating list of all files for: 223\n",
            "Generating list of all files for: 228\n",
            "Generating list of all files for: 230\n",
            "Generating list of all files for: 231\n",
            "Generating list of all files for: 232\n",
            "Generating list of all files for: 233\n",
            "Generating list of all files for: 234\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "MITDB Download complete.\n",
            "\n",
            "Downloading PTB Diagnostic Database (PTBDB)...\n",
            "Generating record list for: patient001/s0010_re\n",
            "Generating record list for: patient001/s0014lre\n",
            "Generating record list for: patient001/s0016lre\n",
            "Generating record list for: patient002/s0015lre\n",
            "Generating record list for: patient003/s0017lre\n",
            "Generating record list for: patient004/s0020are\n",
            "Generating record list for: patient004/s0020bre\n",
            "Generating record list for: patient005/s0021are\n",
            "Generating record list for: patient005/s0021bre\n",
            "Generating record list for: patient005/s0025lre\n",
            "Generating record list for: patient005/s0031lre\n",
            "Generating record list for: patient005/s0101lre\n",
            "Generating record list for: patient006/s0022lre\n",
            "Generating record list for: patient006/s0027lre\n",
            "Generating record list for: patient006/s0064lre\n",
            "Generating record list for: patient007/s0026lre\n",
            "Generating record list for: patient007/s0029lre\n",
            "Generating record list for: patient007/s0038lre\n",
            "Generating record list for: patient007/s0078lre\n",
            "Generating record list for: patient008/s0028lre\n",
            "Generating record list for: patient008/s0037lre\n",
            "Generating record list for: patient008/s0068lre\n",
            "Generating record list for: patient009/s0035_re\n",
            "Generating record list for: patient010/s0036lre\n",
            "Generating record list for: patient010/s0042lre\n",
            "Generating record list for: patient010/s0061lre\n",
            "Generating record list for: patient011/s0039lre\n",
            "Generating record list for: patient011/s0044lre\n",
            "Generating record list for: patient011/s0049lre\n",
            "Generating record list for: patient011/s0067lre\n",
            "Generating record list for: patient012/s0043lre\n",
            "Generating record list for: patient012/s0050lre\n",
            "Generating record list for: patient013/s0045lre\n",
            "Generating record list for: patient013/s0051lre\n",
            "Generating record list for: patient013/s0072lre\n",
            "Generating record list for: patient014/s0046lre\n",
            "Generating record list for: patient014/s0056lre\n",
            "Generating record list for: patient014/s0071lre\n",
            "Generating record list for: patient015/s0047lre\n",
            "Generating record list for: patient015/s0057lre\n",
            "Generating record list for: patient015/s0152lre\n",
            "Generating record list for: patient016/s0052lre\n",
            "Generating record list for: patient016/s0060lre\n",
            "Generating record list for: patient016/s0076lre\n",
            "Generating record list for: patient017/s0053lre\n",
            "Generating record list for: patient017/s0055lre\n",
            "Generating record list for: patient017/s0063lre\n",
            "Generating record list for: patient017/s0075lre\n",
            "Generating record list for: patient018/s0054lre\n",
            "Generating record list for: patient018/s0059lre\n",
            "Generating record list for: patient018/s0082lre\n",
            "Generating record list for: patient019/s0058lre\n",
            "Generating record list for: patient019/s0070lre\n",
            "Generating record list for: patient019/s0077lre\n",
            "Generating record list for: patient020/s0062lre\n",
            "Generating record list for: patient020/s0069lre\n",
            "Generating record list for: patient020/s0079lre\n",
            "Generating record list for: patient021/s0065lre\n",
            "Generating record list for: patient021/s0073lre\n",
            "Generating record list for: patient021/s0097lre\n",
            "Generating record list for: patient022/s0066lre\n",
            "Generating record list for: patient022/s0074lre\n",
            "Generating record list for: patient022/s0149lre\n",
            "Generating record list for: patient023/s0080lre\n",
            "Generating record list for: patient023/s0081lre\n",
            "Generating record list for: patient023/s0085lre\n",
            "Generating record list for: patient023/s0103lre\n",
            "Generating record list for: patient024/s0083lre\n",
            "Generating record list for: patient024/s0084lre\n",
            "Generating record list for: patient024/s0086lre\n",
            "Generating record list for: patient024/s0094lre\n",
            "Generating record list for: patient025/s0087lre\n",
            "Generating record list for: patient025/s0091lre\n",
            "Generating record list for: patient025/s0150lre\n",
            "Generating record list for: patient026/s0088lre\n",
            "Generating record list for: patient026/s0095lre\n",
            "Generating record list for: patient027/s0089lre\n",
            "Generating record list for: patient027/s0096lre\n",
            "Generating record list for: patient027/s0151lre\n",
            "Generating record list for: patient028/s0090lre\n",
            "Generating record list for: patient028/s0093lre\n",
            "Generating record list for: patient028/s0108lre\n",
            "Generating record list for: patient029/s0092lre\n",
            "Generating record list for: patient029/s0098lre\n",
            "Generating record list for: patient029/s0122lre\n",
            "Generating record list for: patient030/s0099lre\n",
            "Generating record list for: patient030/s0107lre\n",
            "Generating record list for: patient030/s0117lre\n",
            "Generating record list for: patient030/s0153lre\n",
            "Generating record list for: patient031/s0100lre\n",
            "Generating record list for: patient031/s0104lre\n",
            "Generating record list for: patient031/s0114lre\n",
            "Generating record list for: patient031/s0127lre\n",
            "Generating record list for: patient032/s0102lre\n",
            "Generating record list for: patient032/s0106lre\n",
            "Generating record list for: patient032/s0115lre\n",
            "Generating record list for: patient032/s0165lre\n",
            "Generating record list for: patient033/s0105lre\n",
            "Generating record list for: patient033/s0113lre\n",
            "Generating record list for: patient033/s0121lre\n",
            "Generating record list for: patient033/s0157lre\n",
            "Generating record list for: patient034/s0109lre\n",
            "Generating record list for: patient034/s0118lre\n",
            "Generating record list for: patient034/s0123lre\n",
            "Generating record list for: patient034/s0158lre\n",
            "Generating record list for: patient035/s0110lre\n",
            "Generating record list for: patient035/s0119lre\n",
            "Generating record list for: patient035/s0124lre\n",
            "Generating record list for: patient035/s0145lre\n",
            "Generating record list for: patient036/s0111lre\n",
            "Generating record list for: patient036/s0116lre\n",
            "Generating record list for: patient036/s0126lre\n",
            "Generating record list for: patient037/s0112lre\n",
            "Generating record list for: patient037/s0120lre\n",
            "Generating record list for: patient038/s0125lre\n",
            "Generating record list for: patient038/s0128lre\n",
            "Generating record list for: patient038/s0162lre\n",
            "Generating record list for: patient039/s0129lre\n",
            "Generating record list for: patient039/s0134lre\n",
            "Generating record list for: patient039/s0164lre\n",
            "Generating record list for: patient040/s0130lre\n",
            "Generating record list for: patient040/s0131lre\n",
            "Generating record list for: patient040/s0133lre\n",
            "Generating record list for: patient040/s0219lre\n",
            "Generating record list for: patient041/s0132lre\n",
            "Generating record list for: patient041/s0136lre\n",
            "Generating record list for: patient041/s0138lre\n",
            "Generating record list for: patient041/s0276lre\n",
            "Generating record list for: patient042/s0135lre\n",
            "Generating record list for: patient042/s0137lre\n",
            "Generating record list for: patient042/s0140lre\n",
            "Generating record list for: patient042/s0347lre\n",
            "Generating record list for: patient043/s0141lre\n",
            "Generating record list for: patient043/s0144lre\n",
            "Generating record list for: patient043/s0278lre\n",
            "Generating record list for: patient044/s0142lre\n",
            "Generating record list for: patient044/s0143lre\n",
            "Generating record list for: patient044/s0146lre\n",
            "Generating record list for: patient044/s0159lre\n",
            "Generating record list for: patient045/s0147lre\n",
            "Generating record list for: patient045/s0148lre\n",
            "Generating record list for: patient045/s0155lre\n",
            "Generating record list for: patient045/s0217lre\n",
            "Generating record list for: patient046/s0156lre\n",
            "Generating record list for: patient046/s0161lre\n",
            "Generating record list for: patient046/s0168lre\n",
            "Generating record list for: patient046/s0184lre\n",
            "Generating record list for: patient047/s0160lre\n",
            "Generating record list for: patient047/s0163lre\n",
            "Generating record list for: patient047/s0167lre\n",
            "Generating record list for: patient048/s0171lre\n",
            "Generating record list for: patient048/s0172lre\n",
            "Generating record list for: patient048/s0180lre\n",
            "Generating record list for: patient048/s0277lre\n",
            "Generating record list for: patient049/s0173lre\n",
            "Generating record list for: patient049/s0178lre\n",
            "Generating record list for: patient049/s0186lre\n",
            "Generating record list for: patient049/s0314lre\n",
            "Generating record list for: patient050/s0174lre\n",
            "Generating record list for: patient050/s0177lre\n",
            "Generating record list for: patient050/s0185lre\n",
            "Generating record list for: patient050/s0215lre\n",
            "Generating record list for: patient051/s0179lre\n",
            "Generating record list for: patient051/s0181lre\n",
            "Generating record list for: patient051/s0187lre\n",
            "Generating record list for: patient051/s0213lre\n",
            "Generating record list for: patient052/s0190lre\n",
            "Generating record list for: patient053/s0191lre\n",
            "Generating record list for: patient054/s0192lre\n",
            "Generating record list for: patient054/s0195lre\n",
            "Generating record list for: patient054/s0197lre\n",
            "Generating record list for: patient054/s0218lre\n",
            "Generating record list for: patient055/s0194lre\n",
            "Generating record list for: patient056/s0196lre\n",
            "Generating record list for: patient057/s0198lre\n",
            "Generating record list for: patient058/s0216lre\n",
            "Generating record list for: patient059/s0208lre\n",
            "Generating record list for: patient060/s0209lre\n",
            "Generating record list for: patient061/s0210lre\n",
            "Generating record list for: patient062/s0212lre\n",
            "Generating record list for: patient063/s0214lre\n",
            "Generating record list for: patient064/s0220lre\n",
            "Generating record list for: patient065/s0221lre\n",
            "Generating record list for: patient065/s0226lre\n",
            "Generating record list for: patient065/s0229lre\n",
            "Generating record list for: patient065/s0282lre\n",
            "Generating record list for: patient066/s0225lre\n",
            "Generating record list for: patient066/s0231lre\n",
            "Generating record list for: patient066/s0280lre\n",
            "Generating record list for: patient067/s0227lre\n",
            "Generating record list for: patient067/s0230lre\n",
            "Generating record list for: patient067/s0283lre\n",
            "Generating record list for: patient068/s0228lre\n",
            "Generating record list for: patient069/s0232lre\n",
            "Generating record list for: patient069/s0233lre\n",
            "Generating record list for: patient069/s0234lre\n",
            "Generating record list for: patient069/s0284lre\n",
            "Generating record list for: patient070/s0235lre\n",
            "Generating record list for: patient071/s0236lre\n",
            "Generating record list for: patient072/s0237lre\n",
            "Generating record list for: patient072/s0240lre\n",
            "Generating record list for: patient072/s0244lre\n",
            "Generating record list for: patient072/s0318lre\n",
            "Generating record list for: patient073/s0238lre\n",
            "Generating record list for: patient073/s0243lre\n",
            "Generating record list for: patient073/s0249lre\n",
            "Generating record list for: patient073/s0252lre\n",
            "Generating record list for: patient074/s0239lre\n",
            "Generating record list for: patient074/s0241lre\n",
            "Generating record list for: patient074/s0245lre\n",
            "Generating record list for: patient074/s0406lre\n",
            "Generating record list for: patient075/s0242lre\n",
            "Generating record list for: patient075/s0246lre\n",
            "Generating record list for: patient075/s0248lre\n",
            "Generating record list for: patient075/s0327lre\n",
            "Generating record list for: patient076/s0247lre\n",
            "Generating record list for: patient076/s0250lre\n",
            "Generating record list for: patient076/s0253lre\n",
            "Generating record list for: patient076/s0319lre\n",
            "Generating record list for: patient077/s0251lre\n",
            "Generating record list for: patient077/s0254lre\n",
            "Generating record list for: patient077/s0258lre\n",
            "Generating record list for: patient077/s0285lre\n",
            "Generating record list for: patient078/s0255lre\n",
            "Generating record list for: patient078/s0259lre\n",
            "Generating record list for: patient078/s0262lre\n",
            "Generating record list for: patient078/s0317lre\n",
            "Generating record list for: patient079/s0256lre\n",
            "Generating record list for: patient079/s0257lre\n",
            "Generating record list for: patient079/s0263lre\n",
            "Generating record list for: patient079/s0269lre\n",
            "Generating record list for: patient080/s0260lre\n",
            "Generating record list for: patient080/s0261lre\n",
            "Generating record list for: patient080/s0265lre\n",
            "Generating record list for: patient080/s0315lre\n",
            "Generating record list for: patient081/s0264lre\n",
            "Generating record list for: patient081/s0266lre\n",
            "Generating record list for: patient081/s0270lre\n",
            "Generating record list for: patient081/s0346lre\n",
            "Generating record list for: patient082/s0267lre\n",
            "Generating record list for: patient082/s0271lre\n",
            "Generating record list for: patient082/s0279lre\n",
            "Generating record list for: patient082/s0320lre\n",
            "Generating record list for: patient083/s0268lre\n",
            "Generating record list for: patient083/s0272lre\n",
            "Generating record list for: patient083/s0286lre\n",
            "Generating record list for: patient083/s0290lre\n",
            "Generating record list for: patient084/s0281lre\n",
            "Generating record list for: patient084/s0288lre\n",
            "Generating record list for: patient084/s0289lre\n",
            "Generating record list for: patient084/s0313lre\n",
            "Generating record list for: patient085/s0296lre\n",
            "Generating record list for: patient085/s0297lre\n",
            "Generating record list for: patient085/s0298lre\n",
            "Generating record list for: patient085/s0345lre\n",
            "Generating record list for: patient086/s0316lre\n",
            "Generating record list for: patient087/s0321lre\n",
            "Generating record list for: patient087/s0326lre\n",
            "Generating record list for: patient087/s0330lre\n",
            "Generating record list for: patient088/s0339lre\n",
            "Generating record list for: patient088/s0343lre\n",
            "Generating record list for: patient088/s0352lre\n",
            "Generating record list for: patient088/s0413lre\n",
            "Generating record list for: patient089/s0344lre\n",
            "Generating record list for: patient089/s0355lre\n",
            "Generating record list for: patient089/s0359lre\n",
            "Generating record list for: patient089/s0372lre\n",
            "Generating record list for: patient090/s0348lre\n",
            "Generating record list for: patient090/s0356lre\n",
            "Generating record list for: patient090/s0360lre\n",
            "Generating record list for: patient090/s0418lre\n",
            "Generating record list for: patient091/s0353lre\n",
            "Generating record list for: patient091/s0357lre\n",
            "Generating record list for: patient091/s0361lre\n",
            "Generating record list for: patient091/s0408lre\n",
            "Generating record list for: patient092/s0354lre\n",
            "Generating record list for: patient092/s0358lre\n",
            "Generating record list for: patient092/s0362lre\n",
            "Generating record list for: patient092/s0411lre\n",
            "Generating record list for: patient093/s0367lre\n",
            "Generating record list for: patient093/s0371lre\n",
            "Generating record list for: patient093/s0375lre\n",
            "Generating record list for: patient093/s0378lre\n",
            "Generating record list for: patient093/s0396lre\n",
            "Generating record list for: patient094/s0368lre\n",
            "Generating record list for: patient094/s0370lre\n",
            "Generating record list for: patient094/s0376lre\n",
            "Generating record list for: patient094/s0412lre\n",
            "Generating record list for: patient095/s0369lre\n",
            "Generating record list for: patient095/s0373lre\n",
            "Generating record list for: patient095/s0377lre\n",
            "Generating record list for: patient095/s0417lre\n",
            "Generating record list for: patient096/s0379lre\n",
            "Generating record list for: patient096/s0381lre\n",
            "Generating record list for: patient096/s0385lre\n",
            "Generating record list for: patient096/s0395lre\n",
            "Generating record list for: patient097/s0380lre\n",
            "Generating record list for: patient097/s0382lre\n",
            "Generating record list for: patient097/s0384lre\n",
            "Generating record list for: patient097/s0394lre\n",
            "Generating record list for: patient098/s0386lre\n",
            "Generating record list for: patient098/s0389lre\n",
            "Generating record list for: patient098/s0398lre\n",
            "Generating record list for: patient098/s0409lre\n",
            "Generating record list for: patient099/s0387lre\n",
            "Generating record list for: patient099/s0388lre\n",
            "Generating record list for: patient099/s0397lre\n",
            "Generating record list for: patient099/s0419lre\n",
            "Generating record list for: patient100/s0399lre\n",
            "Generating record list for: patient100/s0401lre\n",
            "Generating record list for: patient100/s0407lre\n",
            "Generating record list for: patient101/s0400lre\n",
            "Generating record list for: patient101/s0410lre\n",
            "Generating record list for: patient101/s0414lre\n",
            "Generating record list for: patient102/s0416lre\n",
            "Generating record list for: patient103/s0332lre\n",
            "Generating record list for: patient104/s0306lre\n",
            "Generating record list for: patient105/s0303lre\n",
            "Generating record list for: patient106/s0030_re\n",
            "Generating record list for: patient107/s0199_re\n",
            "Generating record list for: patient108/s0013_re\n",
            "Generating record list for: patient109/s0349lre\n",
            "Generating record list for: patient110/s0003_re\n",
            "Generating record list for: patient111/s0203_re\n",
            "Generating record list for: patient112/s0169_re\n",
            "Generating record list for: patient113/s0018cre\n",
            "Generating record list for: patient113/s0018lre\n",
            "Generating record list for: patient114/s0012_re\n",
            "Generating record list for: patient115/s0023_re\n",
            "Generating record list for: patient116/s0302lre\n",
            "Generating record list for: patient117/s0291lre\n",
            "Generating record list for: patient117/s0292lre\n",
            "Generating record list for: patient118/s0183_re\n",
            "Generating record list for: patient119/s0001_re\n",
            "Generating record list for: patient120/s0331lre\n",
            "Generating record list for: patient121/s0311lre\n",
            "Generating record list for: patient122/s0312lre\n",
            "Generating record list for: patient123/s0224_re\n",
            "Generating record list for: patient125/s0006_re\n",
            "Generating record list for: patient126/s0154_re\n",
            "Generating record list for: patient127/s0342lre\n",
            "Generating record list for: patient127/s0383lre\n",
            "Generating record list for: patient128/s0182_re\n",
            "Generating record list for: patient129/s0189_re\n",
            "Generating record list for: patient130/s0166_re\n",
            "Generating record list for: patient131/s0273lre\n",
            "Generating record list for: patient133/s0393lre\n",
            "Generating record list for: patient135/s0334lre\n",
            "Generating record list for: patient136/s0205_re\n",
            "Generating record list for: patient137/s0392lre\n",
            "Generating record list for: patient138/s0005_re\n",
            "Generating record list for: patient139/s0223_re\n",
            "Generating record list for: patient140/s0019_re\n",
            "Generating record list for: patient141/s0307lre\n",
            "Generating record list for: patient142/s0351lre\n",
            "Generating record list for: patient143/s0333lre\n",
            "Generating record list for: patient144/s0341lre\n",
            "Generating record list for: patient145/s0201_re\n",
            "Generating record list for: patient146/s0007_re\n",
            "Generating record list for: patient147/s0211_re\n",
            "Generating record list for: patient148/s0335lre\n",
            "Generating record list for: patient149/s0202are\n",
            "Generating record list for: patient149/s0202bre\n",
            "Generating record list for: patient150/s0287lre\n",
            "Generating record list for: patient151/s0206_re\n",
            "Generating record list for: patient152/s0004_re\n",
            "Generating record list for: patient153/s0391lre\n",
            "Generating record list for: patient154/s0170_re\n",
            "Generating record list for: patient155/s0301lre\n",
            "Generating record list for: patient156/s0299lre\n",
            "Generating record list for: patient157/s0338lre\n",
            "Generating record list for: patient158/s0294lre\n",
            "Generating record list for: patient158/s0295lre\n",
            "Generating record list for: patient159/s0390lre\n",
            "Generating record list for: patient160/s0222_re\n",
            "Generating record list for: patient162/s0193_re\n",
            "Generating record list for: patient163/s0034_re\n",
            "Generating record list for: patient164/s0024are\n",
            "Generating record list for: patient164/s0024bre\n",
            "Generating record list for: patient165/s0322lre\n",
            "Generating record list for: patient165/s0323lre\n",
            "Generating record list for: patient166/s0275lre\n",
            "Generating record list for: patient167/s0200_re\n",
            "Generating record list for: patient168/s0032_re\n",
            "Generating record list for: patient168/s0033_re\n",
            "Generating record list for: patient169/s0328lre\n",
            "Generating record list for: patient169/s0329lre\n",
            "Generating record list for: patient170/s0274lre\n",
            "Generating record list for: patient171/s0364lre\n",
            "Generating record list for: patient172/s0304lre\n",
            "Generating record list for: patient173/s0305lre\n",
            "Generating record list for: patient174/s0300lre\n",
            "Generating record list for: patient174/s0324lre\n",
            "Generating record list for: patient174/s0325lre\n",
            "Generating record list for: patient175/s0009_re\n",
            "Generating record list for: patient176/s0188_re\n",
            "Generating record list for: patient177/s0366lre\n",
            "Generating record list for: patient178/s0011_re\n",
            "Generating record list for: patient179/s0176_re\n",
            "Generating record list for: patient180/s0374lre\n",
            "Generating record list for: patient180/s0475_re\n",
            "Generating record list for: patient180/s0476_re\n",
            "Generating record list for: patient180/s0477_re\n",
            "Generating record list for: patient180/s0490_re\n",
            "Generating record list for: patient180/s0545_re\n",
            "Generating record list for: patient180/s0561_re\n",
            "Generating record list for: patient181/s0204are\n",
            "Generating record list for: patient181/s0204bre\n",
            "Generating record list for: patient182/s0308lre\n",
            "Generating record list for: patient183/s0175_re\n",
            "Generating record list for: patient184/s0363lre\n",
            "Generating record list for: patient185/s0336lre\n",
            "Generating record list for: patient186/s0293lre\n",
            "Generating record list for: patient187/s0207_re\n",
            "Generating record list for: patient188/s0365lre\n",
            "Generating record list for: patient189/s0309lre\n",
            "Generating record list for: patient190/s0040_re\n",
            "Generating record list for: patient190/s0041_re\n",
            "Generating record list for: patient191/s0340lre\n",
            "Generating record list for: patient192/s0048_re\n",
            "Generating record list for: patient193/s0008_re\n",
            "Generating record list for: patient194/s0310lre\n",
            "Generating record list for: patient195/s0337lre\n",
            "Generating record list for: patient196/s0002_re\n",
            "Generating record list for: patient197/s0350lre\n",
            "Generating record list for: patient197/s0403lre\n",
            "Generating record list for: patient198/s0402lre\n",
            "Generating record list for: patient198/s0415lre\n",
            "Generating record list for: patient199/s0404lre\n",
            "Generating record list for: patient200/s0405lre\n",
            "Generating record list for: patient201/s0420_re\n",
            "Generating record list for: patient201/s0423_re\n",
            "Generating record list for: patient202/s0421_re\n",
            "Generating record list for: patient202/s0422_re\n",
            "Generating record list for: patient203/s0424_re\n",
            "Generating record list for: patient204/s0425_re\n",
            "Generating record list for: patient205/s0426_re\n",
            "Generating record list for: patient206/s0427_re\n",
            "Generating record list for: patient207/s0428_re\n",
            "Generating record list for: patient208/s0429_re\n",
            "Generating record list for: patient208/s0430_re\n",
            "Generating record list for: patient209/s0431_re\n",
            "Generating record list for: patient210/s0432_re\n",
            "Generating record list for: patient211/s0433_re\n",
            "Generating record list for: patient212/s0434_re\n",
            "Generating record list for: patient213/s0435_re\n",
            "Generating record list for: patient214/s0436_re\n",
            "Generating record list for: patient215/s0437_re\n",
            "Generating record list for: patient216/s0438_re\n",
            "Generating record list for: patient217/s0439_re\n",
            "Generating record list for: patient218/s0440_re\n",
            "Generating record list for: patient219/s0441_re\n",
            "Generating record list for: patient220/s0442_re\n",
            "Generating record list for: patient221/s0443_re\n",
            "Generating record list for: patient222/s0444_re\n",
            "Generating record list for: patient223/s0445_re\n",
            "Generating record list for: patient223/s0446_re\n",
            "Generating record list for: patient224/s0447_re\n",
            "Generating record list for: patient225/s0448_re\n",
            "Generating record list for: patient226/s0449_re\n",
            "Generating record list for: patient227/s0450_re\n",
            "Generating record list for: patient228/s0451_re\n",
            "Generating record list for: patient229/s0452_re\n",
            "Generating record list for: patient229/s0453_re\n",
            "Generating record list for: patient230/s0454_re\n",
            "Generating record list for: patient231/s0455_re\n",
            "Generating record list for: patient232/s0456_re\n",
            "Generating record list for: patient233/s0457_re\n",
            "Generating record list for: patient233/s0458_re\n",
            "Generating record list for: patient233/s0459_re\n",
            "Generating record list for: patient233/s0482_re\n",
            "Generating record list for: patient233/s0483_re\n",
            "Generating record list for: patient234/s0460_re\n",
            "Generating record list for: patient235/s0461_re\n",
            "Generating record list for: patient236/s0462_re\n",
            "Generating record list for: patient236/s0463_re\n",
            "Generating record list for: patient236/s0464_re\n",
            "Generating record list for: patient237/s0465_re\n",
            "Generating record list for: patient238/s0466_re\n",
            "Generating record list for: patient239/s0467_re\n",
            "Generating record list for: patient240/s0468_re\n",
            "Generating record list for: patient241/s0469_re\n",
            "Generating record list for: patient241/s0470_re\n",
            "Generating record list for: patient242/s0471_re\n",
            "Generating record list for: patient243/s0472_re\n",
            "Generating record list for: patient244/s0473_re\n",
            "Generating record list for: patient245/s0474_re\n",
            "Generating record list for: patient245/s0480_re\n",
            "Generating record list for: patient246/s0478_re\n",
            "Generating record list for: patient247/s0479_re\n",
            "Generating record list for: patient248/s0481_re\n",
            "Generating record list for: patient249/s0484_re\n",
            "Generating record list for: patient250/s0485_re\n",
            "Generating record list for: patient251/s0486_re\n",
            "Generating record list for: patient251/s0503_re\n",
            "Generating record list for: patient251/s0506_re\n",
            "Generating record list for: patient252/s0487_re\n",
            "Generating record list for: patient253/s0488_re\n",
            "Generating record list for: patient254/s0489_re\n",
            "Generating record list for: patient255/s0491_re\n",
            "Generating record list for: patient256/s0492_re\n",
            "Generating record list for: patient257/s0493_re\n",
            "Generating record list for: patient258/s0494_re\n",
            "Generating record list for: patient259/s0495_re\n",
            "Generating record list for: patient260/s0496_re\n",
            "Generating record list for: patient261/s0497_re\n",
            "Generating record list for: patient262/s0498_re\n",
            "Generating record list for: patient263/s0499_re\n",
            "Generating record list for: patient264/s0500_re\n",
            "Generating record list for: patient265/s0501_re\n",
            "Generating record list for: patient266/s0502_re\n",
            "Generating record list for: patient267/s0504_re\n",
            "Generating record list for: patient268/s0505_re\n",
            "Generating record list for: patient269/s0508_re\n",
            "Generating record list for: patient270/s0507_re\n",
            "Generating record list for: patient271/s0509_re\n",
            "Generating record list for: patient272/s0510_re\n",
            "Generating record list for: patient273/s0511_re\n",
            "Generating record list for: patient274/s0512_re\n",
            "Generating record list for: patient275/s0513_re\n",
            "Generating record list for: patient276/s0526_re\n",
            "Generating record list for: patient277/s0527_re\n",
            "Generating record list for: patient278/s0528_re\n",
            "Generating record list for: patient278/s0529_re\n",
            "Generating record list for: patient278/s0530_re\n",
            "Generating record list for: patient279/s0531_re\n",
            "Generating record list for: patient279/s0532_re\n",
            "Generating record list for: patient279/s0533_re\n",
            "Generating record list for: patient279/s0534_re\n",
            "Generating record list for: patient280/s0535_re\n",
            "Generating record list for: patient281/s0537_re\n",
            "Generating record list for: patient282/s0539_re\n",
            "Generating record list for: patient283/s0542_re\n",
            "Generating record list for: patient284/s0543_re\n",
            "Generating record list for: patient284/s0551_re\n",
            "Generating record list for: patient284/s0552_re\n",
            "Generating record list for: patient285/s0544_re\n",
            "Generating record list for: patient286/s0546_re\n",
            "Generating record list for: patient287/s0547_re\n",
            "Generating record list for: patient287/s0548_re\n",
            "Generating record list for: patient288/s0549_re\n",
            "Generating record list for: patient289/s0550_re\n",
            "Generating record list for: patient290/s0553_re\n",
            "Generating record list for: patient291/s0554_re\n",
            "Generating record list for: patient292/s0555_re\n",
            "Generating record list for: patient292/s0556_re\n",
            "Generating record list for: patient293/s0557_re\n",
            "Generating record list for: patient293/s0558_re\n",
            "Generating record list for: patient294/s0559_re\n",
            "Generating list of all files for: patient001/s0010_re\n",
            "Generating list of all files for: patient001/s0014lre\n",
            "Generating list of all files for: patient001/s0016lre\n",
            "Generating list of all files for: patient002/s0015lre\n",
            "Generating list of all files for: patient003/s0017lre\n",
            "Generating list of all files for: patient004/s0020are\n",
            "Generating list of all files for: patient004/s0020bre\n",
            "Generating list of all files for: patient005/s0021are\n",
            "Generating list of all files for: patient005/s0021bre\n",
            "Generating list of all files for: patient005/s0025lre\n",
            "Generating list of all files for: patient005/s0031lre\n",
            "Generating list of all files for: patient005/s0101lre\n",
            "Generating list of all files for: patient006/s0022lre\n",
            "Generating list of all files for: patient006/s0027lre\n",
            "Generating list of all files for: patient006/s0064lre\n",
            "Generating list of all files for: patient007/s0026lre\n",
            "Generating list of all files for: patient007/s0029lre\n",
            "Generating list of all files for: patient007/s0038lre\n",
            "Generating list of all files for: patient007/s0078lre\n",
            "Generating list of all files for: patient008/s0028lre\n",
            "Generating list of all files for: patient008/s0037lre\n",
            "Generating list of all files for: patient008/s0068lre\n",
            "Generating list of all files for: patient009/s0035_re\n",
            "Generating list of all files for: patient010/s0036lre\n",
            "Generating list of all files for: patient010/s0042lre\n",
            "Generating list of all files for: patient010/s0061lre\n",
            "Generating list of all files for: patient011/s0039lre\n",
            "Generating list of all files for: patient011/s0044lre\n",
            "Generating list of all files for: patient011/s0049lre\n",
            "Generating list of all files for: patient011/s0067lre\n",
            "Generating list of all files for: patient012/s0043lre\n",
            "Generating list of all files for: patient012/s0050lre\n",
            "Generating list of all files for: patient013/s0045lre\n",
            "Generating list of all files for: patient013/s0051lre\n",
            "Generating list of all files for: patient013/s0072lre\n",
            "Generating list of all files for: patient014/s0046lre\n",
            "Generating list of all files for: patient014/s0056lre\n",
            "Generating list of all files for: patient014/s0071lre\n",
            "Generating list of all files for: patient015/s0047lre\n",
            "Generating list of all files for: patient015/s0057lre\n",
            "Generating list of all files for: patient015/s0152lre\n",
            "Generating list of all files for: patient016/s0052lre\n",
            "Generating list of all files for: patient016/s0060lre\n",
            "Generating list of all files for: patient016/s0076lre\n",
            "Generating list of all files for: patient017/s0053lre\n",
            "Generating list of all files for: patient017/s0055lre\n",
            "Generating list of all files for: patient017/s0063lre\n",
            "Generating list of all files for: patient017/s0075lre\n",
            "Generating list of all files for: patient018/s0054lre\n",
            "Generating list of all files for: patient018/s0059lre\n",
            "Generating list of all files for: patient018/s0082lre\n",
            "Generating list of all files for: patient019/s0058lre\n",
            "Generating list of all files for: patient019/s0070lre\n",
            "Generating list of all files for: patient019/s0077lre\n",
            "Generating list of all files for: patient020/s0062lre\n",
            "Generating list of all files for: patient020/s0069lre\n",
            "Generating list of all files for: patient020/s0079lre\n",
            "Generating list of all files for: patient021/s0065lre\n",
            "Generating list of all files for: patient021/s0073lre\n",
            "Generating list of all files for: patient021/s0097lre\n",
            "Generating list of all files for: patient022/s0066lre\n",
            "Generating list of all files for: patient022/s0074lre\n",
            "Generating list of all files for: patient022/s0149lre\n",
            "Generating list of all files for: patient023/s0080lre\n",
            "Generating list of all files for: patient023/s0081lre\n",
            "Generating list of all files for: patient023/s0085lre\n",
            "Generating list of all files for: patient023/s0103lre\n",
            "Generating list of all files for: patient024/s0083lre\n",
            "Generating list of all files for: patient024/s0084lre\n",
            "Generating list of all files for: patient024/s0086lre\n",
            "Generating list of all files for: patient024/s0094lre\n",
            "Generating list of all files for: patient025/s0087lre\n",
            "Generating list of all files for: patient025/s0091lre\n",
            "Generating list of all files for: patient025/s0150lre\n",
            "Generating list of all files for: patient026/s0088lre\n",
            "Generating list of all files for: patient026/s0095lre\n",
            "Generating list of all files for: patient027/s0089lre\n",
            "Generating list of all files for: patient027/s0096lre\n",
            "Generating list of all files for: patient027/s0151lre\n",
            "Generating list of all files for: patient028/s0090lre\n",
            "Generating list of all files for: patient028/s0093lre\n",
            "Generating list of all files for: patient028/s0108lre\n",
            "Generating list of all files for: patient029/s0092lre\n",
            "Generating list of all files for: patient029/s0098lre\n",
            "Generating list of all files for: patient029/s0122lre\n",
            "Generating list of all files for: patient030/s0099lre\n",
            "Generating list of all files for: patient030/s0107lre\n",
            "Generating list of all files for: patient030/s0117lre\n",
            "Generating list of all files for: patient030/s0153lre\n",
            "Generating list of all files for: patient031/s0100lre\n",
            "Generating list of all files for: patient031/s0104lre\n",
            "Generating list of all files for: patient031/s0114lre\n",
            "Generating list of all files for: patient031/s0127lre\n",
            "Generating list of all files for: patient032/s0102lre\n",
            "Generating list of all files for: patient032/s0106lre\n",
            "Generating list of all files for: patient032/s0115lre\n",
            "Generating list of all files for: patient032/s0165lre\n",
            "Generating list of all files for: patient033/s0105lre\n",
            "Generating list of all files for: patient033/s0113lre\n",
            "Generating list of all files for: patient033/s0121lre\n",
            "Generating list of all files for: patient033/s0157lre\n",
            "Generating list of all files for: patient034/s0109lre\n",
            "Generating list of all files for: patient034/s0118lre\n",
            "Generating list of all files for: patient034/s0123lre\n",
            "Generating list of all files for: patient034/s0158lre\n",
            "Generating list of all files for: patient035/s0110lre\n",
            "Generating list of all files for: patient035/s0119lre\n",
            "Generating list of all files for: patient035/s0124lre\n",
            "Generating list of all files for: patient035/s0145lre\n",
            "Generating list of all files for: patient036/s0111lre\n",
            "Generating list of all files for: patient036/s0116lre\n",
            "Generating list of all files for: patient036/s0126lre\n",
            "Generating list of all files for: patient037/s0112lre\n",
            "Generating list of all files for: patient037/s0120lre\n",
            "Generating list of all files for: patient038/s0125lre\n",
            "Generating list of all files for: patient038/s0128lre\n",
            "Generating list of all files for: patient038/s0162lre\n",
            "Generating list of all files for: patient039/s0129lre\n",
            "Generating list of all files for: patient039/s0134lre\n",
            "Generating list of all files for: patient039/s0164lre\n",
            "Generating list of all files for: patient040/s0130lre\n",
            "Generating list of all files for: patient040/s0131lre\n",
            "Generating list of all files for: patient040/s0133lre\n",
            "Generating list of all files for: patient040/s0219lre\n",
            "Generating list of all files for: patient041/s0132lre\n",
            "Generating list of all files for: patient041/s0136lre\n",
            "Generating list of all files for: patient041/s0138lre\n",
            "Generating list of all files for: patient041/s0276lre\n",
            "Generating list of all files for: patient042/s0135lre\n",
            "Generating list of all files for: patient042/s0137lre\n",
            "Generating list of all files for: patient042/s0140lre\n",
            "Generating list of all files for: patient042/s0347lre\n",
            "Generating list of all files for: patient043/s0141lre\n",
            "Generating list of all files for: patient043/s0144lre\n",
            "Generating list of all files for: patient043/s0278lre\n",
            "Generating list of all files for: patient044/s0142lre\n",
            "Generating list of all files for: patient044/s0143lre\n",
            "Generating list of all files for: patient044/s0146lre\n",
            "Generating list of all files for: patient044/s0159lre\n",
            "Generating list of all files for: patient045/s0147lre\n",
            "Generating list of all files for: patient045/s0148lre\n",
            "Generating list of all files for: patient045/s0155lre\n",
            "Generating list of all files for: patient045/s0217lre\n",
            "Generating list of all files for: patient046/s0156lre\n",
            "Generating list of all files for: patient046/s0161lre\n",
            "Generating list of all files for: patient046/s0168lre\n",
            "Generating list of all files for: patient046/s0184lre\n",
            "Generating list of all files for: patient047/s0160lre\n",
            "Generating list of all files for: patient047/s0163lre\n",
            "Generating list of all files for: patient047/s0167lre\n",
            "Generating list of all files for: patient048/s0171lre\n",
            "Generating list of all files for: patient048/s0172lre\n",
            "Generating list of all files for: patient048/s0180lre\n",
            "Generating list of all files for: patient048/s0277lre\n",
            "Generating list of all files for: patient049/s0173lre\n",
            "Generating list of all files for: patient049/s0178lre\n",
            "Generating list of all files for: patient049/s0186lre\n",
            "Generating list of all files for: patient049/s0314lre\n",
            "Generating list of all files for: patient050/s0174lre\n",
            "Generating list of all files for: patient050/s0177lre\n",
            "Generating list of all files for: patient050/s0185lre\n",
            "Generating list of all files for: patient050/s0215lre\n",
            "Generating list of all files for: patient051/s0179lre\n",
            "Generating list of all files for: patient051/s0181lre\n",
            "Generating list of all files for: patient051/s0187lre\n",
            "Generating list of all files for: patient051/s0213lre\n",
            "Generating list of all files for: patient052/s0190lre\n",
            "Generating list of all files for: patient053/s0191lre\n",
            "Generating list of all files for: patient054/s0192lre\n",
            "Generating list of all files for: patient054/s0195lre\n",
            "Generating list of all files for: patient054/s0197lre\n",
            "Generating list of all files for: patient054/s0218lre\n",
            "Generating list of all files for: patient055/s0194lre\n",
            "Generating list of all files for: patient056/s0196lre\n",
            "Generating list of all files for: patient057/s0198lre\n",
            "Generating list of all files for: patient058/s0216lre\n",
            "Generating list of all files for: patient059/s0208lre\n",
            "Generating list of all files for: patient060/s0209lre\n",
            "Generating list of all files for: patient061/s0210lre\n",
            "Generating list of all files for: patient062/s0212lre\n",
            "Generating list of all files for: patient063/s0214lre\n",
            "Generating list of all files for: patient064/s0220lre\n",
            "Generating list of all files for: patient065/s0221lre\n",
            "Generating list of all files for: patient065/s0226lre\n",
            "Generating list of all files for: patient065/s0229lre\n",
            "Generating list of all files for: patient065/s0282lre\n",
            "Generating list of all files for: patient066/s0225lre\n",
            "Generating list of all files for: patient066/s0231lre\n",
            "Generating list of all files for: patient066/s0280lre\n",
            "Generating list of all files for: patient067/s0227lre\n",
            "Generating list of all files for: patient067/s0230lre\n",
            "Generating list of all files for: patient067/s0283lre\n",
            "Generating list of all files for: patient068/s0228lre\n",
            "Generating list of all files for: patient069/s0232lre\n",
            "Generating list of all files for: patient069/s0233lre\n",
            "Generating list of all files for: patient069/s0234lre\n",
            "Generating list of all files for: patient069/s0284lre\n",
            "Generating list of all files for: patient070/s0235lre\n",
            "Generating list of all files for: patient071/s0236lre\n",
            "Generating list of all files for: patient072/s0237lre\n",
            "Generating list of all files for: patient072/s0240lre\n",
            "Generating list of all files for: patient072/s0244lre\n",
            "Generating list of all files for: patient072/s0318lre\n",
            "Generating list of all files for: patient073/s0238lre\n",
            "Generating list of all files for: patient073/s0243lre\n",
            "Generating list of all files for: patient073/s0249lre\n",
            "Generating list of all files for: patient073/s0252lre\n",
            "Generating list of all files for: patient074/s0239lre\n",
            "Generating list of all files for: patient074/s0241lre\n",
            "Generating list of all files for: patient074/s0245lre\n",
            "Generating list of all files for: patient074/s0406lre\n",
            "Generating list of all files for: patient075/s0242lre\n",
            "Generating list of all files for: patient075/s0246lre\n",
            "Generating list of all files for: patient075/s0248lre\n",
            "Generating list of all files for: patient075/s0327lre\n",
            "Generating list of all files for: patient076/s0247lre\n",
            "Generating list of all files for: patient076/s0250lre\n",
            "Generating list of all files for: patient076/s0253lre\n",
            "Generating list of all files for: patient076/s0319lre\n",
            "Generating list of all files for: patient077/s0251lre\n",
            "Generating list of all files for: patient077/s0254lre\n",
            "Generating list of all files for: patient077/s0258lre\n",
            "Generating list of all files for: patient077/s0285lre\n",
            "Generating list of all files for: patient078/s0255lre\n",
            "Generating list of all files for: patient078/s0259lre\n",
            "Generating list of all files for: patient078/s0262lre\n",
            "Generating list of all files for: patient078/s0317lre\n",
            "Generating list of all files for: patient079/s0256lre\n",
            "Generating list of all files for: patient079/s0257lre\n",
            "Generating list of all files for: patient079/s0263lre\n",
            "Generating list of all files for: patient079/s0269lre\n",
            "Generating list of all files for: patient080/s0260lre\n",
            "Generating list of all files for: patient080/s0261lre\n",
            "Generating list of all files for: patient080/s0265lre\n",
            "Generating list of all files for: patient080/s0315lre\n",
            "Generating list of all files for: patient081/s0264lre\n",
            "Generating list of all files for: patient081/s0266lre\n",
            "Generating list of all files for: patient081/s0270lre\n",
            "Generating list of all files for: patient081/s0346lre\n",
            "Generating list of all files for: patient082/s0267lre\n",
            "Generating list of all files for: patient082/s0271lre\n",
            "Generating list of all files for: patient082/s0279lre\n",
            "Generating list of all files for: patient082/s0320lre\n",
            "Generating list of all files for: patient083/s0268lre\n",
            "Generating list of all files for: patient083/s0272lre\n",
            "Generating list of all files for: patient083/s0286lre\n",
            "Generating list of all files for: patient083/s0290lre\n",
            "Generating list of all files for: patient084/s0281lre\n",
            "Generating list of all files for: patient084/s0288lre\n",
            "Generating list of all files for: patient084/s0289lre\n",
            "Generating list of all files for: patient084/s0313lre\n",
            "Generating list of all files for: patient085/s0296lre\n",
            "Generating list of all files for: patient085/s0297lre\n",
            "Generating list of all files for: patient085/s0298lre\n",
            "Generating list of all files for: patient085/s0345lre\n",
            "Generating list of all files for: patient086/s0316lre\n",
            "Generating list of all files for: patient087/s0321lre\n",
            "Generating list of all files for: patient087/s0326lre\n",
            "Generating list of all files for: patient087/s0330lre\n",
            "Generating list of all files for: patient088/s0339lre\n",
            "Generating list of all files for: patient088/s0343lre\n",
            "Generating list of all files for: patient088/s0352lre\n",
            "Generating list of all files for: patient088/s0413lre\n",
            "Generating list of all files for: patient089/s0344lre\n",
            "Generating list of all files for: patient089/s0355lre\n",
            "Generating list of all files for: patient089/s0359lre\n",
            "Generating list of all files for: patient089/s0372lre\n",
            "Generating list of all files for: patient090/s0348lre\n",
            "Generating list of all files for: patient090/s0356lre\n",
            "Generating list of all files for: patient090/s0360lre\n",
            "Generating list of all files for: patient090/s0418lre\n",
            "Generating list of all files for: patient091/s0353lre\n",
            "Generating list of all files for: patient091/s0357lre\n",
            "Generating list of all files for: patient091/s0361lre\n",
            "Generating list of all files for: patient091/s0408lre\n",
            "Generating list of all files for: patient092/s0354lre\n",
            "Generating list of all files for: patient092/s0358lre\n",
            "Generating list of all files for: patient092/s0362lre\n",
            "Generating list of all files for: patient092/s0411lre\n",
            "Generating list of all files for: patient093/s0367lre\n",
            "Generating list of all files for: patient093/s0371lre\n",
            "Generating list of all files for: patient093/s0375lre\n",
            "Generating list of all files for: patient093/s0378lre\n",
            "Generating list of all files for: patient093/s0396lre\n",
            "Generating list of all files for: patient094/s0368lre\n",
            "Generating list of all files for: patient094/s0370lre\n",
            "Generating list of all files for: patient094/s0376lre\n",
            "Generating list of all files for: patient094/s0412lre\n",
            "Generating list of all files for: patient095/s0369lre\n",
            "Generating list of all files for: patient095/s0373lre\n",
            "Generating list of all files for: patient095/s0377lre\n",
            "Generating list of all files for: patient095/s0417lre\n",
            "Generating list of all files for: patient096/s0379lre\n",
            "Generating list of all files for: patient096/s0381lre\n",
            "Generating list of all files for: patient096/s0385lre\n",
            "Generating list of all files for: patient096/s0395lre\n",
            "Generating list of all files for: patient097/s0380lre\n",
            "Generating list of all files for: patient097/s0382lre\n",
            "Generating list of all files for: patient097/s0384lre\n",
            "Generating list of all files for: patient097/s0394lre\n",
            "Generating list of all files for: patient098/s0386lre\n",
            "Generating list of all files for: patient098/s0389lre\n",
            "Generating list of all files for: patient098/s0398lre\n",
            "Generating list of all files for: patient098/s0409lre\n",
            "Generating list of all files for: patient099/s0387lre\n",
            "Generating list of all files for: patient099/s0388lre\n",
            "Generating list of all files for: patient099/s0397lre\n",
            "Generating list of all files for: patient099/s0419lre\n",
            "Generating list of all files for: patient100/s0399lre\n",
            "Generating list of all files for: patient100/s0401lre\n",
            "Generating list of all files for: patient100/s0407lre\n",
            "Generating list of all files for: patient101/s0400lre\n",
            "Generating list of all files for: patient101/s0410lre\n",
            "Generating list of all files for: patient101/s0414lre\n",
            "Generating list of all files for: patient102/s0416lre\n",
            "Generating list of all files for: patient103/s0332lre\n",
            "Generating list of all files for: patient104/s0306lre\n",
            "Generating list of all files for: patient105/s0303lre\n",
            "Generating list of all files for: patient106/s0030_re\n",
            "Generating list of all files for: patient107/s0199_re\n",
            "Generating list of all files for: patient108/s0013_re\n",
            "Generating list of all files for: patient109/s0349lre\n",
            "Generating list of all files for: patient110/s0003_re\n",
            "Generating list of all files for: patient111/s0203_re\n",
            "Generating list of all files for: patient112/s0169_re\n",
            "Generating list of all files for: patient113/s0018cre\n",
            "Generating list of all files for: patient113/s0018lre\n",
            "Generating list of all files for: patient114/s0012_re\n",
            "Generating list of all files for: patient115/s0023_re\n",
            "Generating list of all files for: patient116/s0302lre\n",
            "Generating list of all files for: patient117/s0291lre\n",
            "Generating list of all files for: patient117/s0292lre\n",
            "Generating list of all files for: patient118/s0183_re\n",
            "Generating list of all files for: patient119/s0001_re\n",
            "Generating list of all files for: patient120/s0331lre\n",
            "Generating list of all files for: patient121/s0311lre\n",
            "Generating list of all files for: patient122/s0312lre\n",
            "Generating list of all files for: patient123/s0224_re\n",
            "Generating list of all files for: patient125/s0006_re\n",
            "Generating list of all files for: patient126/s0154_re\n",
            "Generating list of all files for: patient127/s0342lre\n",
            "Generating list of all files for: patient127/s0383lre\n",
            "Generating list of all files for: patient128/s0182_re\n",
            "Generating list of all files for: patient129/s0189_re\n",
            "Generating list of all files for: patient130/s0166_re\n",
            "Generating list of all files for: patient131/s0273lre\n",
            "Generating list of all files for: patient133/s0393lre\n",
            "Generating list of all files for: patient135/s0334lre\n",
            "Generating list of all files for: patient136/s0205_re\n",
            "Generating list of all files for: patient137/s0392lre\n",
            "Generating list of all files for: patient138/s0005_re\n",
            "Generating list of all files for: patient139/s0223_re\n",
            "Generating list of all files for: patient140/s0019_re\n",
            "Generating list of all files for: patient141/s0307lre\n",
            "Generating list of all files for: patient142/s0351lre\n",
            "Generating list of all files for: patient143/s0333lre\n",
            "Generating list of all files for: patient144/s0341lre\n",
            "Generating list of all files for: patient145/s0201_re\n",
            "Generating list of all files for: patient146/s0007_re\n",
            "Generating list of all files for: patient147/s0211_re\n",
            "Generating list of all files for: patient148/s0335lre\n",
            "Generating list of all files for: patient149/s0202are\n",
            "Generating list of all files for: patient149/s0202bre\n",
            "Generating list of all files for: patient150/s0287lre\n",
            "Generating list of all files for: patient151/s0206_re\n",
            "Generating list of all files for: patient152/s0004_re\n",
            "Generating list of all files for: patient153/s0391lre\n",
            "Generating list of all files for: patient154/s0170_re\n",
            "Generating list of all files for: patient155/s0301lre\n",
            "Generating list of all files for: patient156/s0299lre\n",
            "Generating list of all files for: patient157/s0338lre\n",
            "Generating list of all files for: patient158/s0294lre\n",
            "Generating list of all files for: patient158/s0295lre\n",
            "Generating list of all files for: patient159/s0390lre\n",
            "Generating list of all files for: patient160/s0222_re\n",
            "Generating list of all files for: patient162/s0193_re\n",
            "Generating list of all files for: patient163/s0034_re\n",
            "Generating list of all files for: patient164/s0024are\n",
            "Generating list of all files for: patient164/s0024bre\n",
            "Generating list of all files for: patient165/s0322lre\n",
            "Generating list of all files for: patient165/s0323lre\n",
            "Generating list of all files for: patient166/s0275lre\n",
            "Generating list of all files for: patient167/s0200_re\n",
            "Generating list of all files for: patient168/s0032_re\n",
            "Generating list of all files for: patient168/s0033_re\n",
            "Generating list of all files for: patient169/s0328lre\n",
            "Generating list of all files for: patient169/s0329lre\n",
            "Generating list of all files for: patient170/s0274lre\n",
            "Generating list of all files for: patient171/s0364lre\n",
            "Generating list of all files for: patient172/s0304lre\n",
            "Generating list of all files for: patient173/s0305lre\n",
            "Generating list of all files for: patient174/s0300lre\n",
            "Generating list of all files for: patient174/s0324lre\n",
            "Generating list of all files for: patient174/s0325lre\n",
            "Generating list of all files for: patient175/s0009_re\n",
            "Generating list of all files for: patient176/s0188_re\n",
            "Generating list of all files for: patient177/s0366lre\n",
            "Generating list of all files for: patient178/s0011_re\n",
            "Generating list of all files for: patient179/s0176_re\n",
            "Generating list of all files for: patient180/s0374lre\n",
            "Generating list of all files for: patient180/s0475_re\n",
            "Generating list of all files for: patient180/s0476_re\n",
            "Generating list of all files for: patient180/s0477_re\n",
            "Generating list of all files for: patient180/s0490_re\n",
            "Generating list of all files for: patient180/s0545_re\n",
            "Generating list of all files for: patient180/s0561_re\n",
            "Generating list of all files for: patient181/s0204are\n",
            "Generating list of all files for: patient181/s0204bre\n",
            "Generating list of all files for: patient182/s0308lre\n",
            "Generating list of all files for: patient183/s0175_re\n",
            "Generating list of all files for: patient184/s0363lre\n",
            "Generating list of all files for: patient185/s0336lre\n",
            "Generating list of all files for: patient186/s0293lre\n",
            "Generating list of all files for: patient187/s0207_re\n",
            "Generating list of all files for: patient188/s0365lre\n",
            "Generating list of all files for: patient189/s0309lre\n",
            "Generating list of all files for: patient190/s0040_re\n",
            "Generating list of all files for: patient190/s0041_re\n",
            "Generating list of all files for: patient191/s0340lre\n",
            "Generating list of all files for: patient192/s0048_re\n",
            "Generating list of all files for: patient193/s0008_re\n",
            "Generating list of all files for: patient194/s0310lre\n",
            "Generating list of all files for: patient195/s0337lre\n",
            "Generating list of all files for: patient196/s0002_re\n",
            "Generating list of all files for: patient197/s0350lre\n",
            "Generating list of all files for: patient197/s0403lre\n",
            "Generating list of all files for: patient198/s0402lre\n",
            "Generating list of all files for: patient198/s0415lre\n",
            "Generating list of all files for: patient199/s0404lre\n",
            "Generating list of all files for: patient200/s0405lre\n",
            "Generating list of all files for: patient201/s0420_re\n",
            "Generating list of all files for: patient201/s0423_re\n",
            "Generating list of all files for: patient202/s0421_re\n",
            "Generating list of all files for: patient202/s0422_re\n",
            "Generating list of all files for: patient203/s0424_re\n",
            "Generating list of all files for: patient204/s0425_re\n",
            "Generating list of all files for: patient205/s0426_re\n",
            "Generating list of all files for: patient206/s0427_re\n",
            "Generating list of all files for: patient207/s0428_re\n",
            "Generating list of all files for: patient208/s0429_re\n",
            "Generating list of all files for: patient208/s0430_re\n",
            "Generating list of all files for: patient209/s0431_re\n",
            "Generating list of all files for: patient210/s0432_re\n",
            "Generating list of all files for: patient211/s0433_re\n",
            "Generating list of all files for: patient212/s0434_re\n",
            "Generating list of all files for: patient213/s0435_re\n",
            "Generating list of all files for: patient214/s0436_re\n",
            "Generating list of all files for: patient215/s0437_re\n",
            "Generating list of all files for: patient216/s0438_re\n",
            "Generating list of all files for: patient217/s0439_re\n",
            "Generating list of all files for: patient218/s0440_re\n",
            "Generating list of all files for: patient219/s0441_re\n",
            "Generating list of all files for: patient220/s0442_re\n",
            "Generating list of all files for: patient221/s0443_re\n",
            "Generating list of all files for: patient222/s0444_re\n",
            "Generating list of all files for: patient223/s0445_re\n",
            "Generating list of all files for: patient223/s0446_re\n",
            "Generating list of all files for: patient224/s0447_re\n",
            "Generating list of all files for: patient225/s0448_re\n",
            "Generating list of all files for: patient226/s0449_re\n",
            "Generating list of all files for: patient227/s0450_re\n",
            "Generating list of all files for: patient228/s0451_re\n",
            "Generating list of all files for: patient229/s0452_re\n",
            "Generating list of all files for: patient229/s0453_re\n",
            "Generating list of all files for: patient230/s0454_re\n",
            "Generating list of all files for: patient231/s0455_re\n",
            "Generating list of all files for: patient232/s0456_re\n",
            "Generating list of all files for: patient233/s0457_re\n",
            "Generating list of all files for: patient233/s0458_re\n",
            "Generating list of all files for: patient233/s0459_re\n",
            "Generating list of all files for: patient233/s0482_re\n",
            "Generating list of all files for: patient233/s0483_re\n",
            "Generating list of all files for: patient234/s0460_re\n",
            "Generating list of all files for: patient235/s0461_re\n",
            "Generating list of all files for: patient236/s0462_re\n",
            "Generating list of all files for: patient236/s0463_re\n",
            "Generating list of all files for: patient236/s0464_re\n",
            "Generating list of all files for: patient237/s0465_re\n",
            "Generating list of all files for: patient238/s0466_re\n",
            "Generating list of all files for: patient239/s0467_re\n",
            "Generating list of all files for: patient240/s0468_re\n",
            "Generating list of all files for: patient241/s0469_re\n",
            "Generating list of all files for: patient241/s0470_re\n",
            "Generating list of all files for: patient242/s0471_re\n",
            "Generating list of all files for: patient243/s0472_re\n",
            "Generating list of all files for: patient244/s0473_re\n",
            "Generating list of all files for: patient245/s0474_re\n",
            "Generating list of all files for: patient245/s0480_re\n",
            "Generating list of all files for: patient246/s0478_re\n",
            "Generating list of all files for: patient247/s0479_re\n",
            "Generating list of all files for: patient248/s0481_re\n",
            "Generating list of all files for: patient249/s0484_re\n",
            "Generating list of all files for: patient250/s0485_re\n",
            "Generating list of all files for: patient251/s0486_re\n",
            "Generating list of all files for: patient251/s0503_re\n",
            "Generating list of all files for: patient251/s0506_re\n",
            "Generating list of all files for: patient252/s0487_re\n",
            "Generating list of all files for: patient253/s0488_re\n",
            "Generating list of all files for: patient254/s0489_re\n",
            "Generating list of all files for: patient255/s0491_re\n",
            "Generating list of all files for: patient256/s0492_re\n",
            "Generating list of all files for: patient257/s0493_re\n",
            "Generating list of all files for: patient258/s0494_re\n",
            "Generating list of all files for: patient259/s0495_re\n",
            "Generating list of all files for: patient260/s0496_re\n",
            "Generating list of all files for: patient261/s0497_re\n",
            "Generating list of all files for: patient262/s0498_re\n",
            "Generating list of all files for: patient263/s0499_re\n",
            "Generating list of all files for: patient264/s0500_re\n",
            "Generating list of all files for: patient265/s0501_re\n",
            "Generating list of all files for: patient266/s0502_re\n",
            "Generating list of all files for: patient267/s0504_re\n",
            "Generating list of all files for: patient268/s0505_re\n",
            "Generating list of all files for: patient269/s0508_re\n",
            "Generating list of all files for: patient270/s0507_re\n",
            "Generating list of all files for: patient271/s0509_re\n",
            "Generating list of all files for: patient272/s0510_re\n",
            "Generating list of all files for: patient273/s0511_re\n",
            "Generating list of all files for: patient274/s0512_re\n",
            "Generating list of all files for: patient275/s0513_re\n",
            "Generating list of all files for: patient276/s0526_re\n",
            "Generating list of all files for: patient277/s0527_re\n",
            "Generating list of all files for: patient278/s0528_re\n",
            "Generating list of all files for: patient278/s0529_re\n",
            "Generating list of all files for: patient278/s0530_re\n",
            "Generating list of all files for: patient279/s0531_re\n",
            "Generating list of all files for: patient279/s0532_re\n",
            "Generating list of all files for: patient279/s0533_re\n",
            "Generating list of all files for: patient279/s0534_re\n",
            "Generating list of all files for: patient280/s0535_re\n",
            "Generating list of all files for: patient281/s0537_re\n",
            "Generating list of all files for: patient282/s0539_re\n",
            "Generating list of all files for: patient283/s0542_re\n",
            "Generating list of all files for: patient284/s0543_re\n",
            "Generating list of all files for: patient284/s0551_re\n",
            "Generating list of all files for: patient284/s0552_re\n",
            "Generating list of all files for: patient285/s0544_re\n",
            "Generating list of all files for: patient286/s0546_re\n",
            "Generating list of all files for: patient287/s0547_re\n",
            "Generating list of all files for: patient287/s0548_re\n",
            "Generating list of all files for: patient288/s0549_re\n",
            "Generating list of all files for: patient289/s0550_re\n",
            "Generating list of all files for: patient290/s0553_re\n",
            "Generating list of all files for: patient291/s0554_re\n",
            "Generating list of all files for: patient292/s0555_re\n",
            "Generating list of all files for: patient292/s0556_re\n",
            "Generating list of all files for: patient293/s0557_re\n",
            "Generating list of all files for: patient293/s0558_re\n",
            "Generating list of all files for: patient294/s0559_re\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2229968752.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# This is a large database. We download the index first to find healthy controls later,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# or just download the whole thing. It's safer to download all to ensure we get the headers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ptbdb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPTBDB_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PTBDB Download complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wfdb/io/record.py\u001b[0m in \u001b[0;36mdl_database\u001b[0;34m(db_dir, dl_dir, records, annotators, keep_subdirs, overwrite)\u001b[0m\n\u001b[1;32m   3204\u001b[0m     \u001b[0;31m# Limit to 2 connections to avoid overloading the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl_pn_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished downloading files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         '''\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PTBDB alternate code to just get first 10 files"
      ],
      "metadata": {
        "id": "oV-qsxFLFRgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import os\n",
        "\n",
        "PTBDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/PTBDB\"\n",
        "\n",
        "# Create directory\n",
        "if not os.path.exists(PTBDB_PATH):\n",
        "    os.makedirs(PTBDB_PATH)\n",
        "\n",
        "print(f\"Fetching record list for PTBDB...\")\n",
        "\n",
        "try:\n",
        "    # 1. Get the full list of records from PhysioNet\n",
        "    full_list = wfdb.get_record_list('ptbdb')\n",
        "\n",
        "    # 2. Slice the first 10 records\n",
        "    target_records = full_list[:10]\n",
        "    print(f\"Downloading first 10 records: {target_records}\")\n",
        "\n",
        "    # 3. Download only those specific records\n",
        "    wfdb.dl_database('ptbdb', PTBDB_PATH, records=target_records)\n",
        "    print(\"\\nDownload complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1KaJvo1FU4s",
        "outputId": "830b0d91-1e4e-4586-a47d-8718656856d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching record list for PTBDB...\n",
            "Downloading first 10 records: ['patient001/s0010_re', 'patient001/s0014lre', 'patient001/s0016lre', 'patient002/s0015lre', 'patient003/s0017lre', 'patient004/s0020are', 'patient004/s0020bre', 'patient005/s0021are', 'patient005/s0021bre', 'patient005/s0025lre']\n",
            "Generating record list for: patient001/s0010_re\n",
            "Generating record list for: patient001/s0014lre\n",
            "Generating record list for: patient001/s0016lre\n",
            "Generating record list for: patient002/s0015lre\n",
            "Generating record list for: patient003/s0017lre\n",
            "Generating record list for: patient004/s0020are\n",
            "Generating record list for: patient004/s0020bre\n",
            "Generating record list for: patient005/s0021are\n",
            "Generating record list for: patient005/s0021bre\n",
            "Generating record list for: patient005/s0025lre\n",
            "Generating list of all files for: patient001/s0010_re\n",
            "Generating list of all files for: patient001/s0014lre\n",
            "Generating list of all files for: patient001/s0016lre\n",
            "Generating list of all files for: patient002/s0015lre\n",
            "Generating list of all files for: patient003/s0017lre\n",
            "Generating list of all files for: patient004/s0020are\n",
            "Generating list of all files for: patient004/s0020bre\n",
            "Generating list of all files for: patient005/s0021are\n",
            "Generating list of all files for: patient005/s0021bre\n",
            "Generating list of all files for: patient005/s0025lre\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MITDB"
      ],
      "metadata": {
        "id": "jTu5BSTR4s1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import detrend, savgol_filter, resample\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "WARMUP_SEC = 30\n",
        "\n",
        "MITDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/MITDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ Model & Helpers ------------------\n",
        "# (Paste your standard LSTMAutoencoder class, load_model, clean_signal,\n",
        "# is_mechanically_sound, get_error_and_morphology, detect_vtac_start_clinical functions here)\n",
        "# For brevity, I assume they are defined as in previous cells.\n",
        "\n",
        "# [PASTE THE FUNCTIONS FROM PREVIOUS CELLS HERE IF NOT ALREADY IN MEMORY]\n",
        "# ...\n",
        "\n",
        "# ==================================================\n",
        "#           MITDB EVALUATION LOOP\n",
        "# ==================================================\n",
        "print(f\"{'Record':<10} | {'Annotated':<10} | {'Predicted':<10} | {'Diff (s)':<10} | {'Status'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# Records known to contain VT/VF or Ventricular Flutter\n",
        "# (Source: PhysioNet MITDB annotations)\n",
        "TARGET_RECORDS = ['200', '201', '202', '203', '205', '207', '210', '213', '214', '215', '219', '220', '221', '223', '228', '233', '234']\n",
        "\n",
        "results = []\n",
        "\n",
        "for rec_name in TARGET_RECORDS:\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(MITDB_PATH, rec_name))\n",
        "        ann = wfdb.rdann(os.path.join(MITDB_PATH, rec_name), 'atr')\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    # MITDB is 360Hz. We MUST Resample to 250Hz.\n",
        "    sig_360 = record.p_signal[:, 0]\n",
        "    num_samples_250 = int(len(sig_360) * (250 / 360))\n",
        "    ecg = resample(sig_360, num_samples_250)\n",
        "\n",
        "    # Adjust annotations to new FS (250Hz)\n",
        "    ratio = 250 / 360\n",
        "\n",
        "    # Find VT Annotation\n",
        "    # MITDB uses '[' for VF start, or specific rhythm strings\n",
        "    vtac_annot_sec = None\n",
        "\n",
        "    for i, (sym, samp) in enumerate(zip(ann.symbol, ann.sample)):\n",
        "        if sym == '[': # Start of VF/Flutter\n",
        "            vtac_annot_sec = int(samp * ratio) // FS\n",
        "            break\n",
        "        # Also check for rhythm text annotations if '[' is missing\n",
        "        if sym == '+' and hasattr(ann, 'aux_note'):\n",
        "            note = ann.aux_note[i]\n",
        "            if '(VT' in note or '(VFL' in note:\n",
        "                vtac_annot_sec = int(samp * ratio) // FS\n",
        "                break\n",
        "\n",
        "    if vtac_annot_sec is None:\n",
        "        continue\n",
        "\n",
        "    # Run Prediction\n",
        "    vtac_pred_sec = detect_vtac_start_clinical(ecg)\n",
        "\n",
        "    if vtac_pred_sec is None:\n",
        "        print(f\"{rec_name:<10} | {vtac_annot_sec:<10} | {'---':<10} | {'---':<10} | MISSED\")\n",
        "    else:\n",
        "        diff_sec = vtac_pred_sec - vtac_annot_sec\n",
        "        results.append(diff_sec)\n",
        "        status = \"EARLY\" if diff_sec < 0 else \"LATE\"\n",
        "        print(f\"{rec_name:<10} | {vtac_annot_sec:<10} | {vtac_pred_sec:<10} | {diff_sec:<10.2f} | {status}\")\n",
        "\n",
        "print(\"-\" * 65)\n",
        "if len(results) > 0:\n",
        "    print(f\"Mean Prediction: {np.mean(results):.2f} s\")\n",
        "else:\n",
        "    print(\"No detections.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5obZb0k64v-Q",
        "outputId": "0562b4a8-dfa4-4823-ef89-834308d7b3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record     | Annotated  | Predicted  | Diff (s)   | Status\n",
            "-----------------------------------------------------------------\n",
            "200        | 108        | ---        | ---        | MISSED\n",
            "203        | 46         | 565        | 519.00     | LATE\n",
            "205        | 298        | ---        | ---        | MISSED\n",
            "207        | 38         | 505        | 467.00     | LATE\n",
            "210        | 420        | ---        | ---        | MISSED\n",
            "213        | 895        | 132        | -763.00    | EARLY\n",
            "214        | 342        | ---        | ---        | MISSED\n",
            "215        | 179        | ---        | ---        | MISSED\n",
            "221        | 784        | ---        | ---        | MISSED\n",
            "223        | 578        | ---        | ---        | MISSED\n",
            "233        | 27         | ---        | ---        | MISSED\n",
            "-----------------------------------------------------------------\n",
            "Mean Prediction: 74.33 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PTBDB"
      ],
      "metadata": {
        "id": "wBYyiH4U4y3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import resample\n",
        "\n",
        "# ... (Previous Configuration) ...\n",
        "PTBDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/PTBDB\"\n",
        "# PTBDB is usually 1000Hz. We verify in loop.\n",
        "\n",
        "print(f\"{'Record':<20} | {'Duration (m)':<12} | {'FP Events':<10}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "total_records = 0\n",
        "total_fp_events = 0\n",
        "total_hours_analyzed = 0.0\n",
        "\n",
        "# Recursively find all header files in PTBDB structure\n",
        "ptb_files = []\n",
        "for root, dirs, files in os.walk(PTBDB_PATH):\n",
        "    for file in files:\n",
        "        if file.endswith(\".hea\"):\n",
        "            ptb_files.append(os.path.join(root, file))\n",
        "\n",
        "for header_path in ptb_files:\n",
        "    try:\n",
        "        # 1. Check Diagnosis in Header\n",
        "        # We read the header as a text file to find 'Reason for admission: Healthy control'\n",
        "        is_healthy = False\n",
        "        with open(header_path, 'r', errors='ignore') as f:\n",
        "            if \"Healthy control\" in f.read():\n",
        "                is_healthy = True\n",
        "\n",
        "        if not is_healthy: continue\n",
        "\n",
        "        # 2. Load Record\n",
        "        rec_name = os.path.splitext(os.path.basename(header_path))[0]\n",
        "        # We load using the full path logic of wfdb, simplified here:\n",
        "        # WFDB library needs the path relative to the base or full path without extension\n",
        "        record_path = os.path.splitext(header_path)[0]\n",
        "        record = wfdb.rdrecord(record_path)\n",
        "\n",
        "        # 3. Resample (1000Hz -> 250Hz)\n",
        "        sig_1000 = record.p_signal[:, 0]\n",
        "        current_fs = record.fs\n",
        "\n",
        "        if current_fs != 250:\n",
        "            num_samples = int(len(sig_1000) * (250 / current_fs))\n",
        "            ecg = resample(sig_1000, num_samples)\n",
        "        else:\n",
        "            ecg = sig_1000\n",
        "\n",
        "        total_records += 1\n",
        "        duration_hours = (len(ecg) / 250) / 3600.0\n",
        "        total_hours_analyzed += duration_hours\n",
        "\n",
        "        # 4. Count False Positives\n",
        "        events = count_false_positives(ecg) # Uses your standard function\n",
        "        total_fp_events += events\n",
        "\n",
        "        status = \"CLEAN\" if events == 0 else str(events)\n",
        "        print(f\"{rec_name:<20} | {duration_hours*60:<12.1f} | {status:<10}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "print(\"-\" * 55)\n",
        "if total_records > 0:\n",
        "    print(f\"Total Healthy Recs: {total_records}\")\n",
        "    print(f\"FP Rate: {total_fp_events / total_hours_analyzed:.2f} FP/Hour\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIMK7l6o4z2A",
        "outputId": "886f39b8-ab4d-4498-a424-3584d4d1054d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record               | Duration (m) | FP Events \n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SDDB"
      ],
      "metadata": {
        "id": "GzffVd9OJlgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import os\n",
        "from tqdm import tqdm  # Import the progress bar library\n",
        "\n",
        "SDDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/SDDB\"\n",
        "\n",
        "if not os.path.exists(SDDB_PATH):\n",
        "    os.makedirs(SDDB_PATH)\n",
        "\n",
        "print(f\"Fetching record list for SDDB...\")\n",
        "\n",
        "try:\n",
        "    # 1. Get the full list of records\n",
        "    full_list = wfdb.get_record_list('sddb')\n",
        "\n",
        "    # 2. Slice the first 10 records\n",
        "    target_records = full_list[:10]\n",
        "    print(f\"Targeting first 10 records: {target_records}\\n\")\n",
        "\n",
        "    # 3. Download with Progress Bar\n",
        "    # We loop through each record so tqdm can update the bar\n",
        "    for record in tqdm(target_records, desc=\"Downloading SDDB\"):\n",
        "        try:\n",
        "            # records=[record] expects a list, so we wrap the single record string in a list\n",
        "            wfdb.dl_database('sddb', SDDB_PATH, records=[record], overwrite=False)\n",
        "        except Exception as inner_e:\n",
        "            print(f\"\\nFailed to download {record}: {inner_e}\")\n",
        "\n",
        "    print(\"\\nDownload complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv-BWWD_JoKz",
        "outputId": "80c56a0a-629b-4d68-81b2-316ec7049f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching record list for SDDB...\n",
            "Targeting first 10 records: ['30', '31', '32', '33', '34', '35', '36', '37', '38', '39']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating record list for: 30\n",
            "Generating list of all files for: 30\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  10%|█         | 1/10 [00:06<00:57,  6.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 31\n",
            "Generating list of all files for: 31\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  20%|██        | 2/10 [00:09<00:36,  4.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 32\n",
            "Generating list of all files for: 32\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  30%|███       | 3/10 [00:12<00:27,  3.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 33\n",
            "Generating list of all files for: 33\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  40%|████      | 4/10 [03:45<08:36, 86.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 34\n",
            "Generating list of all files for: 34\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  50%|█████     | 5/10 [03:48<04:40, 56.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 35\n",
            "Generating list of all files for: 35\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  60%|██████    | 6/10 [03:50<02:32, 38.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 36\n",
            "Generating list of all files for: 36\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  70%|███████   | 7/10 [07:46<05:07, 102.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 37\n",
            "Generating list of all files for: 37\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  80%|████████  | 8/10 [12:11<05:08, 154.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 38\n",
            "Generating list of all files for: 38\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading SDDB:  90%|█████████ | 9/10 [15:31<02:48, 168.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "Generating record list for: 39\n",
            "Generating list of all files for: 39\n",
            "Downloading files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading SDDB: 100%|██████████| 10/10 [16:36<00:00, 99.63s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished downloading files\n",
            "\n",
            "Download complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "from scipy.signal import detrend, savgol_filter\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "WARMUP_SEC = 30\n",
        "\n",
        "SDDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/SDDB\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Model ------------------\n",
        "model = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "else:\n",
        "    print(f\"Error: Model path not found at {MODEL_PATH}\")\n",
        "model.eval()\n",
        "\n",
        "# ------------------ Helpers ------------------\n",
        "def clean_signal(window):\n",
        "    sig = np.array(window, dtype=np.float32)\n",
        "    sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sig = detrend(sig, type='linear')\n",
        "    try:\n",
        "        sig = savgol_filter(sig, window_length=11, polyorder=3)\n",
        "    except:\n",
        "        pass\n",
        "    return sig\n",
        "\n",
        "def is_mechanically_sound(window):\n",
        "    if np.std(window) < 0.001: return False\n",
        "    if np.mean(np.abs(window) > 3.0) > 0.1: return False\n",
        "    return True\n",
        "\n",
        "def get_error_and_morphology(window):\n",
        "    mu = np.mean(window)\n",
        "    norm_window = window - mu\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(norm_window, dtype=torch.float32, device=DEVICE).unsqueeze(0).unsqueeze(-1)\n",
        "        recon = model(x)\n",
        "        recon_np = recon.squeeze().cpu().numpy()\n",
        "        input_np = x.squeeze().cpu().numpy()\n",
        "    mse = ((recon_np - input_np) ** 2).mean()\n",
        "    if mse > ERROR_THRESHOLD:\n",
        "        diff = np.abs(input_np - recon_np)\n",
        "        active_mask = np.abs(input_np) > 0.4\n",
        "        if np.sum(active_mask) > 0:\n",
        "            error_in_qrs = np.mean(diff[active_mask])\n",
        "            error_total = np.mean(diff)\n",
        "            is_structural = error_in_qrs > error_total\n",
        "        else:\n",
        "            is_structural = False\n",
        "        return mse, is_structural\n",
        "    return mse, True\n",
        "\n",
        "def detect_vtac_start_clinical(ecg):\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "    ecg = np.nan_to_num(ecg, nan=0.0)\n",
        "    error_history = []\n",
        "\n",
        "    start_sec = max(SEQ_LEN_SEC, WARMUP_SEC)\n",
        "    for _ in range(start_sec):\n",
        "        error_history.append(0.0)\n",
        "\n",
        "    for sec in range(start_sec, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples: break\n",
        "\n",
        "        # Clinical Pipeline\n",
        "        clean_window = clean_signal(ecg[start:end])\n",
        "        if not is_mechanically_sound(clean_window):\n",
        "            error_history.append(0.0)\n",
        "            continue\n",
        "        mse, is_structural = get_error_and_morphology(clean_window)\n",
        "        if mse > ERROR_THRESHOLD and not is_structural:\n",
        "            mse = 0.0\n",
        "        error_history.append(mse)\n",
        "\n",
        "    # Trend Analysis\n",
        "    error_history = np.array(error_history)\n",
        "    for i in range(len(error_history) - CONSECUTIVE_SEC + 1):\n",
        "        if i < WARMUP_SEC: continue\n",
        "        recent_errors = error_history[i : i + CONSECUTIVE_SEC]\n",
        "        if np.all(recent_errors > ERROR_THRESHOLD):\n",
        "            if i > 5:\n",
        "                # Artifact Rejection\n",
        "                prev_baseline = np.mean(error_history[i-5 : i])\n",
        "                current_level = np.mean(recent_errors)\n",
        "                if (current_level - prev_baseline) > 0.5:\n",
        "                    continue\n",
        "            return i + SEQ_LEN_SEC\n",
        "    return None\n",
        "\n",
        "def get_sddb_death_sample(rec_path):\n",
        "    \"\"\"Parses header comments to find 'vfon' time\"\"\"\n",
        "    try:\n",
        "        header = wfdb.rdheader(rec_path)\n",
        "        base_time = header.base_time # The clock time when recording started\n",
        "\n",
        "        if base_time is None: return None\n",
        "\n",
        "        vfon_time = None\n",
        "        for comment in header.comments:\n",
        "            if 'vfon:' in comment:\n",
        "                # Format is usually 'vfon: HH:MM:SS'\n",
        "                time_str = comment.split('vfon:')[-1].strip()\n",
        "                try:\n",
        "                    vfon_time = datetime.datetime.strptime(time_str, '%H:%M:%S').time()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if vfon_time is None: return None\n",
        "\n",
        "        # Calculate time difference in seconds\n",
        "        # We need dummy dates to handle overnight crossings\n",
        "        start_dt = datetime.datetime.combine(datetime.date.today(), base_time)\n",
        "        vfon_dt = datetime.datetime.combine(datetime.date.today(), vfon_time)\n",
        "\n",
        "        if vfon_dt < start_dt: # Crossed midnight\n",
        "            vfon_dt += datetime.timedelta(days=1)\n",
        "\n",
        "        diff_seconds = (vfon_dt - start_dt).total_seconds()\n",
        "        return int(diff_seconds * FS)\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ==================================================\n",
        "#           SDDB EVALUATION LOOP\n",
        "# ==================================================\n",
        "\n",
        "print(f\"{'Record':<10} | {'Annotated (s)':<15} | {'Predicted (s)':<15} | {'Lead Time (s)':<15} | {'Status'}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "results = []\n",
        "\n",
        "try:\n",
        "    # Scan for header files\n",
        "    files = sorted([f.replace('.hea', '') for f in os.listdir(SDDB_PATH) if f.endswith('.hea')])\n",
        "except Exception:\n",
        "    files = []\n",
        "\n",
        "for rec_name in files:\n",
        "    try:\n",
        "        full_path = os.path.join(SDDB_PATH, rec_name)\n",
        "\n",
        "        # 1. Get Ground Truth from Text Header\n",
        "        death_sample = get_sddb_death_sample(full_path)\n",
        "\n",
        "        if death_sample is None:\n",
        "            # Fallback: Try reading annotations just in case\n",
        "            try:\n",
        "                ann = wfdb.rdann(full_path, 'atr')\n",
        "                for s, sym in zip(ann.sample, ann.symbol):\n",
        "                    if sym == '[':\n",
        "                        death_sample = s\n",
        "                        break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if death_sample is None:\n",
        "            continue # Skip if we can't find when they died\n",
        "\n",
        "        # 2. Extract 30-min Window BEFORE Death\n",
        "        # We want to see if we predict it *approaching*\n",
        "        WINDOW_SIZE = 30 * 60 * FS\n",
        "        start_sample = max(0, death_sample - WINDOW_SIZE)\n",
        "        end_sample = death_sample # Stop exactly at death\n",
        "\n",
        "        record = wfdb.rdrecord(full_path, sampfrom=start_sample, sampto=end_sample)\n",
        "        ecg = record.p_signal[:, 0]\n",
        "\n",
        "        # 3. Run Prediction\n",
        "        # The prediction returns seconds relative to the START of our window\n",
        "        pred_sec_relative = detect_vtac_start_clinical(ecg)\n",
        "\n",
        "        # The death happened at the END of our window (relative time = len(ecg)/FS)\n",
        "        death_sec_relative = len(ecg) // FS\n",
        "\n",
        "        if pred_sec_relative is None:\n",
        "            print(f\"{rec_name:<10} | {death_sec_relative:<15} | {'---':<15} | {'---':<15} | MISSED\")\n",
        "        else:\n",
        "            lead_time = death_sec_relative - pred_sec_relative\n",
        "            results.append(lead_time)\n",
        "            print(f\"{rec_name:<10} | {death_sec_relative:<15} | {pred_sec_relative:<15} | {lead_time:<15.2f} | EARLY\")\n",
        "\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "print(\"-\" * 80)\n",
        "if len(results) > 0:\n",
        "    print(f\"Total Fatal Events Detected: {len(results)}\")\n",
        "    print(f\"Mean Early Warning: {np.mean(results):.2f} seconds ({np.mean(results)/60:.2f} min)\")\n",
        "else:\n",
        "    print(\"No valid detections.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1QUhBkuJ65Q",
        "outputId": "cbbcbc10-9f62-4b98-a651-a7ca5cb7d6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record     | Annotated (s)   | Predicted (s)   | Lead Time (s)   | Status\n",
            "--------------------------------------------------------------------------------\n",
            "30         | 1800            | ---             | ---             | MISSED\n",
            "31         | 1800            | ---             | ---             | MISSED\n",
            "32         | 1800            | ---             | ---             | MISSED\n",
            "33         | 1800            | ---             | ---             | MISSED\n",
            "36         | 1800            | ---             | ---             | MISSED\n",
            "37         | 1800            | ---             | ---             | MISSED\n",
            "--------------------------------------------------------------------------------\n",
            "No valid detections.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Onnx"
      ],
      "metadata": {
        "id": "PvgmDwFIVrm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxscript\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS  # 2500\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_PATH_PTH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "MODEL_PATH_ONNX = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.onnx\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Conversion Logic ------------------\n",
        "# ------------------ Conversion Logic ------------------\n",
        "def convert_to_onnx():\n",
        "    print(\"Loading PyTorch model...\")\n",
        "\n",
        "    # FIX 1: Force the export process to happen on the CPU\n",
        "    # This completely bypasses the GPU/CuDNN FakeTensor memory crash\n",
        "    export_device = torch.device(\"cpu\")\n",
        "    model = LSTMAutoencoder().to(export_device)\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH_PTH):\n",
        "        print(f\"Error: PyTorch model not found at {MODEL_PATH_PTH}\")\n",
        "        return\n",
        "\n",
        "    # Load the weights and map them to the CPU\n",
        "    model.load_state_dict(torch.load(MODEL_PATH_PTH, map_location=export_device))\n",
        "    model.eval() # MUST be in eval mode for export\n",
        "\n",
        "    # Create the dummy input tensor on the CPU\n",
        "    print(\"Generating dummy input tensor (1, 2500, 1)...\")\n",
        "    dummy_input = torch.randn(1, SEQ_LEN, 1, device=export_device)\n",
        "\n",
        "    print(f\"Exporting model to ONNX format...\")\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        MODEL_PATH_ONNX,\n",
        "        export_params=True,\n",
        "        opset_version=14,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={\n",
        "            'input': {0: 'batch_size'},\n",
        "            'output': {0: 'batch_size'}\n",
        "        },\n",
        "        # FIX 2: Explicitly disable the experimental Dynamo exporter\n",
        "        # to use the stable TorchScript tracer for LSTMs.\n",
        "        # (Note: If this throws an \"unexpected keyword\" error in your specific\n",
        "        # PyTorch version, just delete this line, the CPU fix above is usually enough).\n",
        "        dynamo=False\n",
        "    )\n",
        "    print(f\"Success! ONNX model saved successfully to:\\n{MODEL_PATH_ONNX}\")\n",
        "\n",
        "# Run the conversion\n",
        "convert_to_onnx()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TacwsAQWYB34",
        "outputId": "02b260a6-d838-42e3-ef0f-dd9a8978279b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Requirement already satisfied: onnx_ir<2,>=0.1.15 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.1.16)\n",
            "Requirement already satisfied: onnx>=1.17 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (26.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17->onnxscript) (5.29.6)\n",
            "Loading PyTorch model...\n",
            "Generating dummy input tensor (1, 2500, 1)...\n",
            "Exporting model to ONNX format...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1256318602.py:50: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! ONNX model saved successfully to:\n",
            "/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3NqvcFwaCnw",
        "outputId": "107483ed-d1c1-4b59-8d53-f19e2c8af95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: pytorch: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare Onnx and PTH"
      ],
      "metadata": {
        "id": "Il0O8qaRcxB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb onnxruntime\n",
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import onnxruntime as ort\n",
        "from scipy.signal import detrend, savgol_filter\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Clinical Thresholds\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "WARMUP_SEC = 30\n",
        "\n",
        "# Dataset Settings\n",
        "MAX_MINUTES = 30        # Analyze first 30 mins\n",
        "FANTASIA_PATH = \"/content/drive/MyDrive/ECG_Datasets/Fantasia\"\n",
        "\n",
        "# Model Paths\n",
        "MODEL_PATH_PTH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "MODEL_PATH_ONNX = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.onnx\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Models (PTH & ONNX) ------------------\n",
        "# 1. Load PyTorch Model\n",
        "model_pth = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH_PTH):\n",
        "    model_pth.load_state_dict(torch.load(MODEL_PATH_PTH, map_location=DEVICE))\n",
        "    print(f\"PyTorch Model loaded from {MODEL_PATH_PTH}\")\n",
        "else:\n",
        "    print(f\"Error: PyTorch Model path not found at {MODEL_PATH_PTH}\")\n",
        "model_pth.eval()\n",
        "\n",
        "# 2. Load ONNX Model\n",
        "if os.path.exists(MODEL_PATH_ONNX):\n",
        "    ort_session = ort.InferenceSession(MODEL_PATH_ONNX)\n",
        "    input_name = ort_session.get_inputs()[0].name\n",
        "    print(f\"ONNX Model loaded from {MODEL_PATH_ONNX}\")\n",
        "else:\n",
        "    print(f\"Error: ONNX Model path not found at {MODEL_PATH_ONNX}\")\n",
        "\n",
        "# ------------------ 1. CLEANING ------------------\n",
        "def clean_signal(window):\n",
        "    sig = np.array(window, dtype=np.float32)\n",
        "    sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sig = detrend(sig, type='linear')\n",
        "    try:\n",
        "        sig = savgol_filter(sig, window_length=11, polyorder=3)\n",
        "    except:\n",
        "        pass\n",
        "    return sig\n",
        "\n",
        "# ------------------ 2. GATEKEEPER ------------------\n",
        "def is_mechanically_sound(window):\n",
        "    if np.std(window) < 0.001: return False\n",
        "    if np.mean(np.abs(window) > 3.0) > 0.1: return False\n",
        "    return True\n",
        "\n",
        "# ------------------ 3. RECONSTRUCTION (Dual Mode) ------------------\n",
        "def get_error_and_morphology(window, model_type=\"pth\"):\n",
        "    mu = np.mean(window)\n",
        "    norm_window = window - mu\n",
        "\n",
        "    # Prepare NumPy array of shape (1, 2500, 1)\n",
        "    x_np = np.expand_dims(np.expand_dims(norm_window, axis=0), axis=-1).astype(np.float32)\n",
        "\n",
        "    if model_type == \"pth\":\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, device=DEVICE)\n",
        "            recon = model_pth(x_tensor)\n",
        "            recon_np = recon.squeeze().cpu().numpy()\n",
        "    elif model_type == \"onnx\":\n",
        "        ort_inputs = {input_name: x_np}\n",
        "        recon = ort_session.run(None, ort_inputs)[0]\n",
        "        recon_np = np.squeeze(recon)\n",
        "\n",
        "    input_np = np.squeeze(x_np)\n",
        "    mse = ((recon_np - input_np) ** 2).mean()\n",
        "\n",
        "    if mse > ERROR_THRESHOLD:\n",
        "        diff = np.abs(input_np - recon_np)\n",
        "        active_mask = np.abs(input_np) > 0.4\n",
        "\n",
        "        if np.sum(active_mask) > 0:\n",
        "            error_in_qrs = np.mean(diff[active_mask])\n",
        "            error_total = np.mean(diff)\n",
        "            is_structural = error_in_qrs > error_total\n",
        "        else:\n",
        "            is_structural = False\n",
        "\n",
        "        return mse, is_structural\n",
        "\n",
        "    return mse, True\n",
        "\n",
        "# ------------------ 4. COUNT FALSE POSITIVES ------------------\n",
        "def count_false_positives(ecg, model_type=\"pth\"):\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // FS\n",
        "\n",
        "    ecg = np.nan_to_num(ecg, nan=0.0)\n",
        "    error_history = []\n",
        "\n",
        "    start_sec = max(SEQ_LEN_SEC, WARMUP_SEC)\n",
        "    for _ in range(start_sec):\n",
        "        error_history.append(0.0)\n",
        "\n",
        "    # Pass 1: Calc Errors\n",
        "    for sec in range(start_sec, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * FS\n",
        "        end = start + SEQ_LEN\n",
        "        if end > n_samples: break\n",
        "\n",
        "        raw_window = ecg[start:end]\n",
        "        clean_window = clean_signal(raw_window)\n",
        "\n",
        "        if not is_mechanically_sound(clean_window):\n",
        "            error_history.append(0.0)\n",
        "            continue\n",
        "\n",
        "        mse, is_structural = get_error_and_morphology(clean_window, model_type)\n",
        "\n",
        "        if mse > ERROR_THRESHOLD and not is_structural:\n",
        "            mse = 0.0\n",
        "\n",
        "        error_history.append(mse)\n",
        "\n",
        "    # Pass 2: Count Events\n",
        "    error_history = np.array(error_history)\n",
        "    fp_events = 0\n",
        "    i = WARMUP_SEC\n",
        "\n",
        "    while i < len(error_history) - CONSECUTIVE_SEC + 1:\n",
        "        recent_errors = error_history[i : i + CONSECUTIVE_SEC]\n",
        "\n",
        "        if np.all(recent_errors > ERROR_THRESHOLD):\n",
        "            is_artifact = False\n",
        "            if i > 5:\n",
        "                prev_baseline = np.mean(error_history[i-5 : i])\n",
        "                current_level = np.mean(recent_errors)\n",
        "                if (current_level - prev_baseline) > 0.5:\n",
        "                    is_artifact = True\n",
        "\n",
        "            if not is_artifact:\n",
        "                fp_events += 1\n",
        "                i += CONSECUTIVE_SEC\n",
        "                continue\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return fp_events\n",
        "\n",
        "# ==================================================\n",
        "#           FANTASIA EVALUATION LOOP (COMPARISON)\n",
        "# ==================================================\n",
        "\n",
        "print(f\"{'Record':<10} | {'Group':<10} | {'FP (PTH)':<10} | {'FP (ONNX)':<10} | {'Match Status'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "total_records = 0\n",
        "total_hours_analyzed = 0.0\n",
        "total_fp_pth = 0\n",
        "total_fp_onnx = 0\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(FANTASIA_PATH):\n",
        "        print(\"Error: Fantasia folder not found.\")\n",
        "        files = []\n",
        "    else:\n",
        "        files = sorted([f.replace('.dat', '') for f in os.listdir(FANTASIA_PATH) if f.endswith('.dat')])\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    files = []\n",
        "\n",
        "for rec_name in files:\n",
        "    try:\n",
        "        # Load record\n",
        "        record = wfdb.rdrecord(os.path.join(FANTASIA_PATH, rec_name), sampto=MAX_MINUTES*60*FS)\n",
        "\n",
        "        # Fantasia has 'ECG' and 'RESP'. Find the ECG channel.\n",
        "        ecg_idx = 0\n",
        "        if 'ECG' in record.sig_name:\n",
        "            ecg_idx = record.sig_name.index('ECG')\n",
        "        else:\n",
        "            std0 = np.std(record.p_signal[:, 0])\n",
        "            std1 = np.std(record.p_signal[:, 1])\n",
        "            ecg_idx = 0 if std0 > std1 else 1\n",
        "\n",
        "        ecg = record.p_signal[:, ecg_idx]\n",
        "\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    total_records += 1\n",
        "    duration_hours = (len(ecg) / FS) / 3600.0\n",
        "    total_hours_analyzed += duration_hours\n",
        "\n",
        "    # Identify Group\n",
        "    group = \"Young\" if 'y' in rec_name else \"Elderly\"\n",
        "\n",
        "    # Count Events for BOTH models\n",
        "    events_pth = count_false_positives(ecg, \"pth\")\n",
        "    events_onnx = count_false_positives(ecg, \"onnx\")\n",
        "\n",
        "    total_fp_pth += events_pth\n",
        "    total_fp_onnx += events_onnx\n",
        "\n",
        "    status_str_pth = \"CLEAN\" if events_pth == 0 else str(events_pth)\n",
        "    status_str_onnx = \"CLEAN\" if events_onnx == 0 else str(events_onnx)\n",
        "    match_status = \"✅ MATCH\" if events_pth == events_onnx else \"❌ MISMATCH\"\n",
        "\n",
        "    print(f\"{rec_name:<10} | {group:<10} | {status_str_pth:<10} | {status_str_onnx:<10} | {match_status}\")\n",
        "\n",
        "# ==================================================\n",
        "#               FINAL SUMMARY\n",
        "# ==================================================\n",
        "\n",
        "print(\"-\" * 65)\n",
        "if total_records > 0 and total_hours_analyzed > 0:\n",
        "    print(f\"Total Records:      {total_records}\")\n",
        "    print(f\"Total Hours:        {total_hours_analyzed:.2f}\")\n",
        "    print(f\"Total FP (PTH):     {total_fp_pth}\")\n",
        "    print(f\"Total FP (ONNX):    {total_fp_onnx}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    if total_fp_pth == total_fp_onnx:\n",
        "        print(\"SUCCESS: ONNX model behavior perfectly matches PyTorch model.\")\n",
        "    else:\n",
        "        print(\"WARNING: Divergence detected between ONNX and PyTorch outputs.\")\n",
        "else:\n",
        "    print(\"No records processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSa8sr2Fcwvn",
        "outputId": "9fcc43c7-edd1-4091-eddd-87a9d7b4e501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.1)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.24.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.12.19)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (26.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2026.1.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading onnxruntime-1.24.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.24.1\n",
            "PyTorch Model loaded from /content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\n",
            "ONNX Model loaded from /content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.onnx\n",
            "Record     | Group      | FP (PTH)   | FP (ONNX)  | Match Status\n",
            "-----------------------------------------------------------------\n",
            "f1o01      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1o02      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1o03      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1o04      | Elderly    | 2          | 2          | ✅ MATCH\n",
            "f1o05      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1o06      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1o07      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1o08      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1o09      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1o10      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1y01      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1y02      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1y03      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1y04      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1y05      | Young      | 1          | 1          | ✅ MATCH\n",
            "f1y06      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1y07      | Young      | 45         | 45         | ✅ MATCH\n",
            "f1y08      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1y09      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f1y10      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o01      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o02      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o03      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o04      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o05      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o06      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o07      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o08      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o09      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2o10      | Elderly    | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y01      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y02      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y03      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y04      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y05      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y06      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y07      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y08      | Young      | CLEAN      | CLEAN      | ✅ MATCH\n",
            "f2y09      | Young      | 3          | 3          | ✅ MATCH\n",
            "f2y10      | Young      | 3          | 3          | ✅ MATCH\n",
            "-----------------------------------------------------------------\n",
            "Total Records:      40\n",
            "Total Hours:        20.00\n",
            "Total FP (PTH):     54\n",
            "Total FP (ONNX):    54\n",
            "-----------------------------------------------------------------\n",
            "SUCCESS: ONNX model behavior perfectly matches PyTorch model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sine wave test"
      ],
      "metadata": {
        "id": "dRUPlLStuvyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import onnxruntime as ort\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model Paths\n",
        "MODEL_PATH_PTH = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.pth\"\n",
        "MODEL_PATH_ONNX = \"/content/drive/MyDrive/Models/NSR/LSTM_NSR_autoencoder_10s.onnx\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Models ------------------\n",
        "# Load PyTorch Model\n",
        "model_pth = LSTMAutoencoder().to(DEVICE)\n",
        "if os.path.exists(MODEL_PATH_PTH):\n",
        "    model_pth.load_state_dict(torch.load(MODEL_PATH_PTH, map_location=DEVICE))\n",
        "model_pth.eval()\n",
        "\n",
        "# Load ONNX Model\n",
        "if os.path.exists(MODEL_PATH_ONNX):\n",
        "    ort_session = ort.InferenceSession(MODEL_PATH_ONNX)\n",
        "    input_name = ort_session.get_inputs()[0].name\n",
        "\n",
        "# ------------------ Real-Time Sine Wave Simulator ------------------\n",
        "def run_realtime_simulation():\n",
        "    print(\"Initializing Real-Time Sine Wave Simulator...\")\n",
        "    print(f\"Sampling Rate: {FS} Hz\")\n",
        "    print(f\"Window Size: {SEQ_LEN_SEC} seconds\\n\")\n",
        "\n",
        "    buffer = []\n",
        "    t = 0.0          # Time tracker in seconds\n",
        "    freq = 1.0       # 1 Hz Sine Wave (simulating 60 BPM)\n",
        "\n",
        "    print(\"Buffering the first 10 seconds (filling the window)...\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            # 1. Generate 1 second of new data (250 samples)\n",
        "            chunk = []\n",
        "            for _ in range(FS):\n",
        "                val = math.sin(2 * math.pi * freq * t)\n",
        "                chunk.append(val)\n",
        "                t += 1.0 / FS\n",
        "\n",
        "            buffer.extend(chunk)\n",
        "\n",
        "            # 2. Check if we have a full 10-second window\n",
        "            if len(buffer) >= SEQ_LEN:\n",
        "                # Extract the exact 10-second window\n",
        "                window = np.array(buffer[-SEQ_LEN:], dtype=np.float32)\n",
        "\n",
        "                # Normalize (Mean centering)\n",
        "                mu = np.mean(window)\n",
        "                norm_window = window - mu\n",
        "\n",
        "                # Prepare Numpy array for ONNX and PyTorch (1, 2500, 1)\n",
        "                x_np = np.expand_dims(np.expand_dims(norm_window, axis=0), axis=-1).astype(np.float32)\n",
        "                input_sq = np.squeeze(x_np)\n",
        "\n",
        "                # --- PyTorch Inference ---\n",
        "                with torch.no_grad():\n",
        "                    x_tensor = torch.tensor(x_np, device=DEVICE)\n",
        "                    recon_pth = model_pth(x_tensor)\n",
        "                    recon_pth_np = recon_pth.squeeze().cpu().numpy()\n",
        "                mse_pth = ((recon_pth_np - input_sq) ** 2).mean()\n",
        "\n",
        "                # --- ONNX Inference ---\n",
        "                ort_inputs = {input_name: x_np}\n",
        "                recon_onnx = ort_session.run(None, ort_inputs)[0]\n",
        "                recon_onnx_np = np.squeeze(recon_onnx)\n",
        "                mse_onnx = ((recon_onnx_np - input_sq) ** 2).mean()\n",
        "\n",
        "                # Calculate absolute difference to ensure they match\n",
        "                diff = abs(mse_pth - mse_onnx)\n",
        "                match_status = \"✅ MATCH\" if diff < 1e-5 else \"❌ MISMATCH\"\n",
        "\n",
        "                # 3. Print the real-time output\n",
        "                print(f\"Elapsed Time: {t:05.1f}s | MSE (PTH): {mse_pth:.6f} | MSE (ONNX): {mse_onnx:.6f} | Diff: {diff:.2e} | {match_status}\")\n",
        "\n",
        "                # 4. Slide the window forward by dropping the oldest 1 second (250 samples)\n",
        "                buffer = buffer[FS:]\n",
        "\n",
        "                # 5. Wait for exactly 1 second to simulate real-time ECG streaming\n",
        "                time.sleep(1)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSimulation stopped by user.\")\n",
        "\n",
        "# Run the simulation\n",
        "run_realtime_simulation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th2Pyv8nuulo",
        "outputId": "f0a367e5-fe84-4f51-b0a1-b9ff3060d778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Real-Time Sine Wave Simulator...\n",
            "Sampling Rate: 250 Hz\n",
            "Window Size: 10 seconds\n",
            "\n",
            "Buffering the first 10 seconds (filling the window)...\n",
            "Elapsed Time: 010.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 011.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 012.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 013.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 014.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 015.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 016.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 017.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 018.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 019.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 020.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "Elapsed Time: 021.0s | MSE (PTH): 0.385548 | MSE (ONNX): 0.385548 | Diff: 7.45e-07 | ✅ MATCH\n",
            "\n",
            "Simulation stopped by user.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRn4OgNn7jtL"
      },
      "source": [
        "<h1>Master Paragraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZQAS_576Y0u"
      },
      "source": [
        "---\n",
        "**07/12**\n",
        "---\n",
        "\n",
        "Implemented the CUDB preprocessing pipeline using 90-second sliding windows to isolate pre-VTAC segments. Downloaded and processed the PhysioNet Normal Sinus Rhythm (NSR) dataset to train an LSTM next-step predictor.  Established a baseline for healthy heart dynamics, configuring the model to minimize MSE on normal sequences so that pre-VTAC deviations appear as prediction errors during inference.\n",
        "\n",
        "---\n",
        "**09/12**\n",
        "---\n",
        "\n",
        "Pivoted from predictive modeling to an LSTM Autoencoder approach to better handle subtle precursor signals. Trained the model on concatenated 150-second continuous NSR windows to capture rhythm dynamics without beat segmentation.  Established a detection logic using a consecutive-window threshold (e.g., error > 0.25 for 10 windows) to differentiate sustained arrhythmias from transient ectopic beats.\n",
        "\n",
        "---\n",
        "**11/12**\n",
        "---\n",
        "\n",
        "Optimized the inference script to handle memory constraints by sanitizing NaNs and managing window sizes. Adopted a single-record calibration strategy to tune detection thresholds based on actual model behavior rather than arbitrary guessing. Implemented checkpointing to ensure training stability on volatile cloud environments.\n",
        "\n",
        "---\n",
        "**13/12**\n",
        "---\n",
        "\n",
        "Refined the windowing strategy to be strictly retrospective (1-second strides) to ensure causal validity for real-time application. Validated on record CU01, confirming that reconstruction error rises gradually before VTAC onset. This shift to short, causal windows improves temporal resolution and ensures the model reacts to physiological changes rather than window overlap artifacts.\n",
        "\n",
        "---\n",
        "**16/12**\n",
        "---\n",
        "\n",
        "Anchored anomaly detection to clinical ground truth by comparing model-detected onsets against expert annotations for VTAC and VFIB in CUDB. Validated the system on NSRDB, confirming high selectivity with minimal false positives on healthy data. The system now quantifies \"lead time\" before fatal arrhythmias, transforming it from a numerical detector to a clinically relevant early warning system.\n",
        "\n",
        "---\n",
        "**21/12**\n",
        "---\n",
        "\n",
        "Identified a trade-off between sensitivity and false positives. Implemented a two-stage detection framework: an initial high-sensitivity LSTM detector followed by a \"Veto\" logic. This secondary stage checks for rhythm regularity (RR intervals) to explicitly suppress false alarms during normal sinus rhythm, significantly improving specificity without sacrificing early warning capabilities.\n",
        "\n",
        "---\n",
        "**25/12**\n",
        "---\n",
        "\n",
        "Finalized a 5-stage Clinical Pipeline including Signal Cleaning, Mechanical Gatekeeper, and Morphological Logic to mitigate alarm fatigue.  Validated across CUDB, VFDB, NSRDB, and Fantasia, achieving 100% sensitivity for VT/VF and a clinical-grade false positive rate (~0.58 FP/hour). Excluded SDDB from final metrics due to hardware gain mismatches (amplitude domain shift), identifying signal normalization as a future firmware requirement.\n",
        "\n",
        "---\n",
        "**Mobile App**\n",
        "---\n",
        "Mobile ECG Autoencoder Pipeline: The app processes a rolling 10-second data window (2500 samples at 250Hz), advancing by 1 second (250 samples) per inference cycle. Raw input is first converted to clinical millivolts (mV). It undergoes mathematical cleaning matching SciPy standards: explicit NaN removal, linear line-of-best-fit detrending, and smoothing via an 11-tap (polyorder 3) Savitzky-Golay FIR convolution. Because the device is worn by the general public, a strict Gatekeeper drops the window entirely if it detects flatlines (standard deviation < 0.001) or severe motion artifacts (over 10% of samples exceed 3.0mV). Valid data is exclusively mean-centered (no Min-Max scaling) to maintain exact compatibility with the PyTorch training baseline, and passed to an ONNX-based LSTM Autoencoder. Anomaly detection triggers if the Mean Squared Error (MSE) exceeds 0.30, but relies on a structural morphology check to prevent false positives: the average reconstruction error explicitly within the QRS complex region (absolute signal > 0.4mV) must be greater than the overall baseline error, ensuring the model reacts to true cardiac arrhythmias rather than background noise."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Working on the document model"
      ],
      "metadata": {
        "id": "cB8j5xsJTXk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "#        1. ENVIRONMENT SETUP & GOOGLE DRIVE MOUNTING\n",
        "# ==========================================================\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ==========================================================\n",
        "#               2. CONFIGURATION & PATHS\n",
        "# ==========================================================\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "LR = 0.001\n",
        "\n",
        "# Deep Ensemble Configuration\n",
        "NUM_MODELS = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# File Paths\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Models/nsr_100s/X_nsr_final.npy\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/Models/NSR_Ensemble/\"\n",
        "\n",
        "# Verify dataset exists before proceeding\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    raise FileNotFoundError(f\"CRITICAL ERROR: Could not find the dataset at {DATASET_PATH}. Please verify the file path.\")\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "print(f\"Save directory verified at: {MODEL_DIR}\")\n",
        "\n",
        "# ==========================================================\n",
        "#               3. DATASET & ARCHITECTURE\n",
        "# ==========================================================\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx][:, None]\n",
        "        return x, x\n",
        "\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# Load data into RAM\n",
        "print(\"Loading NSR dataset into memory...\")\n",
        "X_nsr = np.load(DATASET_PATH)\n",
        "dataset = ECGDataset(X_nsr)\n",
        "print(f\"Dataset loaded. Total sequences: {len(dataset)}\")\n",
        "\n",
        "# ==========================================================\n",
        "#               4. CRASH-PROOF TRAINING LOOP\n",
        "# ==========================================================\n",
        "total_epochs_all_models = NUM_MODELS * EPOCHS\n",
        "completed_epochs_overall = 0\n",
        "\n",
        "# Pre-scan to count completed epochs for accurate ETA and Resume\n",
        "for m in range(1, NUM_MODELS + 1):\n",
        "    final_path = os.path.join(MODEL_DIR, f\"LSTM_NSR_autoencoder_10s_ensemble_{m}.pth\")\n",
        "    ckpt_path = os.path.join(MODEL_DIR, f\"ensemble_{m}_checkpoint.pth\")\n",
        "\n",
        "    if os.path.exists(final_path):\n",
        "        completed_epochs_overall += EPOCHS\n",
        "    elif os.path.exists(ckpt_path):\n",
        "        checkpoint = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n",
        "        completed_epochs_overall += checkpoint['epoch'] + 1\n",
        "\n",
        "print(f\"\\nSystem Check: {completed_epochs_overall}/{total_epochs_all_models} total epochs already completed.\")\n",
        "print(f\"Starting Deep Ensemble Training on {DEVICE}...\\n\")\n",
        "\n",
        "global_start_time = time.time()\n",
        "epochs_run_this_session = 0\n",
        "\n",
        "for m in range(1, NUM_MODELS + 1):\n",
        "    final_model_path = os.path.join(MODEL_DIR, f\"LSTM_NSR_autoencoder_10s_ensemble_{m}.pth\")\n",
        "    checkpoint_path = os.path.join(MODEL_DIR, f\"ensemble_{m}_checkpoint.pth\")\n",
        "\n",
        "    # Skip if this specific model is already fully trained\n",
        "    if os.path.exists(final_model_path):\n",
        "        print(f\"Model {m} is already fully trained. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n--- Training Model {m}/{NUM_MODELS} ---\")\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    model = LSTMAutoencoder().to(DEVICE)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    start_epoch = 0\n",
        "    next_save_pct = 5.0\n",
        "\n",
        "    # Load Checkpoint if it exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Found partial checkpoint for Model {m}. Recovering state...\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "        # Fast-forward the 5% save tracker\n",
        "        while (start_epoch / EPOCHS) * 100 >= next_save_pct:\n",
        "            next_save_pct += 5.0\n",
        "        print(f\"Successfully resumed from Epoch {start_epoch+1}\")\n",
        "\n",
        "    # Epoch Loop\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x = batch_x.to(DEVICE)\n",
        "            batch_y = batch_y.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(dataset)\n",
        "\n",
        "        # Update Time and Progress Trackers\n",
        "        completed_epochs_overall += 1\n",
        "        epochs_run_this_session += 1\n",
        "\n",
        "        # Calculate ETA based on the average time per epoch in the current session\n",
        "        avg_time_per_epoch = (time.time() - global_start_time) / epochs_run_this_session\n",
        "        epochs_left = total_epochs_all_models - completed_epochs_overall\n",
        "        eta_seconds = epochs_left * avg_time_per_epoch\n",
        "        eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "\n",
        "        # Calculate Percentages\n",
        "        model_pct = ((epoch + 1) / EPOCHS) * 100\n",
        "        total_pct = (completed_epochs_overall / total_epochs_all_models) * 100\n",
        "\n",
        "        print(f\"Model {m} | Epoch {epoch+1}/{EPOCHS} [{model_pct:.1f}%] | Loss: {avg_loss:.6f} | Total Progress: {total_pct:.1f}% | Time Left: {eta_string}\")\n",
        "\n",
        "        # Save Checkpoint at 5% intervals\n",
        "        if model_pct >= next_save_pct:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss\n",
        "            }, checkpoint_path)\n",
        "            print(f\"  -> Checkpoint saved at {model_pct:.1f}% completion.\")\n",
        "            next_save_pct += 5.0\n",
        "\n",
        "    # Save Final Model and wipe the temporary checkpoint\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    print(f\"Model {m} training complete and finalized.\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        os.remove(checkpoint_path)\n",
        "\n",
        "print(\"\\nSuccess! All 10 Deep Ensemble models have been trained and secured in Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZEMhg8CTf2k",
        "outputId": "04a2c2ae-2870-47d9-d01e-e9a2ba987914"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Save directory verified at: /content/drive/MyDrive/Models/NSR_Ensemble/\n",
            "Loading NSR dataset into memory...\n",
            "Dataset loaded. Total sequences: 18\n",
            "\n",
            "System Check: 0/300 total epochs already completed.\n",
            "Starting Deep Ensemble Training on cuda...\n",
            "\n",
            "\n",
            "--- Training Model 1/10 ---\n",
            "Model 1 | Epoch 1/30 [3.3%] | Loss: 0.139030 | Total Progress: 0.3% | Time Left: 0:41:09\n",
            "Model 1 | Epoch 2/30 [6.7%] | Loss: 0.138586 | Total Progress: 0.7% | Time Left: 0:21:32\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 1 | Epoch 3/30 [10.0%] | Loss: 0.138197 | Total Progress: 1.0% | Time Left: 0:15:02\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 1 | Epoch 4/30 [13.3%] | Loss: 0.137612 | Total Progress: 1.3% | Time Left: 0:11:47\n",
            "Model 1 | Epoch 5/30 [16.7%] | Loss: 0.136920 | Total Progress: 1.7% | Time Left: 0:09:49\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 1 | Epoch 6/30 [20.0%] | Loss: 0.136128 | Total Progress: 2.0% | Time Left: 0:08:31\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 1 | Epoch 7/30 [23.3%] | Loss: 0.135132 | Total Progress: 2.3% | Time Left: 0:07:35\n",
            "Model 1 | Epoch 8/30 [26.7%] | Loss: 0.134128 | Total Progress: 2.7% | Time Left: 0:06:52\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 1 | Epoch 9/30 [30.0%] | Loss: 0.132988 | Total Progress: 3.0% | Time Left: 0:06:20\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 1 | Epoch 10/30 [33.3%] | Loss: 0.132033 | Total Progress: 3.3% | Time Left: 0:05:53\n",
            "Model 1 | Epoch 11/30 [36.7%] | Loss: 0.131131 | Total Progress: 3.7% | Time Left: 0:05:31\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 1 | Epoch 12/30 [40.0%] | Loss: 0.130245 | Total Progress: 4.0% | Time Left: 0:05:13\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 1 | Epoch 13/30 [43.3%] | Loss: 0.129537 | Total Progress: 4.3% | Time Left: 0:04:58\n",
            "Model 1 | Epoch 14/30 [46.7%] | Loss: 0.129141 | Total Progress: 4.7% | Time Left: 0:04:45\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 1 | Epoch 15/30 [50.0%] | Loss: 0.128803 | Total Progress: 5.0% | Time Left: 0:04:34\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 1 | Epoch 16/30 [53.3%] | Loss: 0.127562 | Total Progress: 5.3% | Time Left: 0:04:24\n",
            "Model 1 | Epoch 17/30 [56.7%] | Loss: 0.127148 | Total Progress: 5.7% | Time Left: 0:04:15\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 1 | Epoch 18/30 [60.0%] | Loss: 0.126242 | Total Progress: 6.0% | Time Left: 0:04:07\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 1 | Epoch 19/30 [63.3%] | Loss: 0.125772 | Total Progress: 6.3% | Time Left: 0:03:59\n",
            "Model 1 | Epoch 20/30 [66.7%] | Loss: 0.125058 | Total Progress: 6.7% | Time Left: 0:03:53\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 1 | Epoch 21/30 [70.0%] | Loss: 0.124643 | Total Progress: 7.0% | Time Left: 0:03:47\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 1 | Epoch 22/30 [73.3%] | Loss: 0.123969 | Total Progress: 7.3% | Time Left: 0:03:41\n",
            "Model 1 | Epoch 23/30 [76.7%] | Loss: 0.123644 | Total Progress: 7.7% | Time Left: 0:03:36\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 1 | Epoch 24/30 [80.0%] | Loss: 0.123001 | Total Progress: 8.0% | Time Left: 0:03:32\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 1 | Epoch 25/30 [83.3%] | Loss: 0.122641 | Total Progress: 8.3% | Time Left: 0:03:27\n",
            "Model 1 | Epoch 26/30 [86.7%] | Loss: 0.122003 | Total Progress: 8.7% | Time Left: 0:03:23\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 1 | Epoch 27/30 [90.0%] | Loss: 0.121594 | Total Progress: 9.0% | Time Left: 0:03:19\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 1 | Epoch 28/30 [93.3%] | Loss: 0.121009 | Total Progress: 9.3% | Time Left: 0:03:16\n",
            "Model 1 | Epoch 29/30 [96.7%] | Loss: 0.120316 | Total Progress: 9.7% | Time Left: 0:03:12\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 1 | Epoch 30/30 [100.0%] | Loss: 0.119929 | Total Progress: 10.0% | Time Left: 0:03:09\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 1 training complete and finalized.\n",
            "\n",
            "--- Training Model 2/10 ---\n",
            "Model 2 | Epoch 1/30 [3.3%] | Loss: 0.222188 | Total Progress: 10.3% | Time Left: 0:03:07\n",
            "Model 2 | Epoch 2/30 [6.7%] | Loss: 0.210619 | Total Progress: 10.7% | Time Left: 0:03:04\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 2 | Epoch 3/30 [10.0%] | Loss: 0.200776 | Total Progress: 11.0% | Time Left: 0:03:01\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 2 | Epoch 4/30 [13.3%] | Loss: 0.193486 | Total Progress: 11.3% | Time Left: 0:02:59\n",
            "Model 2 | Epoch 5/30 [16.7%] | Loss: 0.187354 | Total Progress: 11.7% | Time Left: 0:02:56\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 2 | Epoch 6/30 [20.0%] | Loss: 0.180387 | Total Progress: 12.0% | Time Left: 0:02:54\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 2 | Epoch 7/30 [23.3%] | Loss: 0.171314 | Total Progress: 12.3% | Time Left: 0:02:52\n",
            "Model 2 | Epoch 8/30 [26.7%] | Loss: 0.159933 | Total Progress: 12.7% | Time Left: 0:02:50\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 2 | Epoch 9/30 [30.0%] | Loss: 0.148155 | Total Progress: 13.0% | Time Left: 0:02:48\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 2 | Epoch 10/30 [33.3%] | Loss: 0.140550 | Total Progress: 13.3% | Time Left: 0:02:46\n",
            "Model 2 | Epoch 11/30 [36.7%] | Loss: 0.138798 | Total Progress: 13.7% | Time Left: 0:02:44\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 2 | Epoch 12/30 [40.0%] | Loss: 0.138976 | Total Progress: 14.0% | Time Left: 0:02:42\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 2 | Epoch 13/30 [43.3%] | Loss: 0.139222 | Total Progress: 14.3% | Time Left: 0:02:40\n",
            "Model 2 | Epoch 14/30 [46.7%] | Loss: 0.139376 | Total Progress: 14.7% | Time Left: 0:02:39\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 2 | Epoch 15/30 [50.0%] | Loss: 0.139461 | Total Progress: 15.0% | Time Left: 0:02:37\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 2 | Epoch 16/30 [53.3%] | Loss: 0.139503 | Total Progress: 15.3% | Time Left: 0:02:36\n",
            "Model 2 | Epoch 17/30 [56.7%] | Loss: 0.139523 | Total Progress: 15.7% | Time Left: 0:02:34\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 2 | Epoch 18/30 [60.0%] | Loss: 0.139531 | Total Progress: 16.0% | Time Left: 0:02:33\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 2 | Epoch 19/30 [63.3%] | Loss: 0.139532 | Total Progress: 16.3% | Time Left: 0:02:31\n",
            "Model 2 | Epoch 20/30 [66.7%] | Loss: 0.139529 | Total Progress: 16.7% | Time Left: 0:02:30\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 2 | Epoch 21/30 [70.0%] | Loss: 0.139522 | Total Progress: 17.0% | Time Left: 0:02:29\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 2 | Epoch 22/30 [73.3%] | Loss: 0.139514 | Total Progress: 17.3% | Time Left: 0:02:27\n",
            "Model 2 | Epoch 23/30 [76.7%] | Loss: 0.139504 | Total Progress: 17.7% | Time Left: 0:02:26\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 2 | Epoch 24/30 [80.0%] | Loss: 0.139492 | Total Progress: 18.0% | Time Left: 0:02:25\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 2 | Epoch 25/30 [83.3%] | Loss: 0.139479 | Total Progress: 18.3% | Time Left: 0:02:24\n",
            "Model 2 | Epoch 26/30 [86.7%] | Loss: 0.139465 | Total Progress: 18.7% | Time Left: 0:02:22\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 2 | Epoch 27/30 [90.0%] | Loss: 0.139450 | Total Progress: 19.0% | Time Left: 0:02:21\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 2 | Epoch 28/30 [93.3%] | Loss: 0.139435 | Total Progress: 19.3% | Time Left: 0:02:20\n",
            "Model 2 | Epoch 29/30 [96.7%] | Loss: 0.139419 | Total Progress: 19.7% | Time Left: 0:02:19\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 2 | Epoch 30/30 [100.0%] | Loss: 0.139403 | Total Progress: 20.0% | Time Left: 0:02:18\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 2 training complete and finalized.\n",
            "\n",
            "--- Training Model 3/10 ---\n",
            "Model 3 | Epoch 1/30 [3.3%] | Loss: 0.234961 | Total Progress: 20.3% | Time Left: 0:02:17\n",
            "Model 3 | Epoch 2/30 [6.7%] | Loss: 0.205885 | Total Progress: 20.7% | Time Left: 0:02:16\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 3 | Epoch 3/30 [10.0%] | Loss: 0.198546 | Total Progress: 21.0% | Time Left: 0:02:15\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 3 | Epoch 4/30 [13.3%] | Loss: 0.189496 | Total Progress: 21.3% | Time Left: 0:02:14\n",
            "Model 3 | Epoch 5/30 [16.7%] | Loss: 0.178279 | Total Progress: 21.7% | Time Left: 0:02:13\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 3 | Epoch 6/30 [20.0%] | Loss: 0.165210 | Total Progress: 22.0% | Time Left: 0:02:12\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 3 | Epoch 7/30 [23.3%] | Loss: 0.151433 | Total Progress: 22.3% | Time Left: 0:02:11\n",
            "Model 3 | Epoch 8/30 [26.7%] | Loss: 0.139652 | Total Progress: 22.7% | Time Left: 0:02:10\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 3 | Epoch 9/30 [30.0%] | Loss: 0.134224 | Total Progress: 23.0% | Time Left: 0:02:09\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 3 | Epoch 10/30 [33.3%] | Loss: 0.137537 | Total Progress: 23.3% | Time Left: 0:02:08\n",
            "Model 3 | Epoch 11/30 [36.7%] | Loss: 0.143202 | Total Progress: 23.7% | Time Left: 0:02:07\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 3 | Epoch 12/30 [40.0%] | Loss: 0.145144 | Total Progress: 24.0% | Time Left: 0:02:06\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 3 | Epoch 13/30 [43.3%] | Loss: 0.143303 | Total Progress: 24.3% | Time Left: 0:02:05\n",
            "Model 3 | Epoch 14/30 [46.7%] | Loss: 0.139190 | Total Progress: 24.7% | Time Left: 0:02:04\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 3 | Epoch 15/30 [50.0%] | Loss: 0.134479 | Total Progress: 25.0% | Time Left: 0:02:04\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 3 | Epoch 16/30 [53.3%] | Loss: 0.130594 | Total Progress: 25.3% | Time Left: 0:02:03\n",
            "Model 3 | Epoch 17/30 [56.7%] | Loss: 0.128264 | Total Progress: 25.7% | Time Left: 0:02:02\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 3 | Epoch 18/30 [60.0%] | Loss: 0.127414 | Total Progress: 26.0% | Time Left: 0:02:01\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 3 | Epoch 19/30 [63.3%] | Loss: 0.127473 | Total Progress: 26.3% | Time Left: 0:02:00\n",
            "Model 3 | Epoch 20/30 [66.7%] | Loss: 0.127788 | Total Progress: 26.7% | Time Left: 0:02:00\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 3 | Epoch 21/30 [70.0%] | Loss: 0.127864 | Total Progress: 27.0% | Time Left: 0:01:59\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 3 | Epoch 22/30 [73.3%] | Loss: 0.127435 | Total Progress: 27.3% | Time Left: 0:01:58\n",
            "Model 3 | Epoch 23/30 [76.7%] | Loss: 0.126437 | Total Progress: 27.7% | Time Left: 0:01:57\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 3 | Epoch 24/30 [80.0%] | Loss: 0.124960 | Total Progress: 28.0% | Time Left: 0:01:57\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 3 | Epoch 25/30 [83.3%] | Loss: 0.123205 | Total Progress: 28.3% | Time Left: 0:01:56\n",
            "Model 3 | Epoch 26/30 [86.7%] | Loss: 0.121440 | Total Progress: 28.7% | Time Left: 0:01:55\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 3 | Epoch 27/30 [90.0%] | Loss: 0.119934 | Total Progress: 29.0% | Time Left: 0:01:55\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 3 | Epoch 28/30 [93.3%] | Loss: 0.118887 | Total Progress: 29.3% | Time Left: 0:01:54\n",
            "Model 3 | Epoch 29/30 [96.7%] | Loss: 0.118336 | Total Progress: 29.7% | Time Left: 0:01:53\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 3 | Epoch 30/30 [100.0%] | Loss: 0.118118 | Total Progress: 30.0% | Time Left: 0:01:53\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 3 training complete and finalized.\n",
            "\n",
            "--- Training Model 4/10 ---\n",
            "Model 4 | Epoch 1/30 [3.3%] | Loss: 0.142957 | Total Progress: 30.3% | Time Left: 0:01:52\n",
            "Model 4 | Epoch 2/30 [6.7%] | Loss: 0.140089 | Total Progress: 30.7% | Time Left: 0:01:51\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 4 | Epoch 3/30 [10.0%] | Loss: 0.138968 | Total Progress: 31.0% | Time Left: 0:01:51\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 4 | Epoch 4/30 [13.3%] | Loss: 0.138948 | Total Progress: 31.3% | Time Left: 0:01:50\n",
            "Model 4 | Epoch 5/30 [16.7%] | Loss: 0.138997 | Total Progress: 31.7% | Time Left: 0:01:49\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 4 | Epoch 6/30 [20.0%] | Loss: 0.138991 | Total Progress: 32.0% | Time Left: 0:01:48\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 4 | Epoch 7/30 [23.3%] | Loss: 0.138948 | Total Progress: 32.3% | Time Left: 0:01:48\n",
            "Model 4 | Epoch 8/30 [26.7%] | Loss: 0.138883 | Total Progress: 32.7% | Time Left: 0:01:47\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 4 | Epoch 9/30 [30.0%] | Loss: 0.138801 | Total Progress: 33.0% | Time Left: 0:01:46\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 4 | Epoch 10/30 [33.3%] | Loss: 0.138706 | Total Progress: 33.3% | Time Left: 0:01:46\n",
            "Model 4 | Epoch 11/30 [36.7%] | Loss: 0.138600 | Total Progress: 33.7% | Time Left: 0:01:45\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 4 | Epoch 12/30 [40.0%] | Loss: 0.138483 | Total Progress: 34.0% | Time Left: 0:01:44\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 4 | Epoch 13/30 [43.3%] | Loss: 0.138355 | Total Progress: 34.3% | Time Left: 0:01:44\n",
            "Model 4 | Epoch 14/30 [46.7%] | Loss: 0.138214 | Total Progress: 34.7% | Time Left: 0:01:43\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 4 | Epoch 15/30 [50.0%] | Loss: 0.138058 | Total Progress: 35.0% | Time Left: 0:01:42\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 4 | Epoch 16/30 [53.3%] | Loss: 0.137886 | Total Progress: 35.3% | Time Left: 0:01:42\n",
            "Model 4 | Epoch 17/30 [56.7%] | Loss: 0.137694 | Total Progress: 35.7% | Time Left: 0:01:41\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 4 | Epoch 18/30 [60.0%] | Loss: 0.137482 | Total Progress: 36.0% | Time Left: 0:01:40\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 4 | Epoch 19/30 [63.3%] | Loss: 0.137246 | Total Progress: 36.3% | Time Left: 0:01:40\n",
            "Model 4 | Epoch 20/30 [66.7%] | Loss: 0.136984 | Total Progress: 36.7% | Time Left: 0:01:39\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 4 | Epoch 21/30 [70.0%] | Loss: 0.136694 | Total Progress: 37.0% | Time Left: 0:01:39\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 4 | Epoch 22/30 [73.3%] | Loss: 0.136371 | Total Progress: 37.3% | Time Left: 0:01:38\n",
            "Model 4 | Epoch 23/30 [76.7%] | Loss: 0.136015 | Total Progress: 37.7% | Time Left: 0:01:37\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 4 | Epoch 24/30 [80.0%] | Loss: 0.135630 | Total Progress: 38.0% | Time Left: 0:01:37\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 4 | Epoch 25/30 [83.3%] | Loss: 0.135231 | Total Progress: 38.3% | Time Left: 0:01:36\n",
            "Model 4 | Epoch 26/30 [86.7%] | Loss: 0.134825 | Total Progress: 38.7% | Time Left: 0:01:35\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 4 | Epoch 27/30 [90.0%] | Loss: 0.134405 | Total Progress: 39.0% | Time Left: 0:01:35\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 4 | Epoch 28/30 [93.3%] | Loss: 0.133950 | Total Progress: 39.3% | Time Left: 0:01:34\n",
            "Model 4 | Epoch 29/30 [96.7%] | Loss: 0.133362 | Total Progress: 39.7% | Time Left: 0:01:34\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 4 | Epoch 30/30 [100.0%] | Loss: 0.132549 | Total Progress: 40.0% | Time Left: 0:01:33\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 4 training complete and finalized.\n",
            "\n",
            "--- Training Model 5/10 ---\n",
            "Model 5 | Epoch 1/30 [3.3%] | Loss: 0.172855 | Total Progress: 40.3% | Time Left: 0:01:33\n",
            "Model 5 | Epoch 2/30 [6.7%] | Loss: 0.167878 | Total Progress: 40.7% | Time Left: 0:01:32\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 5 | Epoch 3/30 [10.0%] | Loss: 0.163827 | Total Progress: 41.0% | Time Left: 0:01:31\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 5 | Epoch 4/30 [13.3%] | Loss: 0.160661 | Total Progress: 41.3% | Time Left: 0:01:31\n",
            "Model 5 | Epoch 5/30 [16.7%] | Loss: 0.158339 | Total Progress: 41.7% | Time Left: 0:01:30\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 5 | Epoch 6/30 [20.0%] | Loss: 0.156802 | Total Progress: 42.0% | Time Left: 0:01:30\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 5 | Epoch 7/30 [23.3%] | Loss: 0.155970 | Total Progress: 42.3% | Time Left: 0:01:29\n",
            "Model 5 | Epoch 8/30 [26.7%] | Loss: 0.155686 | Total Progress: 42.7% | Time Left: 0:01:28\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 5 | Epoch 9/30 [30.0%] | Loss: 0.155653 | Total Progress: 43.0% | Time Left: 0:01:28\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 5 | Epoch 10/30 [33.3%] | Loss: 0.155639 | Total Progress: 43.3% | Time Left: 0:01:27\n",
            "Model 5 | Epoch 11/30 [36.7%] | Loss: 0.155589 | Total Progress: 43.7% | Time Left: 0:01:27\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 5 | Epoch 12/30 [40.0%] | Loss: 0.155512 | Total Progress: 44.0% | Time Left: 0:01:26\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 5 | Epoch 13/30 [43.3%] | Loss: 0.155422 | Total Progress: 44.3% | Time Left: 0:01:25\n",
            "Model 5 | Epoch 14/30 [46.7%] | Loss: 0.155324 | Total Progress: 44.7% | Time Left: 0:01:25\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 5 | Epoch 15/30 [50.0%] | Loss: 0.155223 | Total Progress: 45.0% | Time Left: 0:01:24\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 5 | Epoch 16/30 [53.3%] | Loss: 0.155120 | Total Progress: 45.3% | Time Left: 0:01:24\n",
            "Model 5 | Epoch 17/30 [56.7%] | Loss: 0.155016 | Total Progress: 45.7% | Time Left: 0:01:23\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 5 | Epoch 18/30 [60.0%] | Loss: 0.154910 | Total Progress: 46.0% | Time Left: 0:01:23\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 5 | Epoch 19/30 [63.3%] | Loss: 0.154805 | Total Progress: 46.3% | Time Left: 0:01:22\n",
            "Model 5 | Epoch 20/30 [66.7%] | Loss: 0.154698 | Total Progress: 46.7% | Time Left: 0:01:22\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 5 | Epoch 21/30 [70.0%] | Loss: 0.154591 | Total Progress: 47.0% | Time Left: 0:01:21\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 5 | Epoch 22/30 [73.3%] | Loss: 0.154482 | Total Progress: 47.3% | Time Left: 0:01:20\n",
            "Model 5 | Epoch 23/30 [76.7%] | Loss: 0.154369 | Total Progress: 47.7% | Time Left: 0:01:20\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 5 | Epoch 24/30 [80.0%] | Loss: 0.154253 | Total Progress: 48.0% | Time Left: 0:01:19\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 5 | Epoch 25/30 [83.3%] | Loss: 0.154128 | Total Progress: 48.3% | Time Left: 0:01:19\n",
            "Model 5 | Epoch 26/30 [86.7%] | Loss: 0.153991 | Total Progress: 48.7% | Time Left: 0:01:18\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 5 | Epoch 27/30 [90.0%] | Loss: 0.153832 | Total Progress: 49.0% | Time Left: 0:01:18\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 5 | Epoch 28/30 [93.3%] | Loss: 0.153637 | Total Progress: 49.3% | Time Left: 0:01:17\n",
            "Model 5 | Epoch 29/30 [96.7%] | Loss: 0.153380 | Total Progress: 49.7% | Time Left: 0:01:17\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 5 | Epoch 30/30 [100.0%] | Loss: 0.153015 | Total Progress: 50.0% | Time Left: 0:01:16\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 5 training complete and finalized.\n",
            "\n",
            "--- Training Model 6/10 ---\n",
            "Model 6 | Epoch 1/30 [3.3%] | Loss: 0.213542 | Total Progress: 50.3% | Time Left: 0:01:15\n",
            "Model 6 | Epoch 2/30 [6.7%] | Loss: 0.202287 | Total Progress: 50.7% | Time Left: 0:01:15\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 6 | Epoch 3/30 [10.0%] | Loss: 0.193418 | Total Progress: 51.0% | Time Left: 0:01:14\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 6 | Epoch 4/30 [13.3%] | Loss: 0.184434 | Total Progress: 51.3% | Time Left: 0:01:14\n",
            "Model 6 | Epoch 5/30 [16.7%] | Loss: 0.174726 | Total Progress: 51.7% | Time Left: 0:01:13\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 6 | Epoch 6/30 [20.0%] | Loss: 0.164332 | Total Progress: 52.0% | Time Left: 0:01:13\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 6 | Epoch 7/30 [23.3%] | Loss: 0.154040 | Total Progress: 52.3% | Time Left: 0:01:12\n",
            "Model 6 | Epoch 8/30 [26.7%] | Loss: 0.145640 | Total Progress: 52.7% | Time Left: 0:01:12\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 6 | Epoch 9/30 [30.0%] | Loss: 0.140780 | Total Progress: 53.0% | Time Left: 0:01:11\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 6 | Epoch 10/30 [33.3%] | Loss: 0.139130 | Total Progress: 53.3% | Time Left: 0:01:11\n",
            "Model 6 | Epoch 11/30 [36.7%] | Loss: 0.138946 | Total Progress: 53.7% | Time Left: 0:01:10\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 6 | Epoch 12/30 [40.0%] | Loss: 0.139062 | Total Progress: 54.0% | Time Left: 0:01:10\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 6 | Epoch 13/30 [43.3%] | Loss: 0.139148 | Total Progress: 54.3% | Time Left: 0:01:09\n",
            "Model 6 | Epoch 14/30 [46.7%] | Loss: 0.139178 | Total Progress: 54.7% | Time Left: 0:01:09\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 6 | Epoch 15/30 [50.0%] | Loss: 0.139179 | Total Progress: 55.0% | Time Left: 0:01:08\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 6 | Epoch 16/30 [53.3%] | Loss: 0.139170 | Total Progress: 55.3% | Time Left: 0:01:07\n",
            "Model 6 | Epoch 17/30 [56.7%] | Loss: 0.139159 | Total Progress: 55.7% | Time Left: 0:01:07\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 6 | Epoch 18/30 [60.0%] | Loss: 0.139147 | Total Progress: 56.0% | Time Left: 0:01:06\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 6 | Epoch 19/30 [63.3%] | Loss: 0.139136 | Total Progress: 56.3% | Time Left: 0:01:06\n",
            "Model 6 | Epoch 20/30 [66.7%] | Loss: 0.139126 | Total Progress: 56.7% | Time Left: 0:01:05\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 6 | Epoch 21/30 [70.0%] | Loss: 0.139117 | Total Progress: 57.0% | Time Left: 0:01:05\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 6 | Epoch 22/30 [73.3%] | Loss: 0.139108 | Total Progress: 57.3% | Time Left: 0:01:04\n",
            "Model 6 | Epoch 23/30 [76.7%] | Loss: 0.139100 | Total Progress: 57.7% | Time Left: 0:01:04\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 6 | Epoch 24/30 [80.0%] | Loss: 0.139092 | Total Progress: 58.0% | Time Left: 0:01:03\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 6 | Epoch 25/30 [83.3%] | Loss: 0.139085 | Total Progress: 58.3% | Time Left: 0:01:03\n",
            "Model 6 | Epoch 26/30 [86.7%] | Loss: 0.139078 | Total Progress: 58.7% | Time Left: 0:01:02\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 6 | Epoch 27/30 [90.0%] | Loss: 0.139071 | Total Progress: 59.0% | Time Left: 0:01:02\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 6 | Epoch 28/30 [93.3%] | Loss: 0.139065 | Total Progress: 59.3% | Time Left: 0:01:01\n",
            "Model 6 | Epoch 29/30 [96.7%] | Loss: 0.139059 | Total Progress: 59.7% | Time Left: 0:01:00\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 6 | Epoch 30/30 [100.0%] | Loss: 0.139054 | Total Progress: 60.0% | Time Left: 0:01:00\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 6 training complete and finalized.\n",
            "\n",
            "--- Training Model 7/10 ---\n",
            "Model 7 | Epoch 1/30 [3.3%] | Loss: 0.157768 | Total Progress: 60.3% | Time Left: 0:00:59\n",
            "Model 7 | Epoch 2/30 [6.7%] | Loss: 0.149756 | Total Progress: 60.7% | Time Left: 0:00:59\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 7 | Epoch 3/30 [10.0%] | Loss: 0.147572 | Total Progress: 61.0% | Time Left: 0:00:58\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 7 | Epoch 4/30 [13.3%] | Loss: 0.146839 | Total Progress: 61.3% | Time Left: 0:00:58\n",
            "Model 7 | Epoch 5/30 [16.7%] | Loss: 0.146263 | Total Progress: 61.7% | Time Left: 0:00:57\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 7 | Epoch 6/30 [20.0%] | Loss: 0.145728 | Total Progress: 62.0% | Time Left: 0:00:57\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 7 | Epoch 7/30 [23.3%] | Loss: 0.145225 | Total Progress: 62.3% | Time Left: 0:00:56\n",
            "Model 7 | Epoch 8/30 [26.7%] | Loss: 0.144758 | Total Progress: 62.7% | Time Left: 0:00:56\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 7 | Epoch 9/30 [30.0%] | Loss: 0.144330 | Total Progress: 63.0% | Time Left: 0:00:55\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 7 | Epoch 10/30 [33.3%] | Loss: 0.143942 | Total Progress: 63.3% | Time Left: 0:00:55\n",
            "Model 7 | Epoch 11/30 [36.7%] | Loss: 0.143592 | Total Progress: 63.7% | Time Left: 0:00:54\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 7 | Epoch 12/30 [40.0%] | Loss: 0.143278 | Total Progress: 64.0% | Time Left: 0:00:54\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 7 | Epoch 13/30 [43.3%] | Loss: 0.142995 | Total Progress: 64.3% | Time Left: 0:00:53\n",
            "Model 7 | Epoch 14/30 [46.7%] | Loss: 0.142739 | Total Progress: 64.7% | Time Left: 0:00:53\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 7 | Epoch 15/30 [50.0%] | Loss: 0.142507 | Total Progress: 65.0% | Time Left: 0:00:52\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 7 | Epoch 16/30 [53.3%] | Loss: 0.142295 | Total Progress: 65.3% | Time Left: 0:00:52\n",
            "Model 7 | Epoch 17/30 [56.7%] | Loss: 0.142099 | Total Progress: 65.7% | Time Left: 0:00:51\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 7 | Epoch 18/30 [60.0%] | Loss: 0.141916 | Total Progress: 66.0% | Time Left: 0:00:51\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 7 | Epoch 19/30 [63.3%] | Loss: 0.141745 | Total Progress: 66.3% | Time Left: 0:00:50\n",
            "Model 7 | Epoch 20/30 [66.7%] | Loss: 0.141584 | Total Progress: 66.7% | Time Left: 0:00:50\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 7 | Epoch 21/30 [70.0%] | Loss: 0.141431 | Total Progress: 67.0% | Time Left: 0:00:49\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 7 | Epoch 22/30 [73.3%] | Loss: 0.141286 | Total Progress: 67.3% | Time Left: 0:00:49\n",
            "Model 7 | Epoch 23/30 [76.7%] | Loss: 0.141149 | Total Progress: 67.7% | Time Left: 0:00:48\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 7 | Epoch 24/30 [80.0%] | Loss: 0.141018 | Total Progress: 68.0% | Time Left: 0:00:48\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 7 | Epoch 25/30 [83.3%] | Loss: 0.140894 | Total Progress: 68.3% | Time Left: 0:00:47\n",
            "Model 7 | Epoch 26/30 [86.7%] | Loss: 0.140776 | Total Progress: 68.7% | Time Left: 0:00:47\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 7 | Epoch 27/30 [90.0%] | Loss: 0.140664 | Total Progress: 69.0% | Time Left: 0:00:46\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 7 | Epoch 28/30 [93.3%] | Loss: 0.140558 | Total Progress: 69.3% | Time Left: 0:00:45\n",
            "Model 7 | Epoch 29/30 [96.7%] | Loss: 0.140458 | Total Progress: 69.7% | Time Left: 0:00:45\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 7 | Epoch 30/30 [100.0%] | Loss: 0.140363 | Total Progress: 70.0% | Time Left: 0:00:44\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 7 training complete and finalized.\n",
            "\n",
            "--- Training Model 8/10 ---\n",
            "Model 8 | Epoch 1/30 [3.3%] | Loss: 0.314643 | Total Progress: 70.3% | Time Left: 0:00:44\n",
            "Model 8 | Epoch 2/30 [6.7%] | Loss: 0.303938 | Total Progress: 70.7% | Time Left: 0:00:43\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 8 | Epoch 3/30 [10.0%] | Loss: 0.293169 | Total Progress: 71.0% | Time Left: 0:00:43\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 8 | Epoch 4/30 [13.3%] | Loss: 0.280067 | Total Progress: 71.3% | Time Left: 0:00:42\n",
            "Model 8 | Epoch 5/30 [16.7%] | Loss: 0.265186 | Total Progress: 71.7% | Time Left: 0:00:42\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 8 | Epoch 6/30 [20.0%] | Loss: 0.250401 | Total Progress: 72.0% | Time Left: 0:00:41\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 8 | Epoch 7/30 [23.3%] | Loss: 0.237975 | Total Progress: 72.3% | Time Left: 0:00:41\n",
            "Model 8 | Epoch 8/30 [26.7%] | Loss: 0.229394 | Total Progress: 72.7% | Time Left: 0:00:40\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 8 | Epoch 9/30 [30.0%] | Loss: 0.224663 | Total Progress: 73.0% | Time Left: 0:00:40\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 8 | Epoch 10/30 [33.3%] | Loss: 0.222513 | Total Progress: 73.3% | Time Left: 0:00:39\n",
            "Model 8 | Epoch 11/30 [36.7%] | Loss: 0.221474 | Total Progress: 73.7% | Time Left: 0:00:39\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 8 | Epoch 12/30 [40.0%] | Loss: 0.220751 | Total Progress: 74.0% | Time Left: 0:00:38\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 8 | Epoch 13/30 [43.3%] | Loss: 0.220084 | Total Progress: 74.3% | Time Left: 0:00:38\n",
            "Model 8 | Epoch 14/30 [46.7%] | Loss: 0.219423 | Total Progress: 74.7% | Time Left: 0:00:37\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 8 | Epoch 15/30 [50.0%] | Loss: 0.218766 | Total Progress: 75.0% | Time Left: 0:00:37\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 8 | Epoch 16/30 [53.3%] | Loss: 0.218111 | Total Progress: 75.3% | Time Left: 0:00:36\n",
            "Model 8 | Epoch 17/30 [56.7%] | Loss: 0.217459 | Total Progress: 75.7% | Time Left: 0:00:36\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 8 | Epoch 18/30 [60.0%] | Loss: 0.216809 | Total Progress: 76.0% | Time Left: 0:00:35\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 8 | Epoch 19/30 [63.3%] | Loss: 0.216162 | Total Progress: 76.3% | Time Left: 0:00:35\n",
            "Model 8 | Epoch 20/30 [66.7%] | Loss: 0.215519 | Total Progress: 76.7% | Time Left: 0:00:34\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 8 | Epoch 21/30 [70.0%] | Loss: 0.214877 | Total Progress: 77.0% | Time Left: 0:00:34\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 8 | Epoch 22/30 [73.3%] | Loss: 0.214240 | Total Progress: 77.3% | Time Left: 0:00:33\n",
            "Model 8 | Epoch 23/30 [76.7%] | Loss: 0.213604 | Total Progress: 77.7% | Time Left: 0:00:33\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 8 | Epoch 24/30 [80.0%] | Loss: 0.212972 | Total Progress: 78.0% | Time Left: 0:00:32\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 8 | Epoch 25/30 [83.3%] | Loss: 0.212343 | Total Progress: 78.3% | Time Left: 0:00:32\n",
            "Model 8 | Epoch 26/30 [86.7%] | Loss: 0.211717 | Total Progress: 78.7% | Time Left: 0:00:31\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 8 | Epoch 27/30 [90.0%] | Loss: 0.211094 | Total Progress: 79.0% | Time Left: 0:00:31\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 8 | Epoch 28/30 [93.3%] | Loss: 0.210473 | Total Progress: 79.3% | Time Left: 0:00:30\n",
            "Model 8 | Epoch 29/30 [96.7%] | Loss: 0.209856 | Total Progress: 79.7% | Time Left: 0:00:30\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 8 | Epoch 30/30 [100.0%] | Loss: 0.209242 | Total Progress: 80.0% | Time Left: 0:00:29\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 8 training complete and finalized.\n",
            "\n",
            "--- Training Model 9/10 ---\n",
            "Model 9 | Epoch 1/30 [3.3%] | Loss: 0.167610 | Total Progress: 80.3% | Time Left: 0:00:29\n",
            "Model 9 | Epoch 2/30 [6.7%] | Loss: 0.159078 | Total Progress: 80.7% | Time Left: 0:00:28\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 9 | Epoch 3/30 [10.0%] | Loss: 0.152406 | Total Progress: 81.0% | Time Left: 0:00:28\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 9 | Epoch 4/30 [13.3%] | Loss: 0.147427 | Total Progress: 81.3% | Time Left: 0:00:27\n",
            "Model 9 | Epoch 5/30 [16.7%] | Loss: 0.144789 | Total Progress: 81.7% | Time Left: 0:00:27\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 9 | Epoch 6/30 [20.0%] | Loss: 0.143945 | Total Progress: 82.0% | Time Left: 0:00:26\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 9 | Epoch 7/30 [23.3%] | Loss: 0.143664 | Total Progress: 82.3% | Time Left: 0:00:26\n",
            "Model 9 | Epoch 8/30 [26.7%] | Loss: 0.143488 | Total Progress: 82.7% | Time Left: 0:00:25\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 9 | Epoch 9/30 [30.0%] | Loss: 0.143301 | Total Progress: 83.0% | Time Left: 0:00:25\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 9 | Epoch 10/30 [33.3%] | Loss: 0.143076 | Total Progress: 83.3% | Time Left: 0:00:24\n",
            "Model 9 | Epoch 11/30 [36.7%] | Loss: 0.142812 | Total Progress: 83.7% | Time Left: 0:00:24\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 9 | Epoch 12/30 [40.0%] | Loss: 0.142511 | Total Progress: 84.0% | Time Left: 0:00:23\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 9 | Epoch 13/30 [43.3%] | Loss: 0.142179 | Total Progress: 84.3% | Time Left: 0:00:23\n",
            "Model 9 | Epoch 14/30 [46.7%] | Loss: 0.141820 | Total Progress: 84.7% | Time Left: 0:00:22\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 9 | Epoch 15/30 [50.0%] | Loss: 0.141439 | Total Progress: 85.0% | Time Left: 0:00:22\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 9 | Epoch 16/30 [53.3%] | Loss: 0.141042 | Total Progress: 85.3% | Time Left: 0:00:21\n",
            "Model 9 | Epoch 17/30 [56.7%] | Loss: 0.140636 | Total Progress: 85.7% | Time Left: 0:00:21\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 9 | Epoch 18/30 [60.0%] | Loss: 0.140229 | Total Progress: 86.0% | Time Left: 0:00:20\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 9 | Epoch 19/30 [63.3%] | Loss: 0.139828 | Total Progress: 86.3% | Time Left: 0:00:20\n",
            "Model 9 | Epoch 20/30 [66.7%] | Loss: 0.139442 | Total Progress: 86.7% | Time Left: 0:00:19\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 9 | Epoch 21/30 [70.0%] | Loss: 0.139084 | Total Progress: 87.0% | Time Left: 0:00:19\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 9 | Epoch 22/30 [73.3%] | Loss: 0.138764 | Total Progress: 87.3% | Time Left: 0:00:18\n",
            "Model 9 | Epoch 23/30 [76.7%] | Loss: 0.138498 | Total Progress: 87.7% | Time Left: 0:00:18\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 9 | Epoch 24/30 [80.0%] | Loss: 0.138297 | Total Progress: 88.0% | Time Left: 0:00:17\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 9 | Epoch 25/30 [83.3%] | Loss: 0.138170 | Total Progress: 88.3% | Time Left: 0:00:17\n",
            "Model 9 | Epoch 26/30 [86.7%] | Loss: 0.138107 | Total Progress: 88.7% | Time Left: 0:00:16\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 9 | Epoch 27/30 [90.0%] | Loss: 0.138078 | Total Progress: 89.0% | Time Left: 0:00:16\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 9 | Epoch 28/30 [93.3%] | Loss: 0.138043 | Total Progress: 89.3% | Time Left: 0:00:15\n",
            "Model 9 | Epoch 29/30 [96.7%] | Loss: 0.137969 | Total Progress: 89.7% | Time Left: 0:00:15\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 9 | Epoch 30/30 [100.0%] | Loss: 0.137842 | Total Progress: 90.0% | Time Left: 0:00:14\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 9 training complete and finalized.\n",
            "\n",
            "--- Training Model 10/10 ---\n",
            "Model 10 | Epoch 1/30 [3.3%] | Loss: 0.141327 | Total Progress: 90.3% | Time Left: 0:00:14\n",
            "Model 10 | Epoch 2/30 [6.7%] | Loss: 0.140538 | Total Progress: 90.7% | Time Left: 0:00:13\n",
            "  -> Checkpoint saved at 6.7% completion.\n",
            "Model 10 | Epoch 3/30 [10.0%] | Loss: 0.140034 | Total Progress: 91.0% | Time Left: 0:00:13\n",
            "  -> Checkpoint saved at 10.0% completion.\n",
            "Model 10 | Epoch 4/30 [13.3%] | Loss: 0.139603 | Total Progress: 91.3% | Time Left: 0:00:12\n",
            "Model 10 | Epoch 5/30 [16.7%] | Loss: 0.139174 | Total Progress: 91.7% | Time Left: 0:00:12\n",
            "  -> Checkpoint saved at 16.7% completion.\n",
            "Model 10 | Epoch 6/30 [20.0%] | Loss: 0.138741 | Total Progress: 92.0% | Time Left: 0:00:11\n",
            "  -> Checkpoint saved at 20.0% completion.\n",
            "Model 10 | Epoch 7/30 [23.3%] | Loss: 0.138333 | Total Progress: 92.3% | Time Left: 0:00:11\n",
            "Model 10 | Epoch 8/30 [26.7%] | Loss: 0.137995 | Total Progress: 92.7% | Time Left: 0:00:10\n",
            "  -> Checkpoint saved at 26.7% completion.\n",
            "Model 10 | Epoch 9/30 [30.0%] | Loss: 0.137758 | Total Progress: 93.0% | Time Left: 0:00:10\n",
            "  -> Checkpoint saved at 30.0% completion.\n",
            "Model 10 | Epoch 10/30 [33.3%] | Loss: 0.137583 | Total Progress: 93.3% | Time Left: 0:00:09\n",
            "Model 10 | Epoch 11/30 [36.7%] | Loss: 0.137359 | Total Progress: 93.7% | Time Left: 0:00:09\n",
            "  -> Checkpoint saved at 36.7% completion.\n",
            "Model 10 | Epoch 12/30 [40.0%] | Loss: 0.137018 | Total Progress: 94.0% | Time Left: 0:00:08\n",
            "  -> Checkpoint saved at 40.0% completion.\n",
            "Model 10 | Epoch 13/30 [43.3%] | Loss: 0.136594 | Total Progress: 94.3% | Time Left: 0:00:08\n",
            "Model 10 | Epoch 14/30 [46.7%] | Loss: 0.136207 | Total Progress: 94.7% | Time Left: 0:00:07\n",
            "  -> Checkpoint saved at 46.7% completion.\n",
            "Model 10 | Epoch 15/30 [50.0%] | Loss: 0.135936 | Total Progress: 95.0% | Time Left: 0:00:07\n",
            "  -> Checkpoint saved at 50.0% completion.\n",
            "Model 10 | Epoch 16/30 [53.3%] | Loss: 0.135638 | Total Progress: 95.3% | Time Left: 0:00:06\n",
            "Model 10 | Epoch 17/30 [56.7%] | Loss: 0.135154 | Total Progress: 95.7% | Time Left: 0:00:06\n",
            "  -> Checkpoint saved at 56.7% completion.\n",
            "Model 10 | Epoch 18/30 [60.0%] | Loss: 0.134630 | Total Progress: 96.0% | Time Left: 0:00:05\n",
            "  -> Checkpoint saved at 60.0% completion.\n",
            "Model 10 | Epoch 19/30 [63.3%] | Loss: 0.134349 | Total Progress: 96.3% | Time Left: 0:00:05\n",
            "Model 10 | Epoch 20/30 [66.7%] | Loss: 0.133930 | Total Progress: 96.7% | Time Left: 0:00:04\n",
            "  -> Checkpoint saved at 66.7% completion.\n",
            "Model 10 | Epoch 21/30 [70.0%] | Loss: 0.133376 | Total Progress: 97.0% | Time Left: 0:00:04\n",
            "  -> Checkpoint saved at 70.0% completion.\n",
            "Model 10 | Epoch 22/30 [73.3%] | Loss: 0.133263 | Total Progress: 97.3% | Time Left: 0:00:03\n",
            "Model 10 | Epoch 23/30 [76.7%] | Loss: 0.132700 | Total Progress: 97.7% | Time Left: 0:00:03\n",
            "  -> Checkpoint saved at 76.7% completion.\n",
            "Model 10 | Epoch 24/30 [80.0%] | Loss: 0.132701 | Total Progress: 98.0% | Time Left: 0:00:02\n",
            "  -> Checkpoint saved at 80.0% completion.\n",
            "Model 10 | Epoch 25/30 [83.3%] | Loss: 0.132151 | Total Progress: 98.3% | Time Left: 0:00:02\n",
            "Model 10 | Epoch 26/30 [86.7%] | Loss: 0.132247 | Total Progress: 98.7% | Time Left: 0:00:01\n",
            "  -> Checkpoint saved at 86.7% completion.\n",
            "Model 10 | Epoch 27/30 [90.0%] | Loss: 0.131620 | Total Progress: 99.0% | Time Left: 0:00:01\n",
            "  -> Checkpoint saved at 90.0% completion.\n",
            "Model 10 | Epoch 28/30 [93.3%] | Loss: 0.131680 | Total Progress: 99.3% | Time Left: 0:00:00\n",
            "Model 10 | Epoch 29/30 [96.7%] | Loss: 0.131083 | Total Progress: 99.7% | Time Left: 0:00:00\n",
            "  -> Checkpoint saved at 96.7% completion.\n",
            "Model 10 | Epoch 30/30 [100.0%] | Loss: 0.131049 | Total Progress: 100.0% | Time Left: 0:00:00\n",
            "  -> Checkpoint saved at 100.0% completion.\n",
            "Model 10 training complete and finalized.\n",
            "\n",
            "Success! All 10 Deep Ensemble models have been trained and secured in Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "EtrQS7ZaWnpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb\n",
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------ Configuration ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ERROR_THRESHOLD = 0.30\n",
        "CONSECUTIVE_SEC = 10\n",
        "\n",
        "NUM_MODELS = 10\n",
        "CUDB_PATH = \"/content/drive/MyDrive/ECG_Datasets/CUDB\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/Models/NSR_Ensemble/\"\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Load Ensemble Models ------------------\n",
        "ensemble_models = []\n",
        "print(\"Loading 10-model Deep Ensemble...\")\n",
        "for m in range(1, NUM_MODELS + 1):\n",
        "    model = LSTMAutoencoder().to(DEVICE)\n",
        "    model_path = os.path.join(MODEL_DIR, f\"LSTM_NSR_autoencoder_10s_ensemble_{m}.pth\")\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n",
        "        model.eval()\n",
        "        ensemble_models.append(model)\n",
        "    else:\n",
        "        print(f\"Warning: Model {m} not found at {model_path}\")\n",
        "\n",
        "print(f\"Successfully loaded {len(ensemble_models)} models.\\n\")\n",
        "\n",
        "# ------------------ Ensemble Inference ------------------\n",
        "def get_ensemble_prediction(window):\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(window, dtype=torch.float32, device=DEVICE)\n",
        "        x = x.unsqueeze(0).unsqueeze(-1)\n",
        "\n",
        "        mses = []\n",
        "        for model in ensemble_models:\n",
        "            recon = model(x)\n",
        "            mse = ((recon.squeeze() - x.squeeze()) ** 2).mean().item()\n",
        "            mses.append(mse)\n",
        "\n",
        "        mses_array = np.array(mses)\n",
        "        mean_mse = np.mean(mses_array)\n",
        "        variance = np.var(mses_array)\n",
        "\n",
        "        return mean_mse, variance\n",
        "\n",
        "# ------------------ Predict Abnormal Segments ------------------\n",
        "def predict_abnormal(ecg, fs=FS, seq_len=SEQ_LEN, threshold=ERROR_THRESHOLD, consecutive=CONSECUTIVE_SEC):\n",
        "    mean_errors = []\n",
        "    variances = []\n",
        "    n_samples = len(ecg)\n",
        "    n_seconds = n_samples // fs\n",
        "\n",
        "    for sec in range(SEQ_LEN_SEC, n_seconds):\n",
        "        start = (sec - SEQ_LEN_SEC) * fs\n",
        "        end = start + seq_len\n",
        "        if end > n_samples:\n",
        "            break\n",
        "        window = ecg[start:end].astype(np.float32)\n",
        "        mean_mse, variance = get_ensemble_prediction(window)\n",
        "        mean_errors.append(mean_mse)\n",
        "        variances.append(variance)\n",
        "\n",
        "    mean_errors = np.array(mean_errors)\n",
        "    variances = np.array(variances)\n",
        "    predictions = np.zeros_like(mean_errors, dtype=int)\n",
        "\n",
        "    for i in range(len(mean_errors) - consecutive + 1):\n",
        "        if np.all(mean_errors[i:i+consecutive] > threshold):\n",
        "            predictions[i:i+consecutive] = 1\n",
        "\n",
        "    return mean_errors, variances, predictions\n",
        "\n",
        "# ==================================================\n",
        "#                BATCH EXECUTION\n",
        "# ==================================================\n",
        "\n",
        "summary = []\n",
        "\n",
        "for idx in range(1, 36):\n",
        "    rec_name = f\"cu{idx:02d}\"\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"Processing CUDB record: {rec_name}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(os.path.join(CUDB_PATH, rec_name))\n",
        "        ecg = record.p_signal[:, 0]\n",
        "        ann = wfdb.rdann(os.path.join(CUDB_PATH, rec_name), 'atr')\n",
        "    except Exception:\n",
        "        print(\"Failed to load record or annotation\")\n",
        "        continue\n",
        "\n",
        "    # ---------- Annotations ----------\n",
        "    vtac_annot_sec = None\n",
        "    vfib_annot_sec = None\n",
        "\n",
        "    for sym, samp in zip(ann.symbol, ann.sample):\n",
        "        if sym == '[' and vtac_annot_sec is None:\n",
        "            vtac_annot_sec = samp // FS\n",
        "        if sym == '+' and vfib_annot_sec is None:\n",
        "            vfib_annot_sec = samp // FS\n",
        "\n",
        "    if vtac_annot_sec is not None:\n",
        "        print(f\"Annotated VTAC start time: {vtac_annot_sec} seconds\")\n",
        "    else:\n",
        "        print(\"No VTAC annotation found\")\n",
        "\n",
        "    if vfib_annot_sec is not None:\n",
        "        print(f\"Annotated VFIB start time: {vfib_annot_sec} seconds\")\n",
        "    else:\n",
        "        print(\"No VFIB annotation found\")\n",
        "\n",
        "    # ---------- Model Prediction ----------\n",
        "    mean_errors, variances, predictions = predict_abnormal(ecg)\n",
        "\n",
        "    vtac_start_sec = None\n",
        "    detection_uncertainty = None\n",
        "\n",
        "    for i in range(len(mean_errors) - CONSECUTIVE_SEC + 1):\n",
        "        if np.all(mean_errors[i:i+CONSECUTIVE_SEC] > ERROR_THRESHOLD):\n",
        "            vtac_start_sec = i + SEQ_LEN_SEC\n",
        "            detection_uncertainty = np.mean(variances[i:i+CONSECUTIVE_SEC])\n",
        "            break\n",
        "\n",
        "    if vtac_start_sec is not None:\n",
        "        print(f\"Model detected sustained abnormality at second: {vtac_start_sec}\")\n",
        "        print(f\"Ensemble Uncertainty (Variance) at detection: {detection_uncertainty:.6f}\")\n",
        "    else:\n",
        "        print(\"Model did not detect sustained abnormality\")\n",
        "\n",
        "    # ---------- Compute Differences ----------\n",
        "    vtac_diff_sec = vtac_start_sec - vtac_annot_sec if vtac_start_sec is not None and vtac_annot_sec is not None else None\n",
        "    vfib_diff_sec = vtac_start_sec - vfib_annot_sec if vtac_start_sec is not None and vfib_annot_sec is not None else None\n",
        "\n",
        "    summary.append({\n",
        "        \"record\": rec_name,\n",
        "        \"vtac_annotation_sec\": vtac_annot_sec,\n",
        "        \"vfib_annotation_sec\": vfib_annot_sec,\n",
        "        \"model_detect_sec\": vtac_start_sec,\n",
        "        \"detection_uncertainty\": detection_uncertainty,\n",
        "        \"vtac_diff_sec\": vtac_diff_sec,\n",
        "        \"vfib_diff_sec\": vfib_diff_sec\n",
        "    })\n",
        "\n",
        "# ==================================================\n",
        "#                 FINAL SUMMARY\n",
        "# ==================================================\n",
        "print(\"\\n===================================\")\n",
        "print(\"           FINAL SUMMARY            \")\n",
        "print(\"===================================\")\n",
        "\n",
        "for s in summary:\n",
        "    print(f\"Record: {s['record']}\")\n",
        "    print(f\"  VTAC Annotation: {s['vtac_annotation_sec']} sec\")\n",
        "    print(f\"  VFIB Annotation: {s['vfib_annotation_sec']} sec\")\n",
        "    print(f\"  Model Detection: {s['model_detect_sec']} sec\")\n",
        "\n",
        "    if s['detection_uncertainty'] is not None:\n",
        "        print(f\"  Uncertainty Score: {s['detection_uncertainty']:.6f}\")\n",
        "    else:\n",
        "        print(\"  Uncertainty Score: N/A\")\n",
        "\n",
        "    print(f\"  Difference from VTAC: {s['vtac_diff_sec']} sec\")\n",
        "    print(f\"  Difference from VFIB: {s['vfib_diff_sec']} sec\")\n",
        "    print(\"-----------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUJVE8SdWnN2",
        "outputId": "5110be67-9054-41f3-8ec5-b10713173f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-3.0.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2026.1.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.3.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-3.0.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.1 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.1 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.1 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.1 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.1 which is incompatible.\n",
            "dask-cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-3.0.1 wfdb-4.3.1\n",
            "Loading 10-model Deep Ensemble...\n",
            "Successfully loaded 10 models.\n",
            "\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu01\n",
            "==============================\n",
            "Annotated VTAC start time: 214 seconds\n",
            "Annotated VFIB start time: 214 seconds\n",
            "Model detected sustained abnormality at second: 135\n",
            "Ensemble Uncertainty (Variance) at detection: 0.030729\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu02\n",
            "==============================\n",
            "No VTAC annotation found\n",
            "Annotated VFIB start time: 192 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.005767\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu03\n",
            "==============================\n",
            "Annotated VTAC start time: 465 seconds\n",
            "Annotated VFIB start time: 465 seconds\n",
            "Model detected sustained abnormality at second: 16\n",
            "Ensemble Uncertainty (Variance) at detection: 0.026414\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu04\n",
            "==============================\n",
            "Annotated VTAC start time: 155 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 210\n",
            "Ensemble Uncertainty (Variance) at detection: 0.044797\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu05\n",
            "==============================\n",
            "Annotated VTAC start time: 358 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 34\n",
            "Ensemble Uncertainty (Variance) at detection: 0.000964\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu06\n",
            "==============================\n",
            "Annotated VTAC start time: 184 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.004617\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu07\n",
            "==============================\n",
            "Annotated VTAC start time: 182 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 150\n",
            "Ensemble Uncertainty (Variance) at detection: 0.002033\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu08\n",
            "==============================\n",
            "Annotated VTAC start time: 426 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.037405\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu09\n",
            "==============================\n",
            "Annotated VTAC start time: 239 seconds\n",
            "Annotated VFIB start time: 100 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.002867\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu10\n",
            "==============================\n",
            "Annotated VTAC start time: 316 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 137\n",
            "Ensemble Uncertainty (Variance) at detection: 0.001605\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu11\n",
            "==============================\n",
            "Annotated VTAC start time: 371 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.004815\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu12\n",
            "==============================\n",
            "Annotated VTAC start time: 261 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.008237\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu13\n",
            "==============================\n",
            "Annotated VTAC start time: 427 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 66\n",
            "Ensemble Uncertainty (Variance) at detection: 0.001146\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu14\n",
            "==============================\n",
            "No VTAC annotation found\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.002878\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu15\n",
            "==============================\n",
            "Annotated VTAC start time: 405 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 40\n",
            "Ensemble Uncertainty (Variance) at detection: 0.001355\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu16\n",
            "==============================\n",
            "Annotated VTAC start time: 254 seconds\n",
            "Annotated VFIB start time: 168 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.001933\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu17\n",
            "==============================\n",
            "Annotated VTAC start time: 382 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 391\n",
            "Ensemble Uncertainty (Variance) at detection: 0.002705\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu18\n",
            "==============================\n",
            "Annotated VTAC start time: 334 seconds\n",
            "Annotated VFIB start time: 40 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.001294\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu19\n",
            "==============================\n",
            "Annotated VTAC start time: 409 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 23\n",
            "Ensemble Uncertainty (Variance) at detection: 0.001380\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu20\n",
            "==============================\n",
            "Annotated VTAC start time: 244 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 238\n",
            "Ensemble Uncertainty (Variance) at detection: 0.002209\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu21\n",
            "==============================\n",
            "Annotated VTAC start time: 0 seconds\n",
            "Annotated VFIB start time: 52 seconds\n",
            "Model detected sustained abnormality at second: 10\n",
            "Ensemble Uncertainty (Variance) at detection: 0.010260\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu22\n",
            "==============================\n",
            "Annotated VTAC start time: 338 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 340\n",
            "Ensemble Uncertainty (Variance) at detection: 0.003222\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu23\n",
            "==============================\n",
            "Annotated VTAC start time: 334 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 336\n",
            "Ensemble Uncertainty (Variance) at detection: 0.003770\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu24\n",
            "==============================\n",
            "Annotated VTAC start time: 356 seconds\n",
            "No VFIB annotation found\n",
            "Model detected sustained abnormality at second: 366\n",
            "Ensemble Uncertainty (Variance) at detection: 0.001894\n",
            "\n",
            "==============================\n",
            "Processing CUDB record: cu25\n",
            "==============================\n",
            "Annotated VTAC start time: 421 seconds\n",
            "No VFIB annotation found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>New Databases"
      ],
      "metadata": {
        "id": "i3r618cWxShF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "#print(\"Mounting Google Drive...\")\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the Base Directory\n",
        "BASE_DIR = \"/content/drive/MyDrive/New Research Paper\"\n",
        "\n",
        "# 3. Define Logical Subfolders for the Ensemble Pivot\n",
        "subfolders = [\n",
        "    \"Data\",                     # To store the specific Fantasia/NSRDB subsets\n",
        "    \"Models/Ensemble\",          # To store the 10 separate LSTM .pth files\n",
        "    \"Results\",                  # To store the uncertainty scores and CSVs\n",
        "    \"Scripts\"                   # To keep your training/inference code organized\n",
        "]\n",
        "\n",
        "# 4. Create the Directories\n",
        "print(\"\\nCreating Directory Structure:\")\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "print(f\"✅ Base Directory: {BASE_DIR}\")\n",
        "\n",
        "for folder in subfolders:\n",
        "    folder_path = os.path.join(BASE_DIR, folder)\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "    print(f\"✅ Subfolder: {folder_path}\")\n",
        "\n",
        "print(\"\\nAll folders successfully created and ready for the new framework.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnXWxIuXxV1f",
        "outputId": "1cb1a851-d173-495c-8c6f-9b3ac6acebfd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating Directory Structure:\n",
            "✅ Base Directory: /content/drive/MyDrive/New Research Paper\n",
            "✅ Subfolder: /content/drive/MyDrive/New Research Paper/Data\n",
            "✅ Subfolder: /content/drive/MyDrive/New Research Paper/Models/Ensemble\n",
            "✅ Subfolder: /content/drive/MyDrive/New Research Paper/Results\n",
            "✅ Subfolder: /content/drive/MyDrive/New Research Paper/Scripts\n",
            "\n",
            "All folders successfully created and ready for the new framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import resample\n",
        "\n",
        "TARGET_FS = 250\n",
        "WINDOW_SEC = 100\n",
        "TARGET_SAMPLES = TARGET_FS * WINDOW_SEC\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/New Research Paper/Data/\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "nsr_records = [\n",
        "    \"16272\",\"19830\",\"16539\",\"16773\",\"16786\",\"16265\",\"16420\",\"17453\",\n",
        "    \"17052\",\"16483\",\"19140\",\"19093\",\"16273\",\"18184\",\"18177\",\"19088\",\n",
        "    \"16795\",\"19090\"\n",
        "]\n",
        "\n",
        "fantasia_records = [\n",
        "    'f1y01', 'f1y02', 'f1y03', 'f1y04', 'f1y05', 'f1y06', 'f1y07', 'f1y08', 'f1y09', 'f1y10',\n",
        "    'f1o01', 'f1o02', 'f1o03', 'f1o04', 'f1o05', 'f1o06', 'f1o07', 'f1o08', 'f1o09', 'f1o10',\n",
        "    'f2y01', 'f2y02', 'f2y03', 'f2y04', 'f2y05', 'f2y06', 'f2y07', 'f2y08', 'f2y09', 'f2y10',\n",
        "    'f2o01', 'f2o02', 'f2o03', 'f2o04', 'f2o05', 'f2o06', 'f2o07', 'f2o08', 'f2o09', 'f2o10'\n",
        "]\n",
        "\n",
        "ptb_records = [\n",
        "    'patient104/s0306lre', 'patient105/s0303lre', 'patient116/s0301lre',\n",
        "    'patient117/s0291lre', 'patient121/s0311lre', 'patient122/s0312lre',\n",
        "    'patient130/s0283lre', 'patient131/s0285lre', 'patient150/s0287lre',\n",
        "    'patient155/s0300lre'\n",
        "]\n",
        "\n",
        "def process_database(db_name, record_list, native_fs):\n",
        "    print(f\"\\nProcessing {db_name} Database ({native_fs} Hz)...\")\n",
        "\n",
        "    db_out_dir = os.path.join(BASE_DIR, f\"{db_name}_100s\")\n",
        "    os.makedirs(db_out_dir, exist_ok=True)\n",
        "\n",
        "    saved_files = []\n",
        "    native_samples_needed = native_fs * WINDOW_SEC\n",
        "\n",
        "    for rec_name in record_list:\n",
        "        try:\n",
        "            if db_name == \"PTB\":\n",
        "                record = wfdb.rdrecord(rec_name, pn_dir='ptbdb')\n",
        "            else:\n",
        "                record = wfdb.rdrecord(rec_name, pn_dir=db_name.lower())\n",
        "\n",
        "            ecg_idx = 0\n",
        "            if 'ECG' in record.sig_name:\n",
        "                ecg_idx = record.sig_name.index('ECG')\n",
        "\n",
        "            ecg = record.p_signal[:, ecg_idx]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {rec_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        if len(ecg) < native_samples_needed:\n",
        "            print(f\"Skipping {rec_name}: ECG too short.\")\n",
        "            continue\n",
        "\n",
        "        raw_window = ecg[:native_samples_needed].astype(np.float32)\n",
        "\n",
        "        if native_fs != TARGET_FS:\n",
        "            final_window = resample(raw_window, TARGET_SAMPLES).astype(np.float32)\n",
        "        else:\n",
        "            final_window = raw_window\n",
        "\n",
        "        safe_name = rec_name.replace(\"/\", \"_\")\n",
        "        out_file = os.path.join(db_out_dir, f\"X_{db_name.lower()}_{safe_name}.npy\")\n",
        "        np.save(out_file, final_window)\n",
        "        saved_files.append(out_file)\n",
        "\n",
        "        print(f\"Saved 1 window for {rec_name}\")\n",
        "\n",
        "    print(f\"\\nConcatenating all {db_name} windows...\")\n",
        "    all_data = []\n",
        "    for f in saved_files:\n",
        "        all_data.append(np.load(f))\n",
        "\n",
        "    if all_data:\n",
        "        X_final = np.stack(all_data, axis=0)\n",
        "        final_file = os.path.join(BASE_DIR, f\"X_{db_name.lower()}_final.npy\")\n",
        "        np.save(final_file, X_final)\n",
        "        print(f\"Final {db_name} dataset saved: {final_file}\")\n",
        "        print(f\"Final shape: {X_final.shape}\")\n",
        "    else:\n",
        "        print(f\"No valid data extracted for {db_name}.\")\n",
        "\n",
        "process_database(\"NSRDB\", nsr_records, native_fs=128)\n",
        "process_database(\"Fantasia\", fantasia_records, native_fs=250)\n",
        "process_database(\"PTB\", ptb_records, native_fs=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYtnHIl41A7a",
        "outputId": "a4f5f4e0-b492-4885-c7e5-cc4c87cde470"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing NSRDB Database (128 Hz)...\n",
            "Saved 1 window for 16272\n",
            "Saved 1 window for 19830\n",
            "Saved 1 window for 16539\n",
            "Saved 1 window for 16773\n",
            "Saved 1 window for 16786\n",
            "Saved 1 window for 16265\n",
            "Saved 1 window for 16420\n",
            "Saved 1 window for 17453\n",
            "Saved 1 window for 17052\n",
            "Saved 1 window for 16483\n",
            "Saved 1 window for 19140\n",
            "Saved 1 window for 19093\n",
            "Saved 1 window for 16273\n",
            "Saved 1 window for 18184\n",
            "Saved 1 window for 18177\n",
            "Saved 1 window for 19088\n",
            "Saved 1 window for 16795\n",
            "Saved 1 window for 19090\n",
            "\n",
            "Concatenating all NSRDB windows...\n",
            "Final NSRDB dataset saved: /content/drive/MyDrive/New Research Paper/Data/X_nsrdb_final.npy\n",
            "Final shape: (18, 25000)\n",
            "\n",
            "Processing Fantasia Database (250 Hz)...\n",
            "Saved 1 window for f1y01\n",
            "Saved 1 window for f1y02\n",
            "Saved 1 window for f1y03\n",
            "Saved 1 window for f1y04\n",
            "Saved 1 window for f1y05\n",
            "Saved 1 window for f1y06\n",
            "Saved 1 window for f1y07\n",
            "Saved 1 window for f1y08\n",
            "Saved 1 window for f1y09\n",
            "Saved 1 window for f1y10\n",
            "Saved 1 window for f1o01\n",
            "Saved 1 window for f1o02\n",
            "Saved 1 window for f1o03\n",
            "Saved 1 window for f1o04\n",
            "Saved 1 window for f1o05\n",
            "Saved 1 window for f1o06\n",
            "Saved 1 window for f1o07\n",
            "Saved 1 window for f1o08\n",
            "Saved 1 window for f1o09\n",
            "Saved 1 window for f1o10\n",
            "Saved 1 window for f2y01\n",
            "Saved 1 window for f2y02\n",
            "Saved 1 window for f2y03\n",
            "Saved 1 window for f2y04\n",
            "Saved 1 window for f2y05\n",
            "Saved 1 window for f2y06\n",
            "Saved 1 window for f2y07\n",
            "Saved 1 window for f2y08\n",
            "Saved 1 window for f2y09\n",
            "Saved 1 window for f2y10\n",
            "Saved 1 window for f2o01\n",
            "Saved 1 window for f2o02\n",
            "Saved 1 window for f2o03\n",
            "Saved 1 window for f2o04\n",
            "Saved 1 window for f2o05\n",
            "Saved 1 window for f2o06\n",
            "Saved 1 window for f2o07\n",
            "Saved 1 window for f2o08\n",
            "Saved 1 window for f2o09\n",
            "Saved 1 window for f2o10\n",
            "\n",
            "Concatenating all Fantasia windows...\n",
            "Final Fantasia dataset saved: /content/drive/MyDrive/New Research Paper/Data/X_fantasia_final.npy\n",
            "Final shape: (40, 25000)\n",
            "\n",
            "Processing PTB Database (1000 Hz)...\n",
            "Error loading patient104/s0306lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0306lre.hea\n",
            "Error loading patient105/s0303lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0303lre.hea\n",
            "Error loading patient116/s0301lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0301lre.hea\n",
            "Error loading patient117/s0291lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0291lre.hea\n",
            "Error loading patient121/s0311lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0311lre.hea\n",
            "Error loading patient122/s0312lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0312lre.hea\n",
            "Error loading patient130/s0283lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0283lre.hea\n",
            "Error loading patient131/s0285lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0285lre.hea\n",
            "Error loading patient150/s0287lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0287lre.hea\n",
            "Error loading patient155/s0300lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/s0300lre.hea\n",
            "\n",
            "Concatenating all PTB windows...\n",
            "No valid data extracted for PTB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_database(db_name, record_list, native_fs):\n",
        "    print(f\"\\nProcessing {db_name} Database ({native_fs} Hz)...\")\n",
        "\n",
        "    db_out_dir = os.path.join(BASE_DIR, f\"{db_name}_100s\")\n",
        "    os.makedirs(db_out_dir, exist_ok=True)\n",
        "\n",
        "    saved_files = []\n",
        "    native_samples_needed = native_fs * WINDOW_SEC\n",
        "\n",
        "    for rec_name in record_list:\n",
        "        try:\n",
        "            if db_name == \"PTB\":\n",
        "                # Split the folder and the filename so wfdb builds the URL correctly\n",
        "                folder, file_name = rec_name.split('/')\n",
        "                record = wfdb.rdrecord(file_name, pn_dir=f'ptbdb/{folder}')\n",
        "            else:\n",
        "                record = wfdb.rdrecord(rec_name, pn_dir=db_name.lower())\n",
        "\n",
        "            # Safely find the correct ECG column\n",
        "            ecg_idx = 0\n",
        "            if 'ECG' in record.sig_name:\n",
        "                ecg_idx = record.sig_name.index('ECG')\n",
        "            elif 'I' in record.sig_name:  # Standard Lead I for PTB\n",
        "                ecg_idx = record.sig_name.index('I')\n",
        "            elif 'MLII' in record.sig_name: # Common in NSRDB\n",
        "                ecg_idx = record.sig_name.index('MLII')\n",
        "\n",
        "            ecg = record.p_signal[:, ecg_idx]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {rec_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        if len(ecg) < native_samples_needed:\n",
        "            print(f\"Skipping {rec_name}: ECG too short.\")\n",
        "            continue\n",
        "\n",
        "        raw_window = ecg[:native_samples_needed].astype(np.float32)\n",
        "\n",
        "        # Resample only if necessary\n",
        "        if native_fs != TARGET_FS:\n",
        "            final_window = resample(raw_window, TARGET_SAMPLES).astype(np.float32)\n",
        "        else:\n",
        "            final_window = raw_window\n",
        "\n",
        "        safe_name = rec_name.replace(\"/\", \"_\")\n",
        "        out_file = os.path.join(db_out_dir, f\"X_{db_name.lower()}_{safe_name}.npy\")\n",
        "        np.save(out_file, final_window)\n",
        "        saved_files.append(out_file)\n",
        "\n",
        "        print(f\"Saved 1 window for {rec_name}\")\n",
        "\n",
        "    print(f\"\\nConcatenating all {db_name} windows...\")\n",
        "    all_data = []\n",
        "    for f in saved_files:\n",
        "        all_data.append(np.load(f))\n",
        "\n",
        "    if all_data:\n",
        "        X_final = np.stack(all_data, axis=0)\n",
        "        final_file = os.path.join(BASE_DIR, f\"X_{db_name.lower()}_final.npy\")\n",
        "        np.save(final_file, X_final)\n",
        "        print(f\"Final {db_name} dataset saved: {final_file}\")\n",
        "        print(f\"Final shape: {X_final.shape}\")\n",
        "    else:\n",
        "        print(f\"No valid data extracted for {db_name}.\")\n",
        "\n",
        "ptb_records = [\n",
        "    'patient104/s0306lre', 'patient105/s0303lre', 'patient116/s0301lre',\n",
        "    'patient117/s0291lre', 'patient121/s0311lre', 'patient122/s0312lre',\n",
        "    'patient130/s0283lre', 'patient131/s0285lre', 'patient150/s0287lre',\n",
        "    'patient155/s0300lre'\n",
        "]\n",
        "\n",
        "process_database(\"PTB\", ptb_records, native_fs=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUR5Mio1SvvV",
        "outputId": "57cead9b-48c5-463e-819c-fd7b4bf71f42"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing PTB Database (1000 Hz)...\n",
            "Saved 1 window for patient104/s0306lre\n",
            "Saved 1 window for patient105/s0303lre\n",
            "Error loading patient116/s0301lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/patient116/s0301lre.hea\n",
            "Saved 1 window for patient117/s0291lre\n",
            "Saved 1 window for patient121/s0311lre\n",
            "Saved 1 window for patient122/s0312lre\n",
            "Error loading patient130/s0283lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/patient130/s0283lre.hea\n",
            "Error loading patient131/s0285lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/patient131/s0285lre.hea\n",
            "Saved 1 window for patient150/s0287lre\n",
            "Error loading patient155/s0300lre: 404 Error: Not Found for url: https://physionet.org/files/ptbdb/1.0.0/patient155/s0300lre.hea\n",
            "\n",
            "Concatenating all PTB windows...\n",
            "Final PTB dataset saved: /content/drive/MyDrive/New Research Paper/Data/X_ptb_final.npy\n",
            "Final shape: (6, 25000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n"
      ],
      "metadata": {
        "id": "AVfR_emIUzhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# ------------------ Hyperparameters ------------------\n",
        "FS = 250\n",
        "SEQ_LEN_SEC = 10\n",
        "SEQ_LEN = SEQ_LEN_SEC * FS\n",
        "STRIDE_SEC = 1\n",
        "STRIDE = STRIDE_SEC * FS\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "LR = 0.001\n",
        "\n",
        "NUM_MODELS = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/New Research Paper/Data/\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/New Research Paper/Models/Ensemble/\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------ Dataset, Sanitization, & Slicing ------------------\n",
        "def sanitize_and_normalize(X_raw):\n",
        "    \"\"\"\n",
        "    Removes NaNs/Infs and applies Z-score normalization per record\n",
        "    to prevent exploding gradients during training.\n",
        "    \"\"\"\n",
        "    X_clean = np.nan_to_num(X_raw, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    for i in range(len(X_clean)):\n",
        "        mean_val = np.mean(X_clean[i])\n",
        "        std_val = np.std(X_clean[i])\n",
        "        if std_val > 0:\n",
        "            X_clean[i] = (X_clean[i] - mean_val) / std_val\n",
        "\n",
        "    return X_clean\n",
        "\n",
        "def slice_into_windows(X_raw, window_size=SEQ_LEN, stride=STRIDE):\n",
        "    \"\"\"\n",
        "    Takes a (records, 25000) array and slices it into (N, 2500) overlapping windows.\n",
        "    \"\"\"\n",
        "    num_records, total_samples = X_raw.shape\n",
        "    windows = []\n",
        "\n",
        "    for i in range(num_records):\n",
        "        for start in range(0, total_samples - window_size + 1, stride):\n",
        "            windows.append(X_raw[i, start:start+window_size])\n",
        "\n",
        "    return np.array(windows, dtype=np.float32)\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx][:, None]\n",
        "        return x, x\n",
        "\n",
        "# ------------------ LSTM Autoencoder ------------------\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out, _ = self.encoder(x)\n",
        "        dec_out, _ = self.decoder(enc_out)\n",
        "        return dec_out\n",
        "\n",
        "# ------------------ Pre-load and Slice Data ------------------\n",
        "print(\"Loading, sanitizing, and slicing datasets...\")\n",
        "\n",
        "X_nsrdb_raw = np.load(os.path.join(DATA_DIR, \"X_nsrdb_final.npy\"))\n",
        "X_nsrdb_raw = sanitize_and_normalize(X_nsrdb_raw)\n",
        "X_nsrdb = slice_into_windows(X_nsrdb_raw)\n",
        "print(f\"NSRDB windows generated: {X_nsrdb.shape}\")\n",
        "\n",
        "X_fantasia_raw = np.load(os.path.join(DATA_DIR, \"X_fantasia_final.npy\"))\n",
        "X_fantasia_raw = sanitize_and_normalize(X_fantasia_raw)\n",
        "X_fantasia = slice_into_windows(X_fantasia_raw)\n",
        "print(f\"Fantasia windows generated: {X_fantasia.shape}\")\n",
        "\n",
        "X_ptb_raw = np.load(os.path.join(DATA_DIR, \"X_ptb_final.npy\"))\n",
        "X_ptb_raw = sanitize_and_normalize(X_ptb_raw)\n",
        "X_ptb = slice_into_windows(X_ptb_raw)\n",
        "print(f\"PTB windows generated: {X_ptb.shape}\\n\")\n",
        "\n",
        "# ------------------ Training Distribution Tracker ------------------\n",
        "total_epochs_all_models = NUM_MODELS * EPOCHS\n",
        "completed_epochs_overall = 0\n",
        "\n",
        "for m in range(1, NUM_MODELS + 1):\n",
        "    final_path = os.path.join(MODEL_DIR, f\"LSTM_Ensemble_{m}.pth\")\n",
        "    ckpt_path = os.path.join(MODEL_DIR, f\"ensemble_{m}_checkpoint.pth\")\n",
        "    if os.path.exists(final_path):\n",
        "        completed_epochs_overall += EPOCHS\n",
        "    elif os.path.exists(ckpt_path):\n",
        "        checkpoint = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n",
        "        completed_epochs_overall += checkpoint['epoch'] + 1\n",
        "\n",
        "print(f\"System Check: {completed_epochs_overall}/{total_epochs_all_models} total epochs already completed.\")\n",
        "global_start_time = time.time()\n",
        "epochs_run_this_session = 0\n",
        "\n",
        "# ==================================================\n",
        "#                DEEP ENSEMBLE LOOP\n",
        "# ==================================================\n",
        "for m in range(1, NUM_MODELS + 1):\n",
        "    final_model_path = os.path.join(MODEL_DIR, f\"LSTM_Ensemble_{m}.pth\")\n",
        "    checkpoint_path = os.path.join(MODEL_DIR, f\"ensemble_{m}_checkpoint.pth\")\n",
        "\n",
        "    if os.path.exists(final_model_path):\n",
        "        print(f\"Model {m} already trained. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Dataset Routing\n",
        "    if m <= 3:\n",
        "        target_data = X_nsrdb\n",
        "        db_label = \"NSRDB\"\n",
        "    elif m <= 8:\n",
        "        target_data = X_fantasia\n",
        "        db_label = \"Fantasia\"\n",
        "    else:\n",
        "        target_data = X_ptb\n",
        "        db_label = \"PTB\"\n",
        "\n",
        "    print(f\"\\n--- Training Model {m}/{NUM_MODELS} on {db_label} data ({len(target_data)} windows) ---\")\n",
        "\n",
        "    dataset = ECGDataset(target_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    model = LSTMAutoencoder().to(DEVICE)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    start_epoch = 0\n",
        "    next_save_pct = 5.0\n",
        "\n",
        "    # Crash Recovery\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        while (start_epoch / EPOCHS) * 100 >= next_save_pct:\n",
        "            next_save_pct += 5.0\n",
        "        print(f\"Resumed from checkpoint at Epoch {start_epoch+1}\")\n",
        "\n",
        "    # Epoch Loop\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x = batch_x.to(DEVICE)\n",
        "            batch_y = batch_y.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(dataset)\n",
        "        completed_epochs_overall += 1\n",
        "        epochs_run_this_session += 1\n",
        "\n",
        "        avg_time = (time.time() - global_start_time) / epochs_run_this_session\n",
        "        epochs_left = total_epochs_all_models - completed_epochs_overall\n",
        "        eta_string = str(datetime.timedelta(seconds=int(epochs_left * avg_time)))\n",
        "\n",
        "        model_pct = ((epoch + 1) / EPOCHS) * 100\n",
        "        print(f\"Model {m} | Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.6f} | Total Time Left: {eta_string}\")\n",
        "\n",
        "        if model_pct >= next_save_pct:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss\n",
        "            }, checkpoint_path)\n",
        "            next_save_pct += 5.0\n",
        "\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        os.remove(checkpoint_path)\n",
        "\n",
        "print(\"\\nAll models trained successfully and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "Enp9_beIU06l",
        "outputId": "10f6d8f1-6aa4-4dd6-ceee-2e39056ece56"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading, sanitizing, and slicing datasets...\n",
            "NSRDB windows generated: (1638, 2500)\n",
            "Fantasia windows generated: (3640, 2500)\n",
            "PTB windows generated: (546, 2500)\n",
            "\n",
            "System Check: 123/300 total epochs already completed.\n",
            "Model 1 already trained. Skipping.\n",
            "Model 2 already trained. Skipping.\n",
            "Model 3 already trained. Skipping.\n",
            "Model 4 already trained. Skipping.\n",
            "\n",
            "--- Training Model 5/10 on Fantasia data (3640 windows) ---\n",
            "Resumed from checkpoint at Epoch 4\n",
            "Model 5 | Epoch 4/30 | Loss: nan | Total Time Left: 0:18:04\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1113121259.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0;31m# pyrefly: ignore [invalid-param-spec]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    246\u001b[0m             )\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    249\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    971\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             bias_correction1 = [\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             ]\n\u001b[1;32m    775\u001b[0m             bias_correction2 = [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMNhranHXGEqAtPznQvO/bW"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}